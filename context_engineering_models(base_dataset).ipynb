{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxqfboe9vvbuU1kx3lKbBF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CmgoMnluG1Y",
        "outputId": "fd4ee3aa-cb84-4243-a79a-33da327c72c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is the test file to generate 10 samples (description + code) in JSON format with different models."
      ],
      "metadata": {
        "id": "eWREQ6kyUoWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Generate project descriptions and titles with three different models"
      ],
      "metadata": {
        "id": "HfK-afzwXPZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup the GPT-5 model and OpenRouter API key"
      ],
      "metadata": {
        "id": "ElC6Hg0L9q8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install requests jsonschema"
      ],
      "metadata": {
        "id": "S5uFmQWFVKMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, re, textwrap, datetime\n",
        "import requests\n",
        "from jsonschema import Draft7Validator\n",
        "\n",
        "OPENROUTER_API_KEY = \"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\"\n",
        "MODELS = [\n",
        "    #\"openai/gpt-4.1-mini\",\n",
        "    \"anthropic/claude-sonnet-4.5\",\n",
        "    \"qwen/qwen3-coder\",\n",
        "]"
      ],
      "metadata": {
        "id": "K0l84vbFVFOK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Set the variables"
      ],
      "metadata": {
        "id": "OO1bcsPjWvXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a meticulous AI project designer.\n",
        "Your job is to produce concise, implementable AI mini-project ideas that can be turned into runnable Python scripts.\n",
        "\n",
        "Dataset policy (VERY IMPORTANT):\n",
        "- You must prefer ONLY these real datasets when proposing projects:\n",
        "  - sklearn: iris, digits, wine, breast_cancer, diabetes\n",
        "  - sklearn generators: make_classification, make_regression, make_blobs, make_moons, make_circles\n",
        "  - torchvision: MNIST, FashionMNIST, CIFAR10, FakeData, ImageFolder\n",
        "- Do NOT propose or mention 20 Newsgroups, fetch_20newsgroups, or any \"newsgroups\"/\"news group\" variant.\n",
        "- If the project is NOT naturally covered by the whitelist (e.g. NLP, audio, recommendation, time-series), you MUST say in the description:\n",
        "  - “generate 200–300 synthetic … samples” (text / audio-like / tabular / time-series)\n",
        "  - Keep it clearly offline and small.\n",
        "- You may mention standard AI datasets (MNIST, FashionMNIST, CIFAR10) even if they need a download, BUT you must phrase the description so the script can fall back to a small synthetic dataset if the download is not available.\n",
        "\n",
        "Metrics & acceptance:\n",
        "- Every project idea must propose an acceptance/check that is realistic for the dataset + model.\n",
        "- If the code will FALL BACK to synthetic/FakeData, the acceptance must also FALL BACK to an easier threshold.\n",
        "- Use these safe ranges:\n",
        "  • iris (classification): accuracy ≥ 0.90\n",
        "  • wine (classification): accuracy ≥ 0.90–0.92\n",
        "  • breast_cancer (classification): accuracy ≥ 0.90–0.94\n",
        "  • digits + simple model (logreg / linear SVM): accuracy ≥ 0.90–0.93 (not 0.98)\n",
        "  • diabetes (regression): R² ≥ 0.35–0.45\n",
        "  • classic synthetic classifiers (make_moons, make_circles, make_blobs): accuracy ≥ 0.85–0.90, or silhouette ≥ 0.5 for blobs\n",
        "  • PCA / plotting / KMeans on images: acceptance = “file exists and non-empty” or “score in easy range”\n",
        "\n",
        "- If the task says “use synthetic / generate N samples”: set accuracy to 0.60–0.75 or R² to 0.25–0.35.\n",
        "- Do NOT demand SOTA or long training (no 0.99+, no 1e-4 MAE) for mini-projects.\n",
        "- If the dataset may not be available offline (e.g. Fashion-MNIST, MNIST, Reuters), explicitly tell the code generator:\n",
        "  “If real dataset not available → generate synthetic data → use lower threshold.”\n",
        "\n",
        "Rules:\n",
        "- Output must be valid JSON ONLY (no extra text).\n",
        "- Each item has exactly two keys: \"title\" and \"description\".\n",
        "- Titles are short and specific (≤ 6 words).\n",
        "- Descriptions are 1–2 sentences, concrete, and implementable offline in 20–60 minutes.\n",
        "- Prefer single-file, single-metric projects with tiny data and fast runtime.\n",
        "- Avoid duplicate or near-duplicate ideas.\n",
        "- Prefer standard Python libs or widely used ML libs (numpy, pandas, scikit-learn, PyTorch, TensorFlow, OpenCV).\n",
        "- No external downloads; use built-in toy datasets (e.g., sklearn iris/digits) or tiny synthetic data.\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "5vOUmWY-V7b6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEW_SHOTS = \"\"\"\n",
        "{\"title\":\"Iris KNN Classifier\",\n",
        "\"description\":\"Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.9.\"}\n",
        "{\"title\":\"Synthetic Text Sentiment\",\n",
        "\"description\":\"Create 200 short synthetic sentences labeled positive or negative, vectorize with CountVectorizer, train a LogisticRegression, and print accuracy; print TEST_PASS if accuracy ≥ 0.7.\"}\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "jkZyHY39Wi0b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. A function for describing the task"
      ],
      "metadata": {
        "id": "3heZju0CXrbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def build_task(n=10):\n",
        "    return textwrap.dedent(f\"\"\"\n",
        "    Task: Generate {n} distinct AI mini-project ideas.\n",
        "\n",
        "    Constraints:\n",
        "    - Return a JSON array of length {n}.\n",
        "    - Each item: object with exactly \"title\" (string) and \"description\" (string).\n",
        "    - No comments, no prose outside JSON.\n",
        "\n",
        "    Scope & Simplicity:\n",
        "    - Each project is doable offline in 20–60 minutes on CPU.\n",
        "    - Single-file mindset: one clear goal, one primary metric or artifact.\n",
        "    - Keep dependencies minimal (numpy/pandas/sklearn/torch/tf/opencv only).\n",
        "    - Mention one artifact or metric (png, accuracy, inertia, silhouette, TEST_PASS).\n",
        "\n",
        "    Dataset whitelist (must follow):\n",
        "    - sklearn: iris, digits, wine, breast_cancer, diabetes\n",
        "    - sklearn generators: make_classification, make_regression, make_blobs, make_moons, make_circles\n",
        "    - torchvision: MNIST, FashionMNIST, CIFAR10, FakeData, ImageFolder\n",
        "    - Do NOT use 20 Newsgroups or fetch_20newsgroups.\n",
        "\n",
        "    For NLP / audio / task-specific topics:\n",
        "    - Explicitly say: “generate 200–300 synthetic <domain> samples” so the code agent knows to build data in-code.\n",
        "\n",
        "    Description style:\n",
        "    - Titles ≤ 6 words, specific.\n",
        "    - Descriptions are 1–2 sentences with concrete I/O hints (flags, paths, outputs).\n",
        "    - Include at least one quick validation (e.g., accuracy threshold, file existence, non-empty output).\n",
        "\n",
        "    Diversity (within the allowed areas):\n",
        "    - Avoid repeating the same idea or trivial variants.\n",
        "\n",
        "    Follow the style of these examples without repeating them:\n",
        "    {FEW_SHOTS}\n",
        "\n",
        "    Now produce the JSON array of {n} items.\n",
        "    \"\"\").strip()\n"
      ],
      "metadata": {
        "id": "6-5DGgCOXZjl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. OpenRouter call helper"
      ],
      "metadata": {
        "id": "Fp4QApEQXwmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to make a safe Python variable name from a slug\n",
        "def varname_from_slug(slug: str) -> str:\n",
        "    name = slug.lower().replace(\"/\", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\")\n",
        "    return f\"{name}_result\""
      ],
      "metadata": {
        "id": "NzPyoCu1N0-S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic OpenRouter caller taking model_id\n",
        "def call_openrouter_model(model_id, messages, temperature=0.3, top_p=0.9, max_tokens=6000):\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
        "        \"X-Title\": f\"Multi-Model Project Generator\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": float(temperature),\n",
        "        \"top_p\": float(top_p),\n",
        "        \"max_tokens\": int(max_tokens),\n",
        "    }\n",
        "\n",
        "    # force SiliconFlow for Qwen models\n",
        "    if model_id.startswith(\"qwen/\"):\n",
        "      payload[\"provider\"] = {\n",
        "          \"only\": [\"atlas-cloud/fp8\"],\n",
        "          \"allow_fallbacks\": False\n",
        "      }\n",
        "\n",
        "    t0 = time.time()\n",
        "    r = requests.post(url, headers=headers, json=payload, timeout=120)\n",
        "    latency = time.time() - t0\n",
        "    r.raise_for_status()\n",
        "    content = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content, latency"
      ],
      "metadata": {
        "id": "rWSuWp7WMSRz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validator for title/description array\n",
        "from jsonschema import Draft7Validator\n",
        "def validate_items(arr, N):\n",
        "    ITEM_SCHEMA = {\n",
        "        \"type\":\"object\",\n",
        "        \"required\":[\"title\",\"description\"],\n",
        "        \"properties\":{\n",
        "            \"title\":{\"type\":\"string\",\"minLength\":3, \"maxLength\":100},\n",
        "            \"description\":{\"type\":\"string\",\"minLength\":20, \"maxLength\":600}\n",
        "        },\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "    ARRAY_SCHEMA = {\"type\":\"array\",\"items\":ITEM_SCHEMA, \"minItems\":N, \"maxItems\":N}\n",
        "    errs = [e.message for e in Draft7Validator(ARRAY_SCHEMA).iter_errors(arr)]\n",
        "    titles = [ (x.get(\"title\") or \"\").strip().lower() for x in arr ]\n",
        "    if len(set(titles)) != len(titles):\n",
        "        errs.append(\"Duplicate titles detected.\")\n",
        "    return errs"
      ],
      "metadata": {
        "id": "Vf2XvtJbOCqB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust extractor (keeps your previous logic)\n",
        "import re, json, time, requests\n",
        "def extract_json_array(text: str):\n",
        "    m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = m.group(1).strip()\n",
        "    start = text.find('[')\n",
        "    if start == -1:\n",
        "        return None\n",
        "    depth = 0\n",
        "    for i in range(start, len(text)):\n",
        "        ch = text[i]\n",
        "        if ch == '[': depth += 1\n",
        "        elif ch == ']':\n",
        "            depth -= 1\n",
        "            if depth == 0:\n",
        "                return text[start:i+1]\n",
        "    return None"
      ],
      "metadata": {
        "id": "FPYRGxi3OGxS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single shared message set\n",
        "N = 50\n",
        "task = build_task(N) + \"\\n\\nReturn a raw JSON array only — no prose, no code fences, no markdown.\"\n",
        "messages = [\n",
        "    {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
        "    {\"role\":\"user\",\"content\": task}\n",
        "]\n",
        "\n",
        "# Loop models and create a separate variable per model with the results\n",
        "for model_id in MODELS:\n",
        "    print(f\"\\n===== {model_id} =====\")\n",
        "    varname = varname_from_slug(model_id)\n",
        "    try:\n",
        "        raw, secs = call_openrouter_model(model_id, messages, temperature=0.2)\n",
        "        json_str = extract_json_array(raw)\n",
        "        if not json_str:\n",
        "            globals()[varname] = {\n",
        "                \"raw\": raw, \"json_str\": None, \"items\": None,\n",
        "                \"errors\": [\"No JSON array found\"], \"latency\": secs\n",
        "            }\n",
        "            print(f\"❌ No JSON array found | {secs:.1f}s\")\n",
        "            continue\n",
        "\n",
        "        items = json.loads(json_str)\n",
        "        errors = validate_items(items, N)\n",
        "\n",
        "        globals()[varname] = {\n",
        "            \"raw\": raw, \"json_str\": json_str, \"items\": items,\n",
        "            \"errors\": errors, \"latency\": secs\n",
        "        }\n",
        "\n",
        "        if errors:\n",
        "            print(f\"⚠️ Parsed but validation errors ({len(errors)}) | {secs:.1f}s\")\n",
        "            for e in errors[:5]:\n",
        "                print(\" -\", e)\n",
        "        else:\n",
        "            print(f\"✅ Valid JSON ({len(items)} items) | {secs:.1f}s\")\n",
        "            print(json.dumps(items[:2], indent=2, ensure_ascii=False))\n",
        "    except Exception as e:\n",
        "        globals()[varname] = {\n",
        "            \"raw\": None, \"json_str\": None, \"items\": None,\n",
        "            \"errors\": [str(e)], \"latency\": None\n",
        "        }\n",
        "        print(\"❌ Exception:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoWEsUznP-wi",
        "outputId": "c9db09ab-3552-46f6-edad-271537d3f2ce"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== anthropic/claude-sonnet-4.5 =====\n",
            "✅ Valid JSON (50 items) | 44.2s\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Iris KNN Classifier\",\n",
            "    \"description\": \"Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Logistic Regression\",\n",
            "    \"description\": \"Load sklearn digits, flatten images, train LogisticRegression with max_iter=1000, print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "===== qwen/qwen3-coder =====\n",
            "✅ Valid JSON (50 items) | 38.4s\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Iris KNN Classifier\",\n",
            "    \"description\": \"Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.9.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Logistic Regression\",\n",
            "    \"description\": \"Use sklearn wine dataset, scale features, train logistic regression, and report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model_id in MODELS:\n",
        "    print(\" -\", varname_from_slug(model_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEr6Y84rbtaM",
        "outputId": "7d46b514-0233-47fe-cd80-7083d60379fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - anthropic_claude_sonnet_4_5_result\n",
            " - qwen_qwen3_coder_result\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All the resuts for three models that I used are below:"
      ],
      "metadata": {
        "id": "4lC6XKaeW9ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(anthropic_claude_sonnet_4_5_result['items'], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXqi-EmWRcja",
        "outputId": "4d430481-c29f-4744-f0c1-ad0c9b71ac65"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Iris KNN Classifier\",\n",
            "    \"description\": \"Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Logistic Regression\",\n",
            "    \"description\": \"Load sklearn digits, flatten images, train LogisticRegression with max_iter=1000, print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Random Forest\",\n",
            "    \"description\": \"Load sklearn wine dataset, train a RandomForestClassifier with 50 trees, print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SVM\",\n",
            "    \"description\": \"Load sklearn breast_cancer, train a linear SVM (SVC kernel='linear'), print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Ridge Regression\",\n",
            "    \"description\": \"Load sklearn diabetes, train Ridge regression (alpha=1.0), compute test R². Print TEST_PASS if R² ≥ 0.35.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Moons Neural Net\",\n",
            "    \"description\": \"Generate 500 samples with make_moons, train a 2-layer MLP (sklearn MLPClassifier hidden_layer_sizes=(10,10)), print accuracy. Print TEST_PASS if accuracy ≥ 0.85.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Circles SVM RBF\",\n",
            "    \"description\": \"Generate 400 samples with make_circles (noise=0.1), train SVC with rbf kernel, print test accuracy. Print TEST_PASS if accuracy ≥ 0.85.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris PCA Visualization\",\n",
            "    \"description\": \"Load iris, apply PCA to 2 components, scatter plot colored by species, save as iris_pca.png. Print TEST_PASS if file exists and size > 0.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits KMeans Clustering\",\n",
            "    \"description\": \"Load digits, run KMeans with k=10, compute inertia and silhouette score, print both. Print TEST_PASS if silhouette ≥ 0.10.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Decision Tree\",\n",
            "    \"description\": \"Load wine, train DecisionTreeClassifier (max_depth=5), print test accuracy. Print TEST_PASS if accuracy ≥ 0.88.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Regression Lasso\",\n",
            "    \"description\": \"Generate 300 samples with make_regression (n_features=10), train Lasso (alpha=0.1), compute test R². Print TEST_PASS if R² ≥ 0.70.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Naive Bayes\",\n",
            "    \"description\": \"Load breast_cancer, train GaussianNB, print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Cross-Validation\",\n",
            "    \"description\": \"Load iris, perform 5-fold cross-validation with LogisticRegression, print mean CV accuracy. Print TEST_PASS if mean ≥ 0.92.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits t-SNE Plot\",\n",
            "    \"description\": \"Load digits, apply t-SNE to 2D, scatter plot colored by digit label, save as digits_tsne.png. Print TEST_PASS if file exists and size > 0.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Blobs KMeans\",\n",
            "    \"description\": \"Generate 400 samples with make_blobs (centers=4), run KMeans k=4, compute silhouette score. Print TEST_PASS if silhouette ≥ 0.50.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine StandardScaler Pipeline\",\n",
            "    \"description\": \"Load wine, build pipeline with StandardScaler + LogisticRegression, print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes ElasticNet Regression\",\n",
            "    \"description\": \"Load diabetes, train ElasticNet (alpha=0.1, l1_ratio=0.5), compute test R². Print TEST_PASS if R² ≥ 0.35.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Text Sentiment\",\n",
            "    \"description\": \"Generate 200 synthetic sentences labeled positive/negative, vectorize with CountVectorizer, train LogisticRegression, print accuracy. Print TEST_PASS if accuracy ≥ 0.70.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Grid Search KNN\",\n",
            "    \"description\": \"Load iris, use GridSearchCV to tune k in KNeighborsClassifier (k=1,3,5,7), print best k and test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Confusion Matrix\",\n",
            "    \"description\": \"Load digits, train LogisticRegression, compute confusion matrix on test set, save heatmap as digits_cm.png. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Feature Importance\",\n",
            "    \"description\": \"Load breast_cancer, train RandomForestClassifier, extract feature importances, save bar plot as feature_importance.png. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Classification AdaBoost\",\n",
            "    \"description\": \"Generate 500 samples with make_classification (n_features=20), train AdaBoostClassifier (50 estimators), print test accuracy. Print TEST_PASS if accuracy ≥ 0.80.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine PCA Variance\",\n",
            "    \"description\": \"Load wine, apply PCA, plot cumulative explained variance, save as wine_pca_variance.png. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Polynomial Features\",\n",
            "    \"description\": \"Load diabetes, create polynomial features (degree=2), train LinearRegression, compute test R². Print TEST_PASS if R² ≥ 0.40.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris ROC Curve\",\n",
            "    \"description\": \"Load iris, train binary classifier (setosa vs rest) with LogisticRegression, plot ROC curve, save as iris_roc.png. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Audio Classification\",\n",
            "    \"description\": \"Generate 250 synthetic audio-like feature vectors (MFCCs simulation) with 2 classes, train SVM, print accuracy. Print TEST_PASS if accuracy ≥ 0.65.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits SGD Classifier\",\n",
            "    \"description\": \"Load digits, train SGDClassifier (loss='log', max_iter=1000), print test accuracy. Print TEST_PASS if accuracy ≥ 0.88.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Moons Decision Boundary\",\n",
            "    \"description\": \"Generate make_moons (300 samples), train LogisticRegression, plot decision boundary, save as moons_boundary.png. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Learning Curve\",\n",
            "    \"description\": \"Load breast_cancer, compute learning curve for LogisticRegression, plot train/val scores vs training size, save as learning_curve.png. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Multi-Class Metrics\",\n",
            "    \"description\": \"Load wine, train RandomForestClassifier, print classification report (precision, recall, f1). Print TEST_PASS if macro avg f1 ≥ 0.88.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Time-Series Forecasting\",\n",
            "    \"description\": \"Generate 300 synthetic time-series points (sine + noise), train LinearRegression on lag features, compute test MAE. Print TEST_PASS if MAE ≤ 0.5.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Voting Classifier\",\n",
            "    \"description\": \"Load iris, build VotingClassifier with LogisticRegression, DecisionTree, KNN, print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Mini-Batch KMeans\",\n",
            "    \"description\": \"Load digits, run MiniBatchKMeans (k=10, batch_size=50), compute inertia. Print TEST_PASS if inertia < 100000.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Regression Ridge CV\",\n",
            "    \"description\": \"Generate 400 samples with make_regression, use RidgeCV to select alpha, compute test R². Print TEST_PASS if R² ≥ 0.70.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Calibration Plot\",\n",
            "    \"description\": \"Load breast_cancer, train LogisticRegression, plot calibration curve, save as calibration.png. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Outlier Detection\",\n",
            "    \"description\": \"Load wine, apply IsolationForest, count outliers, print percentage. Print TEST_PASS if outliers < 10%.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Tabular Autoencoder\",\n",
            "    \"description\": \"Generate 300 synthetic tabular samples (10 features), build simple autoencoder with PyTorch, train 20 epochs, print final MSE. Print TEST_PASS if MSE ≤ 0.5.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Stratified Split\",\n",
            "    \"description\": \"Load iris, perform stratified train/test split, train LogisticRegression, verify class balance, print accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Gaussian Mixture\",\n",
            "    \"description\": \"Load digits, fit GaussianMixture (n_components=10), compute BIC score. Print TEST_PASS if BIC < 200000.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Circles Kernel PCA\",\n",
            "    \"description\": \"Generate make_circles (400 samples), apply KernelPCA (rbf), plot 2D projection, save as circles_kpca.png. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Feature Selection\",\n",
            "    \"description\": \"Load diabetes, use SelectKBest (f_regression, k=5), train LinearRegression, compute test R². Print TEST_PASS if R² ≥ 0.35.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Recommendation Matrix\",\n",
            "    \"description\": \"Generate 200 synthetic user-item ratings (sparse matrix), apply NMF factorization, compute reconstruction error. Print TEST_PASS if error ≤ 1.0.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Precision-Recall\",\n",
            "    \"description\": \"Load breast_cancer, train LogisticRegression, plot precision-recall curve, save as pr_curve.png. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Label Encoding\",\n",
            "    \"description\": \"Load wine, encode target with LabelEncoder, train SVC, print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Bagging Classifier\",\n",
            "    \"description\": \"Load iris, train BaggingClassifier with DecisionTree base (10 estimators), print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Dimensionality Reduction\",\n",
            "    \"description\": \"Load digits, apply TruncatedSVD to 20 components, train LogisticRegression, print test accuracy. Print TEST_PASS if accuracy ≥ 0.88.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Blobs DBSCAN\",\n",
            "    \"description\": \"Generate make_blobs (500 samples, centers=3), run DBSCAN (eps=0.5), compute number of clusters found. Print TEST_PASS if clusters == 3.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Quantile Regression\",\n",
            "    \"description\": \"Load diabetes, train QuantileRegressor (quantile=0.5), compute test MAE. Print TEST_PASS if MAE ≤ 55.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic NLP Topic Modeling\",\n",
            "    \"description\": \"Generate 250 synthetic documents (bag-of-words), apply LatentDirichletAllocation (3 topics), print top words per topic. Print TEST_PASS if 3 topics extracted.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Threshold Tuning\",\n",
            "    \"description\": \"Load breast_cancer, train LogisticRegression, tune decision threshold for best f1-score, print optimal threshold and f1. Print TEST_PASS if f1 ≥ 0.93.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(qwen_qwen3_coder_result['items'], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv0RTeu2Ur29",
        "outputId": "ba835364-e3ea-4e97-90ec-99a7900154c1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Iris KNN Classifier\",\n",
            "    \"description\": \"Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.9.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Logistic Regression\",\n",
            "    \"description\": \"Use sklearn wine dataset, scale features, train logistic regression, and report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SVM\",\n",
            "    \"description\": \"Train a linear SVM on sklearn breast_cancer dataset, report test accuracy. Print TEST_PASS if accuracy ≥ 0.93.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Linear Classifier\",\n",
            "    \"description\": \"Classify sklearn digits using LogisticRegression, report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Ridge Regression\",\n",
            "    \"description\": \"Fit Ridge regression on sklearn diabetes dataset, compute R² score. Print TEST_PASS if R² ≥ 0.4.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons Decision Tree\",\n",
            "    \"description\": \"Generate make_moons data (n=300), train decision tree, plot decision boundary, save as 'moons_tree.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles Random Forest\",\n",
            "    \"description\": \"Generate make_circles data (n=300), train random forest, report accuracy. Print TEST_PASS if accuracy ≥ 0.88.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Blobs KMeans Clustering\",\n",
            "    \"description\": \"Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score. Print TEST_PASS if silhouette ≥ 0.5.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Classification Synthetic Data\",\n",
            "    \"description\": \"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.85.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Regression Synthetic Data\",\n",
            "    \"description\": \"Generate make_regression data (n=300), fit LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"PCA on Digits\",\n",
            "    \"description\": \"Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as 'digits_pca.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Pairplot Visualization\",\n",
            "    \"description\": \"Plot pairwise feature scatter plots for iris dataset using matplotlib, save as 'iris_pairplot.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Feature Importance\",\n",
            "    \"description\": \"Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart as 'wine_importance.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Confusion Matrix\",\n",
            "    \"description\": \"Train LogisticRegression on breast_cancer dataset, plot confusion matrix, save as 'bc_confusion.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Confusion Heatmap\",\n",
            "    \"description\": \"Train SVC on digits dataset, plot confusion matrix heatmap, save as 'digits_heatmap.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Residual Plot\",\n",
            "    \"description\": \"Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as 'residuals_plot.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Audio Classification\",\n",
            "    \"description\": \"Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Time Series Forecast\",\n",
            "    \"description\": \"Generate 200 synthetic time series samples, predict next value using lag features, compute R². Print TEST_PASS if R² ≥ 0.25.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Tabular Regression\",\n",
            "    \"description\": \"Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Text Sentiment\",\n",
            "    \"description\": \"Create 200 short synthetic sentences labeled positive or negative, vectorize with CountVectorizer, train a LogisticRegression, and print accuracy; print TEST_PASS if accuracy ≥ 0.7.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST CNN Classifier\",\n",
            "    \"description\": \"If MNIST available, train simple CNN (2 conv layers), else generate FakeData (32x32 grayscale), train CNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.85 or fallback ≥ 0.6.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST KNN\",\n",
            "    \"description\": \"If FashionMNIST available, flatten images, train KNN (k=5), else use FakeData, train KNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.7 or fallback ≥ 0.5.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"CIFAR10 Class Balance\",\n",
            "    \"description\": \"If CIFAR10 available, count class frequencies, else generate FakeData with 10 classes, count, save bar chart as 'class_balance.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"ImageFolder Histogram\",\n",
            "    \"description\": \"If ImageFolder available, compute average RGB histogram, else generate 50 FakeData images, compute histogram, save as 'histogram.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FakeData GAN Discriminator\",\n",
            "    \"description\": \"Generate FakeData images, train simple CNN discriminator, report accuracy distinguishing real/fake. Print TEST_PASS if accuracy ≥ 0.7.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"KMeans on CIFAR Colors\",\n",
            "    \"description\": \"If CIFAR10 available, reshape to pixels, run KMeans (k=5) on RGB values, else use FakeData, compute cluster inertia. Print TEST_PASS if inertia < 1.0e5.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Decision Boundary\",\n",
            "    \"description\": \"Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as 'iris_boundary.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine PCA Scatter\",\n",
            "    \"description\": \"Apply PCA to wine dataset (2 components), scatter plot colored by target, save as 'wine_pca.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer ROC Curve\",\n",
            "    \"description\": \"Train LogisticRegression on breast_cancer, compute ROC curve, save as 'roc_curve.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits t-SNE Plot\",\n",
            "    \"description\": \"Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as 'tsne_digits.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Cross Validation\",\n",
            "    \"description\": \"Run 5-fold CV on diabetes dataset with Ridge regression, compute mean R². Print TEST_PASS if mean R² ≥ 0.35.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons SVM Margin\",\n",
            "    \"description\": \"Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as 'svm_margin.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles Neural Net\",\n",
            "    \"description\": \"Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.87.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Blobs Elbow Method\",\n",
            "    \"description\": \"Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as 'elbow_plot.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Classification Learning Curve\",\n",
            "    \"description\": \"Generate make_classification data, train LogisticRegression, plot learning curve, save as 'learning_curve.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Regression Residual Analysis\",\n",
            "    \"description\": \"Generate make_regression data, train LinearRegression, plot residual histogram, save as 'resid_hist.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic NLP Spam Filter\",\n",
            "    \"description\": \"Generate 250 synthetic email texts labeled spam/ham, vectorize with TF-IDF, train Naive Bayes, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Recommender System\",\n",
            "    \"description\": \"Generate 200 synthetic user-item ratings, apply matrix factorization (NMF), compute RMSE. Print TEST_PASS if RMSE ≤ 1.5.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Speech Emotion\",\n",
            "    \"description\": \"Generate 200 synthetic speech-like feature vectors labeled emotion, train RandomForest, report accuracy. Print TEST_PASS if accuracy ≥ 0.65.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Financial Forecast\",\n",
            "    \"description\": \"Generate 300 synthetic stock-like sequences, predict direction (up/down), train LogisticRegression, report accuracy. Print TEST_PASS if accuracy ≥ 0.6.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST Autoencoder\",\n",
            "    \"description\": \"If MNIST available, train simple autoencoder, else use FakeData, reconstruct sample image, save original/recon as 'autoencode.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST VAE Latent\",\n",
            "    \"description\": \"If FashionMNIST available, train basic VAE encoder, else use FakeData, visualize latent space, save as 'latent_vae.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"CIFAR10 Transfer Features\",\n",
            "    \"description\": \"If CIFAR10 available, extract features via pretrained CNN, else use FakeData, train LogisticRegression on features, report accuracy. Print TEST_PASS if accuracy ≥ 0.6 or fallback ≥ 0.4.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"ImageFolder Augmentation\",\n",
            "    \"description\": \"If ImageFolder available, apply rotation/flips to 20 images, else generate FakeData, augment, save augmented grid as 'augment_grid.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FakeData Style Transfer\",\n",
            "    \"description\": \"Generate FakeData content/style pairs, apply basic style transfer algorithm, save result as 'style_transfer.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"KMeans CIFAR Segmentation\",\n",
            "    \"description\": \"If CIFAR10 available, segment image via KMeans clustering, else use FakeData, save segmented image as 'segmented.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Outlier Detection\",\n",
            "    \"description\": \"Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as 'outliers.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Anomaly Score\",\n",
            "    \"description\": \"Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as 'anomaly_scores.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SHAP Values\",\n",
            "    \"description\": \"Train RandomForest on breast_cancer, compute SHAP values for one sample, save waterfall plot as 'shap_waterfall.png'. Print TEST_PASS if file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Grad-CAM Heatmap\",\n",
            "    \"description\": \"Train simple CNN on digits, compute Grad-CAM heatmap for one image, overlay on input, save as 'gradcam.png'. Print TEST_PASS if file exists.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generating the codes for each projects that have been generated earlier with the models"
      ],
      "metadata": {
        "id": "HXJrkTU-XJ3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- External memory (compact, curated) --------\n",
        "memory = {\n",
        "  \"style_guide\": [\n",
        "    \"Single-file script with `if __name__ == '__main__':` entrypoint.\",\n",
        "    \"Use argparse with clear --help and sensible defaults.\",\n",
        "    \"Prefer standard library datasets (sklearn, torchvision, keras).\",\n",
        "    \"Attempt auto-download/cache with short timeout; if unavailable, fallback to a tiny structured synthetic dataset.\",\n",
        "    \"Fix randomness: set seeds for random, numpy; torch if used; run on CPU by default.\",\n",
        "    \"Validate inputs (paths, columns, image loads) and fail gracefully with one-line reason.\",\n",
        "    \"Keep runtime < 2 minutes (few epochs, small subsets).\",\n",
        "    \"Print `TEST_PASS` on success; otherwise `TEST_FAIL: <reason>`.\"\n",
        "  ],\n",
        "  \"lessons\": [\n",
        "    \"When standardizing features use sklearn.pipeline.Pipeline to avoid leakage.\",\n",
        "    \"For OpenCV Canny, expose --threshold1 and --threshold2; convert to grayscale before edges.\",\n",
        "    \"For CSV tasks, explicitly validate required columns; show a friendly error if missing.\",\n",
        "    \"For plotting, save figures to disk and plt.close() to avoid backend issues.\",\n",
        "    \"One file only. Return exactly ONE ```python block. No extra prose.\",\n",
        "    \"CLI + help. Use a single 'argparse.ArgumentParser()'. All help strings are single-line (no embedded newlines).\",\n",
        "    \"Seeds in 'main()'. Expose '--seed' and set seeds for random, numpy, and torch (if present) inside main().\",\n",
        "    \"Data access policy. Only use library datasets when --allow-download is passed. Otherwise do not download; use a robust fallback (sklearn tabular, torchvision FakeData, PIL shapes, etc.). If using 20NG, call with download_if_missing=False unless allowed.\",\n",
        "    \"Task–dataset match. Choose datasets that match the task (e.g., do not use 20 Newsgroups for spam/ham).\",\n",
        "    \"CV safety. For OpenCV: 1. convert to grayscale if needed. 2. ensure input to detectors is uint8 (cv2.convertScaleAbs if needed). 3. for Haar, check face_cascade.empty() == False or fail.\",\n",
        "    \"Acceptance contract. Implement explicit pass/fail checks (files exist, metrics ≥ thresholds, non-empty edge map, etc.). Print TEST_PASS only when all conditions hold; otherwise TEST_FAIL: <reason> and sys.exit(1).\",\n",
        "    \"No broken syntax. Never split identifiers across lines. Never break f-strings or string literals across lines.\",\n",
        "    \"End marker. Append '# END_OF_SCRIPT' as the last line of the file.\"\n",
        "  ],\n",
        "  \"snippets\": [\n",
        "    # seed block to embed in each script\n",
        "    \"import random, numpy as np\\nrandom.seed(42)\\nnp.random.seed(42)\\ntry:\\n    import torch\\n    torch.manual_seed(42)\\nexcept Exception:\\n    pass\"\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "9lD2Trr6Ux6B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT_CODEGEN = \"\"\"\n",
        "You are a meticulous senior Python engineer who writes production-quality, runnable scripts.\n",
        "Priorities: (1) correctness, (2) reproducibility, (3) clarity, (4) speed.\n",
        "\n",
        "Formatting & Output Contract:\n",
        "- Return ONE code block only: ```python ...```\n",
        "- The code must be a single file with `if __name__ == \"__main__\":` entrypoint.\n",
        "- Provide a clear CLI via argparse and `--help`. All help strings must be single-line (no embedded newlines).\n",
        "- Do not print explanations. Do not include markdown outside the single code block.\n",
        "- Append `# END_OF_SCRIPT` as the final line of the file.\n",
        "\n",
        "Dataset whitelist (MUST follow):\n",
        "- You may directly load/use ONLY these real datasets:\n",
        "  - sklearn: iris, digits, wine, breast_cancer, diabetes\n",
        "  - sklearn generators: make_classification, make_regression, make_blobs, make_moons, make_circles\n",
        "  - torchvision: MNIST, FashionMNIST, CIFAR10, FakeData, ImageFolder\n",
        "- Do NOT use 20 Newsgroups, fetch_20newsgroups, or any \"newsgroups\" variant.\n",
        "- If the project description says to “generate 200–300 synthetic … samples”, you MUST implement that synthetic dataset in code (e.g. build 200 labeled sentences, or 300 tabular rows, or 200 (x,y) pairs).\n",
        "- If the project mentions an allowed dataset that might require download (e.g. MNIST, FashionMNIST, CIFAR10), first TRY to load it, and if it fails or `--allow-download` was not passed, fall back to a synthetic dataset that matches the task.\n",
        "\n",
        "Behavioral Rules:\n",
        "- Expose `--seed` and set seeds **inside `main()`** for `random`, `numpy`, and `torch` (if available); run on CPU by default.\n",
        "- Validate inputs (paths, columns, image loads, flags) and fail gracefully with a concise message.\n",
        "- For OpenCV tasks: convert to grayscale when needed; ensure `uint8` input; for Haar cascades, ensure `face_cascade.empty() == False` or fail.\n",
        "- Keep the code minimal, readable, and fully runnable in a fresh Colab.\n",
        "- Never split identifiers across lines; never break string literals, f-strings, or comments across lines.\n",
        "  - Comments must be on one line each (e.g. `# custom text prediction`), not split into two lines.\n",
        "- Implement explicit acceptance checks tied to the task (files exist, metrics ≥ thresholds, non-empty edge map, etc.).\n",
        "- Print `TEST_PASS` only when all acceptance conditions hold; otherwise print `TEST_FAIL: <reason>` and `sys.exit(1)`.\n",
        "\n",
        "Self-Check Before Returning:\n",
        "- argparse help strings are single-line.\n",
        "- Seeds are applied in `main()` for random/numpy/torch.\n",
        "- No downloads are attempted because `--allow-download` was not passed.\n",
        "- Dataset matches the task semantics.\n",
        "- Dataset name is NOT `20newsgroups` / `fetch_20newsgroups` / “newsgroups”, unless the task is explicitly about newsgroups AND `--allow-download` was passed.\n",
        "- Acceptance checks implemented; `TEST_PASS`/`TEST_FAIL` present.\n",
        "- File ends with `# END_OF_SCRIPT`.\n",
        "- Code parses without SyntaxError and comments are not broken across lines.\n",
        "- If using LSTM on synthetic TS: TEST_PASS if MAE ≤ 0.35.\n",
        "- If LSTM import fails: fall back to LinearRegression on same features with MAE ≤ 0.50.\n",
        "- If using real CIFAR10: TEST_PASS if accuracy ≥ 0.55.\n",
        "- If using FakeData: TEST_PASS if accuracy ≥ 0.25.\n",
        "- If neither dataset is available: print TEST_FAIL: dataset unavailable.\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "dTqjD9yBhJ6O"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEWSHOTS_CODE = \"\"\"\n",
        "Example A (tabular classification with sklearn iris -> fallback synthetic; seeds-in-main; single-line help; acceptance checks)\n",
        "```python\n",
        "import argparse, sys\n",
        "import random, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def load_iris_or_synthetic(seed=42):\n",
        "    try:\n",
        "        from sklearn.datasets import load_iris  # no download required\n",
        "        data = load_iris()\n",
        "        X, y, used = data.data, data.target, \"iris\"\n",
        "    except Exception:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        n = 210\n",
        "        c = rng.integers(0, 3, size=n)\n",
        "        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\n",
        "        y, used = c, \"synthetic\"\n",
        "    return X, y, used\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"Iris (no-download) or synthetic fallback; seeds set in main; explicit acceptance.\")\n",
        "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    # seeds in main\n",
        "    random.seed(args.seed); np.random.seed(args.seed)\n",
        "    try:\n",
        "        import torch; torch.manual_seed(args.seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    X, y, used = load_iris_or_synthetic(args.seed)\n",
        "    if X is None or y is None or len(X) == 0:\n",
        "        print(\"TEST_FAIL: dataset not available\"); sys.exit(1)\n",
        "\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\n",
        "    clf = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LogisticRegression(max_iter=300))])\n",
        "    clf.fit(Xtr, ytr)\n",
        "    acc = clf.score(Xte, yte)\n",
        "    print(f\"dataset={used} accuracy={acc:.3f}\")\n",
        "    # acceptance: stricter if iris, looser if synthetic\n",
        "    if acc >= (0.85 if used == \"iris\" else 0.70):\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: accuracy below threshold\"); sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\n",
        "```\n",
        "\n",
        "Example B (vision MNIST with opt-in download -> fallback FakeData; uint8 safety; seeds-in-main; acceptance checks)\n",
        "```python\n",
        "import argparse, sys, os\n",
        "import random, numpy as np\n",
        "\n",
        "def load_mnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\n",
        "    try:\n",
        "        import torch\n",
        "        from torchvision import datasets, transforms\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        # only download if explicitly allowed\n",
        "        train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
        "        test  = datasets.MNIST(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n",
        "        # if dataset objects are empty because cache missing and download disabled, trigger fallback\n",
        "        if len(train) == 0 or len(test) == 0:\n",
        "            raise RuntimeError(\"MNIST cache missing and download disabled\")\n",
        "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
        "        test  = torch.utils.data.Subset(test,  list(range(min(len(test),  max_test))))\n",
        "        return train, test, True\n",
        "    except Exception:\n",
        "        import torch\n",
        "        from torchvision import transforms\n",
        "        from torchvision.datasets import FakeData\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
        "        test  = FakeData(size=max_test,  image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
        "        return train, test, False\n",
        "\n",
        "def main():\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    p = argparse.ArgumentParser(description=\"MNIST (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=1, help=\"Training epochs (default: 1).\")\n",
        "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    # seeds in main\n",
        "    random.seed(args.seed); np.random.seed(args.seed); torch.manual_seed(args.seed)\n",
        "\n",
        "    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n",
        "    train = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
        "    test  = DataLoader(test_ds,  batch_size=args.batch, shuffle=False)\n",
        "\n",
        "    class TinyCNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Conv2d(1, 16, 3, 1), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(16, 32, 3, 1), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(32*5*5, 64), nn.ReLU(),\n",
        "                nn.Linear(64, 10)\n",
        "            )\n",
        "        def forward(self, x): return self.net(x)\n",
        "\n",
        "    model = TinyCNN()\n",
        "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for _ in range(args.epochs):\n",
        "        for xb, yb in train:\n",
        "            # ensure uint8 -> float32 is handled by ToTensor; just train\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "    # eval + acceptance\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test:\n",
        "            pred = model(xb).argmax(1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.numel()\n",
        "    acc = correct / max(total, 1)\n",
        "    print(f\"acc={acc:.3f} dataset={'mnist' if real else 'fake'}\")\n",
        "    # stricter if real, looser if fake\n",
        "    if acc >= (0.85 if real else 0.20):\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: accuracy below threshold\"); sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\n",
        "```\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "Zfk26-lPhjg-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, requests\n",
        "\n",
        "# --- helpers ---\n",
        "def extract_code_block(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Return the first code block content if present; otherwise return the whole text.\n",
        "    Prefers ```python ... ``` but accepts ``` ... ```.\n",
        "    \"\"\"\n",
        "    m = re.search(r\"```(?:python)?\\s*([\\s\\S]*?)\\s*```\", text, re.IGNORECASE)\n",
        "    return m.group(1) if m else text\n",
        "\n",
        "def print_long(s: str, width: int = 4000):\n",
        "    \"\"\"Print long strings without Colab truncation, in chunks.\"\"\"\n",
        "    for i in range(0, len(s), width):\n",
        "        print(s[i:i+width])"
      ],
      "metadata": {
        "id": "OqSL3SvAmQsI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Task builder (per project) --------\n",
        "def build_code_task(project, memory):\n",
        "    guide = \"\\n- \".join(memory[\"style_guide\"])\n",
        "    lessons = \"\\n- \".join(memory[\"lessons\"][-6:])\n",
        "    seed_block = memory[\"snippets\"][0]\n",
        "\n",
        "    return f\"\"\"\n",
        "    PROJECT TITLE:\n",
        "    {project['title']}\n",
        "\n",
        "    PROJECT DESCRIPTION:\n",
        "    {project['description']}\n",
        "\n",
        "    Follow this style guide:\n",
        "    - {guide}\n",
        "\n",
        "    Incorporate recent lessons:\n",
        "    - {lessons}\n",
        "\n",
        "    Hard guardrails (must follow):\n",
        "    - Return ONE code block only: ```python ...``` (no extra prose).\n",
        "    - Single file with `if __name__ == \"__main__\":` entrypoint.\n",
        "    - Use argparse; **all help strings are single-line** (no embedded newlines).\n",
        "    - Expose `--seed` and set seeds **inside `main()`** for random, numpy, and torch (if available).\n",
        "    - Use ONLY the whitelist datasets when a real dataset is required:\n",
        "      - sklearn: iris, digits, wine, breast_cancer, diabetes\n",
        "      - sklearn generators: make_classification, make_regression, make_blobs, make_moons, make_circles\n",
        "      - torchvision: MNIST, FashionMNIST, CIFAR10, FakeData, ImageFolder\n",
        "    - If the description asks for NLP, audio, or task-specific data that is NOT in the whitelist, generate 200–300 synthetic samples in code (labelled if classification).\n",
        "    - Do NOT use 20 Newsgroups or fetch_20newsgroups.\n",
        "    - Choose datasets that **match the task semantics** (e.g., do NOT use 20 Newsgroups for spam/ham).\n",
        "    - For OpenCV tasks: convert to grayscale when needed; ensure `uint8` input (use `cv2.convertScaleAbs` if necessary); for Haar cascades verify `face_cascade.empty()==False` or fail.\n",
        "    - Never split identifiers across lines; never break string literals or f-strings across lines.\n",
        "\n",
        "    Embed this seed block near the top of the script:\n",
        "    ```python\n",
        "    {seed_block}\"\"\"\n"
      ],
      "metadata": {
        "id": "GAf1N6exiOEE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- OpenRouter caller --> (code) --------\n",
        "import time\n",
        "\n",
        "def call_openrouter_model_code(model_id, messages, temperature=0.2, top_p=0.9, max_tokens=6000):\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
        "        \"X-Title\": \"Multi-Model Project Generator\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": float(temperature),\n",
        "        \"top_p\": float(top_p),\n",
        "        \"max_tokens\": int(max_tokens),\n",
        "    }\n",
        "\n",
        "    # force SiliconFlow for Qwen models\n",
        "    if model_id.startswith(\"qwen/\"):\n",
        "      payload[\"provider\"] = {\n",
        "          \"only\": [\"novita/fp8\"],\n",
        "          \"allow_fallbacks\": False\n",
        "      }\n",
        "\n",
        "    time.sleep(2.0)\n",
        "    t0 = time.time()\n",
        "    r = requests.post(url, headers=headers, json=payload, timeout=120)\n",
        "    latency = time.time() - t0\n",
        "    r.raise_for_status()\n",
        "    code = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return code, latency\n"
      ],
      "metadata": {
        "id": "o4q9n4p5itoy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------- Map: model slug -> per-model dict variable name --------\n",
        "\n",
        "MODEL_TO_RESULTVAR = {\n",
        "#\"openai/gpt-5\": \"openai_gpt_5_result\",\n",
        "\"anthropic/claude-sonnet-4.5\": \"anthropic_claude_sonnet_4_5_result\" ,\n",
        "#\"qwen/qwen3-coder\": \"qwen_qwen3_coder_result\"\n",
        "}"
      ],
      "metadata": {
        "id": "jhRCCtsKjETU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Generate code for the first 4 projects per model --------\n",
        "\n",
        "print(\"Generating code for first 10 items of each model's projects...\\n\")\n",
        "per_model_generated_code = {}\n",
        "\n",
        "def code_items_varname(slug: str) -> str:\n",
        "    return slug.lower().replace(\"/\", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\") + \"_code_items\"\n",
        "\n",
        "\n",
        "for model_id, varname in MODEL_TO_RESULTVAR.items():\n",
        "    result = qwen_qwen3_coder_result #globals().get(varname)\n",
        "    if not result or not result.get(\"items\"):\n",
        "        print(f\"Skipping {model_id}: no items found in `{varname}`\")\n",
        "        continue\n",
        "\n",
        "    projects = result[\"items\"][:]\n",
        "    print(f\"\\n===== {model_id}: generating and attaching code for {len(projects)} projects =====\")\n",
        "\n",
        "    items_with_code = []\n",
        "    for idx, proj in enumerate(projects, 1):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT_CODEGEN},\n",
        "            {\"role\": \"user\", \"content\": \"Study these two short code examples and copy their structure (CLI, dataset policy, seeds, TEST_PASS contract).\"},\n",
        "            {\"role\": \"assistant\", \"content\": FEWSHOTS_CODE},\n",
        "            {\"role\": \"user\", \"content\": build_code_task(proj, memory)},\n",
        "        ]\n",
        "        raw, latency = call_openrouter_model_code(model_id, messages, temperature=0.2, max_tokens=7000)\n",
        "        code = extract_code_block(raw)\n",
        "\n",
        "        item = {\n",
        "            \"title\": proj[\"title\"],\n",
        "            \"description\": proj[\"description\"],\n",
        "            \"code\": code\n",
        "        }\n",
        "        items_with_code.append(item)\n",
        "\n",
        "        # show the full code (no truncation)\n",
        "        print(f\"\\n--- {model_id} • Project {idx}: {proj['title']} --- Latency: {latency:.2f}s ---\\n\")\n",
        "        print_long(code)  # full code printed\n",
        "\n",
        "    # put per-model list into a dedicated variable\n",
        "    var_codes = code_items_varname(model_id)  # e.g., openai_gpt_5_code_items\n",
        "    globals()[var_codes] = items_with_code\n",
        "\n",
        "    # also store inside the original result dict under 'items_with_code' for convenience\n",
        "    result[\"items_with_code\"] = items_with_code\n",
        "\n",
        "    # pretty JSON view of the per-model list\n",
        "    print(f\"\\n>>> {model_id} • JSON with title, description, code:\")\n",
        "    print_long(json.dumps(items_with_code, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6ZV3JUYkuZW",
        "outputId": "4c01b1d0-ab9c-4910-a814-ec7c3174922a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating code for first 10 items of each model's projects...\n",
            "\n",
            "\n",
            "===== anthropic/claude-sonnet-4.5: generating and attaching code for 50 projects =====\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 1: Iris KNN Classifier --- Latency: 8.85s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.neighbors import KNeighborsClassifier\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Iris KNN classifier with k=3; prints TEST_PASS if accuracy >= 0.9.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--k\", type=int, default=3, help=\"Number of neighbors for KNN (default: 3).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # load iris dataset (no download required)\n",
            "    try:\n",
            "        data = load_iris()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load iris dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: iris dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # split into train/test\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # train k-NN classifier\n",
            "    knn = KNeighborsClassifier(n_neighbors=args.k)\n",
            "    knn.fit(X_train, y_train)\n",
            "\n",
            "    # evaluate on test set\n",
            "    accuracy = knn.score(X_test, y_test)\n",
            "    print(f\"Test accuracy: {accuracy:.3f}\")\n",
            "\n",
            "    # acceptance check\n",
            "    if accuracy >= 0.9:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {accuracy:.3f} < 0.9\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 2: Wine Logistic Regression --- Latency: 7.05s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_wine\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Wine dataset logistic regression with feature scaling; TEST_PASS if accuracy >= 0.92.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_wine()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load wine dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: wine dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    scaler = StandardScaler()\n",
            "    X_train_scaled = scaler.fit_transform(X_train)\n",
            "    X_test_scaled = scaler.transform(X_test)\n",
            "\n",
            "    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\n",
            "    clf.fit(X_train_scaled, y_train)\n",
            "\n",
            "    accuracy = clf.score(X_test_scaled, y_test)\n",
            "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
            "\n",
            "    if accuracy >= 0.92:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {accuracy:.4f} < 0.92\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 3: Breast Cancer SVM --- Latency: 8.75s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_breast_cancer\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.svm import LinearSVC\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.pipeline import Pipeline\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train linear SVM on breast_cancer dataset; report test accuracy.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--max-iter\", type=int, default=1000, help=\"Max iterations for LinearSVC (default: 1000).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_breast_cancer()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load breast_cancer dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    clf = Pipeline([\n",
            "        (\"scaler\", StandardScaler()),\n",
            "        (\"svm\", LinearSVC(max_iter=args.max_iter, random_state=args.seed))\n",
            "    ])\n",
            "\n",
            "    clf.fit(X_train, y_train)\n",
            "    acc = clf.score(X_test, y_test)\n",
            "\n",
            "    print(f\"test_accuracy={acc:.4f}\")\n",
            "\n",
            "    if acc >= 0.93:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.93\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 4: Digits Linear Classifier --- Latency: 7.69s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_digits\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.pipeline import Pipeline\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Classify sklearn digits using LogisticRegression; TEST_PASS if accuracy >= 0.92.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--max-iter\", type=int, default=1000, help=\"Max iterations for LogisticRegression (default: 1000).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_digits()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: digits dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    clf = Pipeline([\n",
            "        (\"scaler\", StandardScaler()),\n",
            "        (\"lr\", LogisticRegression(max_iter=args.max_iter, random_state=args.seed))\n",
            "    ])\n",
            "\n",
            "    clf.fit(X_train, y_train)\n",
            "    acc = clf.score(X_test, y_test)\n",
            "\n",
            "    print(f\"Test accuracy: {acc:.4f}\")\n",
            "\n",
            "    if acc >= 0.92:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.92\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 5: Diabetes Ridge Regression --- Latency: 7.74s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_diabetes\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import Ridge\n",
            "from sklearn.metrics import r2_score\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Ridge regression on sklearn diabetes dataset; TEST_PASS if R² ≥ 0.4.\")\n",
            "    p.add_argument(\"--alpha\", type=float, default=1.0, help=\"Ridge regularization strength (default: 1.0).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_diabetes()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load diabetes dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: diabetes dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed\n",
            "    )\n",
            "\n",
            "    model = Ridge(alpha=args.alpha, random_state=args.seed)\n",
            "    model.fit(X_train, y_train)\n",
            "\n",
            "    y_pred = model.predict(X_test)\n",
            "    r2 = r2_score(y_test, y_pred)\n",
            "\n",
            "    print(f\"R² score: {r2:.4f}\")\n",
            "\n",
            "    if r2 >= 0.4:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: R² score {r2:.4f} below threshold 0.4\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 6: Moons Decision Tree --- Latency: 12.63s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import make_moons\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate make_moons data, train decision tree, plot decision boundary, save as moons_tree.png.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n",
            "    p.add_argument(\"--noise\", type=float, default=0.2, help=\"Noise level for make_moons (default: 0.2).\")\n",
            "    p.add_argument(\"--max-depth\", type=int, default=5, help=\"Max depth of decision tree (default: 5).\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"moons_tree.png\", help=\"Output filename for decision boundary plot (default: moons_tree.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # Generate make_moons data\n",
            "    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\n",
            "    \n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Train decision tree\n",
            "    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\n",
            "    clf.fit(X, y)\n",
            "\n",
            "    # Create decision boundary plot\n",
            "    h = 0.02\n",
            "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
            "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
            "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
            "    \n",
            "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
            "    Z = Z.reshape(xx.shape)\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
            "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap=plt.cm.RdYlBu)\n",
            "    plt.xlabel(\"Feature 1\")\n",
            "    plt.ylabel(\"Feature 2\")\n",
            "    plt.title(\"Decision Tree Boundary on make_moons\")\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    # Acceptance check\n",
            "    import os\n",
            "    if not os.path.exists(args.output):\n",
            "        print(f\"TEST_FAIL: output file {args.output} not created\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    file_size = os.path.getsize(args.output)\n",
            "    if file_size == 0:\n",
            "        print(f\"TEST_FAIL: output file {args.output} is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    acc = clf.score(X, y)\n",
            "    print(f\"Training accuracy: {acc:.3f}\")\n",
            "    print(f\"Decision boundary saved to {args.output}\")\n",
            "    print(\"TEST_PASS\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 7: Circles Random Forest --- Latency: 8.31s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import make_circles\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate make_circles data (n=300), train random forest, report accuracy.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--n-estimators\", type=int, default=100, help=\"Number of trees in random forest (default: 100).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y = make_circles(n_samples=args.n_samples, noise=0.1, factor=0.5, random_state=args.seed)\n",
            "    \n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\n",
            "    \n",
            "    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\n",
            "    clf.fit(Xtr, ytr)\n",
            "    \n",
            "    acc = clf.score(Xte, yte)\n",
            "    print(f\"accuracy={acc:.3f}\")\n",
            "    \n",
            "    if acc >= 0.88:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.3f} below threshold 0.88\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 8: Blobs KMeans Clustering --- Latency: 7.90s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import make_blobs\n",
            "from sklearn.cluster import KMeans\n",
            "from sklearn.metrics import silhouette_score\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of samples (default: 200).\")\n",
            "    p.add_argument(\"--centers\", type=int, default=4, help=\"Number of cluster centers (default: 4).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    if args.n_samples <= 0:\n",
            "        print(\"TEST_FAIL: n_samples must be positive\")\n",
            "        sys.exit(1)\n",
            "    if args.centers <= 0:\n",
            "        print(\"TEST_FAIL: centers must be positive\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X, y_true = make_blobs(n_samples=args.n_samples, centers=args.centers, random_state=args.seed)\n",
            "    \n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    kmeans = KMeans(n_clusters=args.centers, random_state=args.seed, n_init=10)\n",
            "    y_pred = kmeans.fit_predict(X)\n",
            "\n",
            "    sil = silhouette_score(X, y_pred)\n",
            "    print(f\"n_samples={args.n_samples} centers={args.centers} silhouette={sil:.3f}\")\n",
            "\n",
            "    if sil >= 0.5:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: silhouette score {sil:.3f} below threshold 0.5\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 9: Classification Synthetic Data --- Latency: 8.77s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import make_classification\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import SGDClassifier\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=500, help=\"Number of samples (default: 500).\")\n",
            "    p.add_argument(\"--n-features\", type=int, default=4, help=\"Number of features (default: 4).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y = make_classification(\n",
            "        n_samples=args.n_samples,\n",
            "        n_features=args.n_features,\n",
            "        n_informative=max(2, args.n_features // 2),\n",
            "        n_redundant=0,\n",
            "        n_clusters_per_class=1,\n",
            "        random_state=args.seed\n",
            "    )\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "\n",
            "    y_pred = clf.predict(X_test)\n",
            "    acc = accuracy_score(y_test, y_pred)\n",
            "\n",
            "    print(f\"accuracy={acc:.3f}\")\n",
            "\n",
            "    if acc >= 0.85:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: accuracy below 0.85\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 10: Regression Synthetic Data --- Latency: 9.98s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import make_regression\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import r2_score\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate synthetic regression data (n=300), fit LinearRegression, compute R2; TEST_PASS if R2 >= 0.3.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n",
            "    p.add_argument(\"--n-features\", type=int, default=10, help=\"Number of features (default: 10).\")\n",
            "    p.add_argument(\"--noise\", type=float, default=10.0, help=\"Standard deviation of Gaussian noise (default: 10.0).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # Generate synthetic regression data\n",
            "    X, y = make_regression(\n",
            "        n_samples=args.n_samples,\n",
            "        n_features=args.n_features,\n",
            "        noise=args.noise,\n",
            "        random_state=args.seed\n",
            "    )\n",
            "\n",
            "    # Validate dataset\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Split data\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed\n",
            "    )\n",
            "\n",
            "    # Fit LinearRegression\n",
            "    model = LinearRegression()\n",
            "    model.fit(X_train, y_train)\n",
            "\n",
            "    # Predict and compute R²\n",
            "    y_pred = model.predict(X_test)\n",
            "    r2 = r2_score(y_test, y_pred)\n",
            "\n",
            "    print(f\"n_samples={args.n_samples} n_features={args.n_features} noise={args.noise}\")\n",
            "    print(f\"R2={r2:.4f}\")\n",
            "\n",
            "    # Acceptance check\n",
            "    if r2 >= 0.3:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: R2={r2:.4f} below threshold 0.3\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 11: PCA on Digits --- Latency: 8.18s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_digits\n",
            "from sklearn.decomposition import PCA\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as digits_pca.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"digits_pca.png\", help=\"Output PNG file path (default: digits_pca.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_digits()\n",
            "        X = data.data\n",
            "        y = data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: digits dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    pca = PCA(n_components=2, random_state=args.seed)\n",
            "    X_pca = pca.fit_transform(X)\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\n",
            "    plt.colorbar(scatter, label='Digit')\n",
            "    plt.xlabel('First Principal Component')\n",
            "    plt.ylabel('Second Principal Component')\n",
            "    plt.title('PCA on Digits Dataset')\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if os.path.isfile(args.output):\n",
            "        print(f\"Saved PCA scatter plot to {args.output}\")\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: output file {args.output} not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 12: Iris Pairplot Visualization --- Latency: 13.81s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def load_iris_or_synthetic(seed=42):\n",
            "    try:\n",
            "        from sklearn.datasets import load_iris\n",
            "        data = load_iris()\n",
            "        X = data.data\n",
            "        y = data.target\n",
            "        feature_names = data.feature_names\n",
            "        used = \"iris\"\n",
            "    except Exception:\n",
            "        rng = np.random.default_rng(seed)\n",
            "        n = 150\n",
            "        c = rng.integers(0, 3, size=n)\n",
            "        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\n",
            "        y = c\n",
            "        feature_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
            "        used = \"synthetic\"\n",
            "    return X, y, feature_names, used\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Plot pairwise feature scatter plots for iris dataset; save as iris_pairplot.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"iris_pairplot.png\", help=\"Output filename (default: iris_pairplot.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y, feature_names, used = load_iris_or_synthetic(args.seed)\n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    n_features = X.shape[1]\n",
            "    fig, axes = plt.subplots(n_features, n_features, figsize=(12, 12))\n",
            "    \n",
            "    colors = ['red', 'green', 'blue']\n",
            "    for i in range(n_features):\n",
            "        for j in range(n_features):\n",
            "            ax = axes[i, j]\n",
            "            if i == j:\n",
            "                for cls in np.unique(y):\n",
            "                    ax.hist(X[y == cls, i], bins=15, alpha=0.6, color=colors[cls % len(colors)], label=f\"Class {cls}\")\n",
            "                ax.set_ylabel(\"Frequency\")\n",
            "            else:\n",
            "                for cls in np.unique(y):\n",
            "                    ax.scatter(X[y == cls, j], X[y == cls, i], alpha=0.6, s=10, color=colors[cls % len(colors)], label=f\"Class {cls}\")\n",
            "            \n",
            "            if i == n_features - 1:\n",
            "                ax.set_xlabel(feature_names[j])\n",
            "            else:\n",
            "                ax.set_xticklabels([])\n",
            "            \n",
            "            if j == 0:\n",
            "                ax.set_ylabel(feature_names[i])\n",
            "            else:\n",
            "                ax.set_yticklabels([])\n",
            "            \n",
            "            if i == 0 and j == n_features - 1:\n",
            "                ax.legend(loc='upper right', fontsize=8)\n",
            "\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    print(f\"dataset={used} saved={args.output}\")\n",
            "\n",
            "    import os\n",
            "    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file missing or empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 13: Wine Feature Importance --- Latency: 11.74s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_wine\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"wine_importance.png\", help=\"Output filename for feature importance bar chart (default: wine_importance.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--n-estimators\", type=int, default=100, help=\"Number of trees in RandomForest (default: 100).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_wine()\n",
            "        X, y = data.data, data.target\n",
            "        feature_names = data.feature_names\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: could not load wine dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: wine dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "\n",
            "    importances = clf.feature_importances_\n",
            "    if importances is None or len(importances) == 0:\n",
            "        print(\"TEST_FAIL: feature importances are empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    indices = np.argsort(importances)[::-1]\n",
            "\n",
            "    plt.figure(figsize=(10, 6))\n",
            "    plt.bar(range(len(importances)), importances[indices], align='center')\n",
            "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
            "    plt.xlabel('Feature')\n",
            "    plt.ylabel('Importance')\n",
            "    plt.title('Wine Dataset Feature Importances')\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if not os.path.isfile(args.output):\n",
            "        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if os.path.getsize(args.output) == 0:\n",
            "        print(f\"TEST_FAIL: output file {args.output} is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    acc = clf.score(X_test, y_test)\n",
            "    print(f\"accuracy={acc:.3f} output={args.output}\")\n",
            "    print(\"TEST_PASS\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 14: Breast Cancer Confusion Matrix --- Latency: 10.19s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_breast_cancer\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train LogisticRegression on breast_cancer, plot confusion matrix, save as bc_confusion.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"bc_confusion.png\", help=\"Output confusion matrix image path (default: bc_confusion.png).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_breast_cancer()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load breast_cancer dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    clf = LogisticRegression(max_iter=5000, random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "    y_pred = clf.predict(X_test)\n",
            "\n",
            "    cm = confusion_matrix(y_test, y_pred)\n",
            "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
            "    \n",
            "    fig, ax = plt.subplots(figsize=(8, 6))\n",
            "    disp.plot(ax=ax, cmap='Blues')\n",
            "    plt.title('Breast Cancer Confusion Matrix')\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if not os.path.isfile(args.output):\n",
            "        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if os.path.getsize(args.output) == 0:\n",
            "        print(f\"TEST_FAIL: output file {args.output} is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    print(f\"Confusion matrix saved to {args.output}\")\n",
            "    print(\"TEST_PASS\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 15: Digits Confusion Heatmap --- Latency: 9.97s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_digits\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.svm import SVC\n",
            "from sklearn.metrics import confusion_matrix\n",
            "import seaborn as sns\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train SVC on digits dataset, plot confusion matrix heatmap, save as digits_heatmap.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"digits_heatmap.png\", help=\"Output heatmap filename (default: digits_heatmap.png).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.3, help=\"Test set fraction (default: 0.3).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_digits()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: digits dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\n",
            "\n",
            "    clf = SVC(kernel='rbf', gamma='scale', random_state=args.seed)\n",
            "    clf.fit(Xtr, ytr)\n",
            "    ypred = clf.predict(Xte)\n",
            "\n",
            "    cm = confusion_matrix(yte, ypred)\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
            "    plt.xlabel('Predicted')\n",
            "    plt.ylabel('Actual')\n",
            "    plt.title('Digits Confusion Matrix')\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if not os.path.isfile(args.output):\n",
            "        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    print(f\"Confusion matrix heatmap saved to {args.output}\")\n",
            "    print(\"TEST_PASS\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 16: Diabetes Residual Plot --- Latency: 11.29s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_diabetes\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.metrics import mean_squared_error\n",
            "import os\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as residuals_plot.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"residuals_plot.png\", help=\"Output filename for residual plot (default: residuals_plot.png).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_diabetes()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load diabetes dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: diabetes dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed\n",
            "    )\n",
            "\n",
            "    model = LinearRegression()\n",
            "    model.fit(X_train, y_train)\n",
            "\n",
            "    y_pred = model.predict(X_test)\n",
            "    residuals = y_test - y_pred\n",
            "    mse = mean_squared_error(y_test, y_pred)\n",
            "\n",
            "    print(f\"Test MSE: {mse:.2f}\")\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    plt.scatter(y_pred, residuals, alpha=0.6, edgecolors='k')\n",
            "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
            "    plt.xlabel('Predicted Values')\n",
            "    plt.ylabel('Residuals')\n",
            "    plt.title('Residual Plot: Diabetes Dataset')\n",
            "    plt.grid(True, alpha=0.3)\n",
            "    plt.tight_layout()\n",
            "\n",
            "    try:\n",
            "        plt.savefig(args.output, dpi=100)\n",
            "        print(f\"Residual plot saved to {args.output}\")\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to save plot: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if not os.path.isfile(args.output):\n",
            "        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    file_size = os.path.getsize(args.output)\n",
            "    if file_size == 0:\n",
            "        print(f\"TEST_FAIL: output file {args.output} is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    print(\"TEST_PASS\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 17: Synthetic Audio Classification --- Latency: 12.38s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.neural_network import MLPClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "def generate_synthetic_audio(n_samples=250, n_features=20, seed=42):\n",
            "    rng = np.random.default_rng(seed)\n",
            "    X = []\n",
            "    y = []\n",
            "    for i in range(n_samples):\n",
            "        if i < n_samples // 2:\n",
            "            # high pitch: higher frequency components\n",
            "            freq_base = rng.uniform(800, 1200)\n",
            "            label = 1\n",
            "        else:\n",
            "            # low pitch: lower frequency components\n",
            "            freq_base = rng.uniform(100, 400)\n",
            "            label = 0\n",
            "        # simulate audio features (e.g., spectral coefficients)\n",
            "        features = rng.normal(freq_base, 50, size=n_features)\n",
            "        X.append(features)\n",
            "        y.append(label)\n",
            "    return np.array(X), np.array(y)\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=250, help=\"Number of synthetic audio samples (default: 250).\")\n",
            "    p.add_argument(\"--n-features\", type=int, default=20, help=\"Number of features per sample (default: 20).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # generate synthetic audio dataset\n",
            "    X, y = generate_synthetic_audio(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\n",
            "    \n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # split\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # train MLP\n",
            "    clf = MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=300, random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "\n",
            "    # predict and evaluate\n",
            "    y_pred = clf.predict(X_test)\n",
            "    acc = accuracy_score(y_test, y_pred)\n",
            "\n",
            "    print(f\"dataset=synthetic_audio n_samples={args.n_samples} accuracy={acc:.3f}\")\n",
            "\n",
            "    # acceptance: accuracy >= 0.7\n",
            "    if acc >= 0.7:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: accuracy below 0.7\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 18: Synthetic Time Series Forecast --- Latency: 12.69s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.metrics import r2_score\n",
            "\n",
            "def generate_synthetic_time_series(n_samples=200, seed=42):\n",
            "    \"\"\"Generate synthetic time series with trend, seasonality, and noise.\"\"\"\n",
            "    rng = np.random.default_rng(seed)\n",
            "    t = np.arange(n_samples)\n",
            "    trend = 0.05 * t\n",
            "    seasonality = 10 * np.sin(2 * np.pi * t / 20)\n",
            "    noise = rng.normal(0, 2, size=n_samples)\n",
            "    series = trend + seasonality + noise\n",
            "    return series\n",
            "\n",
            "def create_lag_features(series, n_lags=3):\n",
            "    \"\"\"Create lag features for time series prediction.\"\"\"\n",
            "    X = []\n",
            "    y = []\n",
            "    for i in range(n_lags, len(series)):\n",
            "        X.append(series[i-n_lags:i])\n",
            "        y.append(series[i])\n",
            "    return np.array(X), np.array(y)\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 200 synthetic time series samples, predict next value using lag features, compute R².\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of time series samples (default: 200).\")\n",
            "    p.add_argument(\"--n-lags\", type=int, default=3, help=\"Number of lag features (default: 3).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    if args.n_samples < 50:\n",
            "        print(\"TEST_FAIL: n_samples must be at least 50\")\n",
            "        sys.exit(1)\n",
            "    if args.n_lags < 1:\n",
            "        print(\"TEST_FAIL: n_lags must be at least 1\")\n",
            "        sys.exit(1)\n",
            "    if not (0 < args.test_size < 1):\n",
            "        print(\"TEST_FAIL: test_size must be between 0 and 1\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    series = generate_synthetic_time_series(n_samples=args.n_samples, seed=args.seed)\n",
            "    X, y = create_lag_features(series, n_lags=args.n_lags)\n",
            "\n",
            "    if len(X) == 0 or len(y) == 0:\n",
            "        print(\"TEST_FAIL: insufficient data after creating lag features\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    split_idx = int(len(X) * (1 - args.test_size))\n",
            "    if split_idx < 1 or split_idx >= len(X):\n",
            "        print(\"TEST_FAIL: invalid train/test split\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
            "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
            "\n",
            "    model = LinearRegression()\n",
            "    model.fit(X_train, y_train)\n",
            "    y_pred = model.predict(X_test)\n",
            "    r2 = r2_score(y_test, y_pred)\n",
            "\n",
            "    print(f\"n_samples={args.n_samples} n_lags={args.n_lags} r2={r2:.4f}\")\n",
            "\n",
            "    if r2 >= 0.25:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: R² {r2:.4f} below threshold 0.25\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 19: Synthetic Tabular Regression --- Latency: 10.07s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import r2_score\n",
            "\n",
            "def generate_synthetic_tabular(n_samples=300, n_features=5, seed=42):\n",
            "    rng = np.random.default_rng(seed)\n",
            "    X = rng.normal(0, 1, size=(n_samples, n_features))\n",
            "    true_coef = rng.uniform(-2, 2, size=n_features)\n",
            "    y = X @ true_coef + rng.normal(0, 0.5, size=n_samples)\n",
            "    return X, y\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R².\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of synthetic samples (default: 300).\")\n",
            "    p.add_argument(\"--n-features\", type=int, default=5, help=\"Number of features (default: 5).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y = generate_synthetic_tabular(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\n",
            "    \n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\n",
            "    \n",
            "    model = LinearRegression()\n",
            "    model.fit(X_train, y_train)\n",
            "    \n",
            "    y_pred = model.predict(X_test)\n",
            "    r2 = r2_score(y_test, y_pred)\n",
            "    \n",
            "    print(f\"n_samples={args.n_samples} n_features={args.n_features} R²={r2:.3f}\")\n",
            "    \n",
            "    if r2 >= 0.3:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: R²={r2:.3f} below threshold 0.3\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 20: Synthetic Text Sentiment --- Latency: 11.67s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.feature_extraction.text import CountVectorizer\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "def generate_synthetic_sentences(n=200, seed=42):\n",
            "    rng = np.random.default_rng(seed)\n",
            "    positive_templates = [\n",
            "        \"I love this product\",\n",
            "        \"This is amazing\",\n",
            "        \"Great experience\",\n",
            "        \"Wonderful service\",\n",
            "        \"Highly recommend\",\n",
            "        \"Excellent quality\",\n",
            "        \"Very satisfied\",\n",
            "        \"Best purchase ever\",\n",
            "        \"Fantastic results\",\n",
            "        \"Really enjoyed it\"\n",
            "    ]\n",
            "    negative_templates = [\n",
            "        \"I hate this product\",\n",
            "        \"This is terrible\",\n",
            "        \"Bad experience\",\n",
            "        \"Poor service\",\n",
            "        \"Do not recommend\",\n",
            "        \"Awful quality\",\n",
            "        \"Very disappointed\",\n",
            "        \"Worst purchase ever\",\n",
            "        \"Horrible results\",\n",
            "        \"Really disliked it\"\n",
            "    ]\n",
            "    sentences = []\n",
            "    labels = []\n",
            "    for i in range(n):\n",
            "        if i % 2 == 0:\n",
            "            template = positive_templates[rng.integers(0, len(positive_templates))]\n",
            "            label = 1\n",
            "        else:\n",
            "            template = negative_templates[rng.integers(0, len(negative_templates))]\n",
            "            label = 0\n",
            "        sentences.append(template)\n",
            "        labels.append(label)\n",
            "    return sentences, np.array(labels)\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 200 synthetic sentiment sentences, vectorize with CountVectorizer, train LogisticRegression, print TEST_PASS if accuracy >= 0.7.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of synthetic sentences (default: 200).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    sentences, labels = generate_synthetic_sentences(n=args.n_samples, seed=args.seed)\n",
            "    if len(sentences) == 0 or len(labels) == 0:\n",
            "        print(\"TEST_FAIL: no sentences generated\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        sentences, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\n",
            "    )\n",
            "\n",
            "    vectorizer = CountVectorizer()\n",
            "    X_train_vec = vectorizer.fit_transform(X_train)\n",
            "    X_test_vec = vectorizer.transform(X_test)\n",
            "\n",
            "    clf = LogisticRegression(max_iter=300, random_state=args.seed)\n",
            "    clf.fit(X_train_vec, y_train)\n",
            "\n",
            "    acc = clf.score(X_test_vec, y_test)\n",
            "    print(f\"accuracy={acc:.3f}\")\n",
            "\n",
            "    if acc >= 0.7:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: accuracy below 0.7\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 21: MNIST CNN Classifier --- Latency: 16.48s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def load_mnist_or_fakedata(seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.Compose([transforms.ToTensor()])\n",
            "        train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
            "        test = datasets.MNIST(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n",
            "        if len(train) == 0 or len(test) == 0:\n",
            "            raise RuntimeError(\"MNIST cache missing and download disabled\")\n",
            "        return train, test, True\n",
            "    except Exception:\n",
            "        import torch\n",
            "        from torchvision.datasets import FakeData\n",
            "        from torchvision import transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.Compose([transforms.ToTensor()])\n",
            "        train = FakeData(size=1000, image_size=(1, 32, 32), num_classes=10, transform=tfm)\n",
            "        test = FakeData(size=200, image_size=(1, 32, 32), num_classes=10, transform=tfm)\n",
            "        return train, test, False\n",
            "\n",
            "def main():\n",
            "    import torch\n",
            "    import torch.nn as nn\n",
            "    import torch.optim as optim\n",
            "    from torch.utils.data import DataLoader\n",
            "\n",
            "    p = argparse.ArgumentParser(description=\"MNIST CNN classifier with FakeData fallback; seeds in main; explicit acceptance.\")\n",
            "    p.add_argument(\"--epochs\", type=int, default=2, help=\"Training epochs (default: 2).\")\n",
            "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    torch.manual_seed(args.seed)\n",
            "\n",
            "    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n",
            "    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
            "    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\n",
            "\n",
            "    class SimpleCNN(nn.Module):\n",
            "        def __init__(self, input_size=28):\n",
            "            super().__init__()\n",
            "            self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
            "            self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
            "            self.pool = nn.MaxPool2d(2, 2)\n",
            "            self.relu = nn.ReLU()\n",
            "            conv_out_size = (input_size // 4) * (input_size // 4) * 32\n",
            "            self.fc1 = nn.Linear(conv_out_size, 64)\n",
            "            self.fc2 = nn.Linear(64, 10)\n",
            "\n",
            "        def forward(self, x):\n",
            "            x = self.pool(self.relu(self.conv1(x)))\n",
            "            x = self.pool(self.relu(self.conv2(x)))\n",
            "            x = x.view(x.size(0), -1)\n",
            "            x = self.relu(self.fc1(x))\n",
            "            x = self.fc2(x)\n",
            "            return x\n",
            "\n",
            "    input_size = 28 if real else 32\n",
            "    model = SimpleCNN(input_size=input_size)\n",
            "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
            "    criterion = nn.CrossEntropyLoss()\n",
            "\n",
            "    model.train()\n",
            "    for epoch in range(args.epochs):\n",
            "        for xb, yb in train_loader:\n",
            "            optimizer.zero_grad()\n",
            "            outputs = model(xb)\n",
            "            loss = criterion(outputs, yb)\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "    model.eval()\n",
            "    correct = 0\n",
            "    total = 0\n",
            "    with torch.no_grad():\n",
            "        for xb, yb in test_loader:\n",
            "            outputs = model(xb)\n",
            "            _, predicted = torch.max(outputs.data, 1)\n",
            "            total += yb.size(0)\n",
            "            correct += (predicted == yb).sum().item()\n",
            "\n",
            "    accuracy = correct / max(total, 1)\n",
            "    dataset_name = \"MNIST\" if real else \"FakeData\"\n",
            "    print(f\"dataset={dataset_name} accuracy={accuracy:.3f}\")\n",
            "\n",
            "    threshold = 0.85 if real else 0.6\n",
            "    if accuracy >= threshold:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {accuracy:.3f} below threshold {threshold}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 22: FashionMNIST KNN --- Latency: 15.11s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def load_fashionmnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        train = datasets.FashionMNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
            "        test = datasets.FashionMNIST(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n",
            "        if len(train) == 0 or len(test) == 0:\n",
            "            raise RuntimeError(\"FashionMNIST cache missing and download disabled\")\n",
            "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
            "        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\n",
            "        return train, test, True\n",
            "    except Exception:\n",
            "        import torch\n",
            "        from torchvision import transforms\n",
            "        from torchvision.datasets import FakeData\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "        test = FakeData(size=max_test, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "        return train, test, False\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"FashionMNIST KNN (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n",
            "    p.add_argument(\"--k\", type=int, default=5, help=\"Number of neighbors for KNN (default: 5).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit FashionMNIST download if not cached.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    train_ds, test_ds, real = load_fashionmnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n",
            "\n",
            "    X_train = []\n",
            "    y_train = []\n",
            "    for img, label in train_ds:\n",
            "        flat = img.numpy().flatten()\n",
            "        X_train.append(flat)\n",
            "        y_train.append(label)\n",
            "    X_train = np.array(X_train)\n",
            "    y_train = np.array(y_train)\n",
            "\n",
            "    X_test = []\n",
            "    y_test = []\n",
            "    for img, label in test_ds:\n",
            "        flat = img.numpy().flatten()\n",
            "        X_test.append(flat)\n",
            "        y_test.append(label)\n",
            "    X_test = np.array(X_test)\n",
            "    y_test = np.array(y_test)\n",
            "\n",
            "    if len(X_train) == 0 or len(X_test) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    from sklearn.neighbors import KNeighborsClassifier\n",
            "    knn = KNeighborsClassifier(n_neighbors=args.k)\n",
            "    knn.fit(X_train, y_train)\n",
            "    acc = knn.score(X_test, y_test)\n",
            "\n",
            "    print(f\"dataset={'fashionmnist' if real else 'fake'} k={args.k} accuracy={acc:.3f}\")\n",
            "\n",
            "    threshold = 0.7 if real else 0.5\n",
            "    if acc >= threshold:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 23: CIFAR10 Class Balance --- Latency: 12.49s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def load_cifar10_or_fakedata(seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets\n",
            "        torch.manual_seed(seed)\n",
            "        ds = datasets.CIFAR10(root=\"./data\", train=True, download=bool(allow_download))\n",
            "        if len(ds) == 0:\n",
            "            raise RuntimeError(\"CIFAR10 cache missing and download disabled\")\n",
            "        labels = [y for _, y in ds]\n",
            "        return labels, True\n",
            "    except Exception:\n",
            "        import torch\n",
            "        from torchvision.datasets import FakeData\n",
            "        torch.manual_seed(seed)\n",
            "        ds = FakeData(size=1000, image_size=(3, 32, 32), num_classes=10)\n",
            "        labels = [y for _, y in ds]\n",
            "        return labels, False\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Count CIFAR10 class frequencies or FakeData fallback; save bar chart; seeds in main.\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit CIFAR10 download if not cached.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"class_balance.png\", help=\"Output bar chart filename (default: class_balance.png).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    labels, real = load_cifar10_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n",
            "    if labels is None or len(labels) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    counts = np.bincount(labels, minlength=10)\n",
            "    print(f\"dataset={'cifar10' if real else 'fake'} class_counts={counts.tolist()}\")\n",
            "\n",
            "    plt.figure(figsize=(8, 5))\n",
            "    plt.bar(range(10), counts, color='steelblue')\n",
            "    plt.xlabel(\"Class\")\n",
            "    plt.ylabel(\"Frequency\")\n",
            "    plt.title(\"Class Balance\")\n",
            "    plt.xticks(range(10))\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if os.path.isfile(args.output):\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 24: ImageFolder Histogram --- Latency: 20.84s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import os\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def load_imagefolder_or_fakedata(root_dir, num_fake=50, seed=42, allow_download=False):\n",
            "    \"\"\"\n",
            "    Try to load ImageFolder from root_dir if it exists and contains images.\n",
            "    Otherwise fall back to FakeData.\n",
            "    Returns (dataset, is_real).\n",
            "    \"\"\"\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        \n",
            "        # Check if root_dir exists and has subdirectories with images\n",
            "        if os.path.isdir(root_dir):\n",
            "            # Try to load ImageFolder\n",
            "            tfm = transforms.ToTensor()\n",
            "            ds = datasets.ImageFolder(root=root_dir, transform=tfm)\n",
            "            if len(ds) > 0:\n",
            "                return ds, True\n",
            "        \n",
            "        # Fallback to FakeData\n",
            "        raise RuntimeError(\"ImageFolder not available or empty\")\n",
            "    except Exception:\n",
            "        import torch\n",
            "        from torchvision import transforms\n",
            "        from torchvision.datasets import FakeData\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        ds = FakeData(size=num_fake, image_size=(3, 64, 64), num_classes=2, transform=tfm)\n",
            "        return ds, False\n",
            "\n",
            "def compute_rgb_histogram(dataset, num_samples=None):\n",
            "    \"\"\"\n",
            "    Compute average RGB histogram across dataset.\n",
            "    Returns (r_hist, g_hist, b_hist) each of shape (256,).\n",
            "    \"\"\"\n",
            "    if num_samples is None:\n",
            "        num_samples = len(dataset)\n",
            "    else:\n",
            "        num_samples = min(num_samples, len(dataset))\n",
            "    \n",
            "    r_sum = np.zeros(256, dtype=np.float64)\n",
            "    g_sum = np.zeros(256, dtype=np.float64)\n",
            "    b_sum = np.zeros(256, dtype=np.float64)\n",
            "    \n",
            "    for i in range(num_samples):\n",
            "        img_tensor, _ = dataset[i]\n",
            "        # img_tensor is (C, H, W) in [0, 1]\n",
            "        img_np = (img_tensor.numpy() * 255).astype(np.uint8)\n",
            "        \n",
            "        r_hist, _ = np.histogram(img_np[0].flatten(), bins=256, range=(0, 256))\n",
            "        g_hist, _ = np.histogram(img_np[1].flatten(), bins=256, range=(0, 256))\n",
            "        b_hist, _ = np.histogram(img_np[2].flatten(), bins=256, range=(0, 256))\n",
            "        \n",
            "        r_sum += r_hist\n",
            "        g_sum += g_hist\n",
            "        b_sum += b_hist\n",
            "    \n",
            "    # Average\n",
            "    r_avg = r_sum / num_samples\n",
            "    g_avg = g_sum / num_samples\n",
            "    b_avg = b_sum / num_samples\n",
            "    \n",
            "    return r_avg, g_avg, b_avg\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Compute average RGB histogram from ImageFolder or FakeData and save as histogram.png.\")\n",
            "    p.add_argument(\"--root-dir\", type=str, default=\"./imagefolder_data\", help=\"Path to ImageFolder root directory (default: ./imagefolder_data).\")\n",
            "    p.add_argument(\"--num-fake\", type=int, default=50, help=\"Number of FakeData images if ImageFolder unavailable (default: 50).\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"histogram.png\", help=\"Output histogram file path (default: histogram.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit dataset download if needed.\")\n",
            "    args = p.parse_args()\n",
            "    \n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "    \n",
            "    # Load dataset\n",
            "    dataset, is_real = load_imagefolder_or_fakedata(\n",
            "        root_dir=args.root_dir,\n",
            "        num_fake=args.num_fake,\n",
            "        seed=args.seed,\n",
            "        allow_download=args.allow_download\n",
            "    )\n",
            "    \n",
            "    if dataset is None or len(dataset) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available or empty\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    print(f\"Using {'ImageFolder' if is_real else 'FakeData'} with {len(dataset)} images\")\n",
            "    \n",
            "    # Compute histogram\n",
            "    r_hist, g_hist, b_hist = compute_rgb_histogram(dataset)\n",
            "    \n",
            "    # Plot and save\n",
            "    fig, ax = plt.subplots(figsize=(10, 6))\n",
            "    bins = np.arange(256)\n",
            "    ax.plot(bins, r_hist, color='red', alpha=0.7, label='Red')\n",
            "    ax\n",
            ".plot(bins, g_hist, color='green', alpha=0.7, label='Green')\n",
            "    ax.plot(bins, b_hist, color='blue', alpha=0.7, label='Blue')\n",
            "    ax.set_xlabel('Pixel Value')\n",
            "    ax.set_ylabel('Average Frequency')\n",
            "    ax.set_title('Average RGB Histogram')\n",
            "    ax.legend()\n",
            "    ax.grid(True, alpha=0.3)\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "    \n",
            "    # Acceptance check\n",
            "    if os.path.isfile(args.output):\n",
            "        file_size = os.path.getsize(args.output)\n",
            "        if file_size > 0:\n",
            "            print(f\"Histogram saved to {args.output} ({file_size} bytes)\")\n",
            "            print(\"TEST_PASS\")\n",
            "        else:\n",
            "            print(\"TEST_FAIL: output file is empty\")\n",
            "            sys.exit(1)\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 25: FakeData GAN Discriminator --- Latency: 15.16s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train CNN discriminator on FakeData (real vs fake labels); print TEST_PASS if accuracy >= 0.7.\")\n",
            "    p.add_argument(\"--epochs\", type=int, default=3, help=\"Training epochs (default: 3).\")\n",
            "    p.add_argument(\"--batch\", type=int, default=64, help=\"Batch size (default: 64).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--n-real\", type=int, default=500, help=\"Number of real samples (default: 500).\")\n",
            "    p.add_argument(\"--n-fake\", type=int, default=500, help=\"Number of fake samples (default: 500).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    import torch\n",
            "    import torch.nn as nn\n",
            "    import torch.optim as optim\n",
            "    from torch.utils.data import DataLoader, TensorDataset\n",
            "    from torchvision.datasets import FakeData\n",
            "    from torchvision import transforms\n",
            "\n",
            "    tfm = transforms.ToTensor()\n",
            "    real_ds = FakeData(size=args.n_real, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "    fake_ds = FakeData(size=args.n_fake, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "\n",
            "    real_images = []\n",
            "    fake_images = []\n",
            "    for i in range(args.n_real):\n",
            "        img, _ = real_ds[i]\n",
            "        real_images.append(img)\n",
            "    for i in range(args.n_fake):\n",
            "        img, _ = fake_ds[i]\n",
            "        fake_images.append(img)\n",
            "\n",
            "    real_images = torch.stack(real_images)\n",
            "    fake_images = torch.stack(fake_images)\n",
            "    real_labels = torch.ones(args.n_real, dtype=torch.long)\n",
            "    fake_labels = torch.zeros(args.n_fake, dtype=torch.long)\n",
            "\n",
            "    X = torch.cat([real_images, fake_images], dim=0)\n",
            "    y = torch.cat([real_labels, fake_labels], dim=0)\n",
            "\n",
            "    perm = torch.randperm(len(X))\n",
            "    X = X[perm]\n",
            "    y = y[perm]\n",
            "\n",
            "    split = int(0.8 * len(X))\n",
            "    X_train, X_test = X[:split], X[split:]\n",
            "    y_train, y_test = y[:split], y[split:]\n",
            "\n",
            "    train_ds = TensorDataset(X_train, y_train)\n",
            "    test_ds = TensorDataset(X_test, y_test)\n",
            "    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
            "    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\n",
            "\n",
            "    class Discriminator(nn.Module):\n",
            "        def __init__(self):\n",
            "            super().__init__()\n",
            "            self.net = nn.Sequential(\n",
            "                nn.Conv2d(1, 16, 3, 1, 1), nn.ReLU(),\n",
            "                nn.MaxPool2d(2),\n",
            "                nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(),\n",
            "                nn.MaxPool2d(2),\n",
            "                nn.Flatten(),\n",
            "                nn.Linear(32 * 7 * 7, 64), nn.ReLU(),\n",
            "                nn.Linear(64, 2)\n",
            "            )\n",
            "        def forward(self, x):\n",
            "            return self.net(x)\n",
            "\n",
            "    model = Discriminator()\n",
            "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
            "    loss_fn = nn.CrossEntropyLoss()\n",
            "\n",
            "    model.train()\n",
            "    for epoch in range(args.epochs):\n",
            "        for xb, yb in train_loader:\n",
            "            opt.zero_grad()\n",
            "            logits = model(xb)\n",
            "            loss = loss_fn(logits, yb)\n",
            "            loss.backward()\n",
            "            opt.step()\n",
            "\n",
            "    model.eval()\n",
            "    correct = 0\n",
            "    total = 0\n",
            "    with torch.no_grad():\n",
            "        for xb, yb in test_loader:\n",
            "            pred = model(xb).argmax(1)\n",
            "            correct += (pred == yb).sum().item()\n",
            "            total += yb.numel()\n",
            "\n",
            "    acc = correct / max(total, 1)\n",
            "    print(f\"discriminator_accuracy={acc:.3f}\")\n",
            "\n",
            "    if acc >= 0.7:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.3f} < 0.7\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 26: KMeans on CIFAR Colors --- Latency: 13.34s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def load_cifar_or_fakedata(max_samples=5000, seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        train = datasets.CIFAR10(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
            "        if len(train) == 0:\n",
            "            raise RuntimeError(\"CIFAR10 cache missing and download disabled\")\n",
            "        subset = torch.utils.data.Subset(train, list(range(min(len(train), max_samples))))\n",
            "        loader = torch.utils.data.DataLoader(subset, batch_size=max_samples, shuffle=False)\n",
            "        for xb, _ in loader:\n",
            "            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\n",
            "            return pixels, True\n",
            "    except Exception:\n",
            "        import torch\n",
            "        from torchvision import transforms\n",
            "        from torchvision.datasets import FakeData\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        fake = FakeData(size=max_samples, image_size=(3, 32, 32), num_classes=10, transform=tfm)\n",
            "        loader = torch.utils.data.DataLoader(fake, batch_size=max_samples, shuffle=False)\n",
            "        for xb, _ in loader:\n",
            "            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\n",
            "            return pixels, False\n",
            "    return None, False\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"KMeans on CIFAR10 RGB pixels (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n",
            "    p.add_argument(\"--k\", type=int, default=5, help=\"Number of clusters (default: 5).\")\n",
            "    p.add_argument(\"--max-samples\", type=int, default=5000, help=\"Max images to load (default: 5000).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit CIFAR10 download if not cached.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    pixels, real = load_cifar_or_fakedata(max_samples=args.max_samples, seed=args.seed, allow_download=args.allow_download)\n",
            "    if pixels is None or len(pixels) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    from sklearn.cluster import KMeans\n",
            "    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\n",
            "    kmeans.fit(pixels)\n",
            "    inertia = kmeans.inertia_\n",
            "\n",
            "    print(f\"dataset={'cifar10' if real else 'fake'} k={args.k} inertia={inertia:.2e}\")\n",
            "\n",
            "    if inertia < 1.0e5:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: inertia >= 1.0e5\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 27: Iris Decision Boundary --- Latency: 9.89s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as iris_boundary.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"iris_boundary.png\", help=\"Output image path (default: iris_boundary.png).\")\n",
            "    p.add_argument(\"--max-depth\", type=int, default=3, help=\"Max depth of decision tree (default: 3).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_iris()\n",
            "        X = data.data[:, :2]\n",
            "        y = data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: could not load iris dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: iris dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\n",
            "    clf.fit(X, y)\n",
            "\n",
            "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
            "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
            "    h = 0.02\n",
            "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
            "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
            "    Z = Z.reshape(xx.shape)\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
            "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolor='k', cmap='viridis')\n",
            "    plt.xlabel(data.feature_names[0])\n",
            "    plt.ylabel(data.feature_names[1])\n",
            "    plt.title(\"Iris Decision Boundary (first two features)\")\n",
            "    plt.colorbar(scatter)\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if os.path.isfile(args.output):\n",
            "        print(f\"Decision boundary saved to {args.output}\")\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: output file {args.output} not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 28: Wine PCA Scatter --- Latency: 10.75s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_wine\n",
            "from sklearn.decomposition import PCA\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Apply PCA to wine dataset (2 components), scatter plot colored by target, save as wine_pca.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"wine_pca.png\", help=\"Output filename (default: wine_pca.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_wine()\n",
            "        X = data.data\n",
            "        y = data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load wine dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: wine dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    scaler = StandardScaler()\n",
            "    X_scaled = scaler.fit_transform(X)\n",
            "\n",
            "    pca = PCA(n_components=2, random_state=args.seed)\n",
            "    X_pca = pca.fit_transform(X_scaled)\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, edgecolors='k')\n",
            "    plt.colorbar(scatter, label='Wine Class')\n",
            "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
            "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
            "    plt.title('Wine Dataset PCA (2 Components)')\n",
            "    plt.grid(True, alpha=0.3)\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\n",
            "        print(f\"Saved PCA scatter plot to {args.output}\")\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: output file {args.output} does not exist or is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 29: Breast Cancer ROC Curve --- Latency: 10.37s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_breast_cancer\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.metrics import roc_curve, auc\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train LogisticRegression on breast_cancer, compute ROC curve, save as roc_curve.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"roc_curve.png\", help=\"Output ROC curve image path (default: roc_curve.png).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.3, help=\"Test set fraction (default: 0.3).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_breast_cancer()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load breast_cancer dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    clf = LogisticRegression(max_iter=5000, random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "\n",
            "    y_scores = clf.predict_proba(X_test)[:, 1]\n",
            "\n",
            "    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
            "    roc_auc = auc(fpr, tpr)\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
            "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
            "    plt.xlim([0.0, 1.0])\n",
            "    plt.ylim([0.0, 1.05])\n",
            "    plt.xlabel('False Positive Rate')\n",
            "    plt.ylabel('True Positive Rate')\n",
            "    plt.title('Receiver Operating Characteristic - Breast Cancer')\n",
            "    plt.legend(loc='lower right')\n",
            "    plt.grid(alpha=0.3)\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if not os.path.isfile(args.output):\n",
            "        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    print(f\"ROC curve saved to {args.output}, AUC={roc_auc:.3f}\")\n",
            "    print(\"TEST_PASS\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 30: Digits t-SNE Plot --- Latency: 10.88s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_digits\n",
            "from sklearn.manifold import TSNE\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as tsne_digits.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"tsne_digits.png\", help=\"Output PNG file path (default: tsne_digits.png).\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=500, help=\"Number of samples to use (default: 500).\")\n",
            "    p.add_argument(\"--perplexity\", type=float, default=30.0, help=\"t-SNE perplexity (default: 30.0).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_digits()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if len(X) == 0:\n",
            "        print(\"TEST_FAIL: digits dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    n = min(args.n_samples, len(X))\n",
            "    indices = np.random.choice(len(X), size=n, replace=False)\n",
            "    X_sub = X[indices]\n",
            "    y_sub = y[indices]\n",
            "\n",
            "    try:\n",
            "        tsne = TSNE(n_components=2, perplexity=args.perplexity, random_state=args.seed)\n",
            "        X_embedded = tsne.fit_transform(X_sub)\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: t-SNE failed: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_sub, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\n",
            "    plt.colorbar(scatter, label='Digit')\n",
            "    plt.title('t-SNE of Digits Dataset')\n",
            "    plt.xlabel('t-SNE Component 1')\n",
            "    plt.ylabel('t-SNE Component 2')\n",
            "    plt.tight_layout()\n",
            "\n",
            "    try:\n",
            "        plt.savefig(args.output, dpi=100)\n",
            "        plt.close()\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to save plot: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    import os\n",
            "    if os.path.isfile(args.output):\n",
            "        print(f\"Saved t-SNE plot to {args.output}\")\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 31: Diabetes Cross Validation --- Latency: 6.28s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_diabetes\n",
            "from sklearn.linear_model import Ridge\n",
            "from sklearn.model_selection import cross_val_score\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Run 5-fold CV on diabetes dataset with Ridge regression; print TEST_PASS if mean R² ≥ 0.35.\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--alpha\", type=float, default=1.0, help=\"Ridge regularization strength (default: 1.0).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_diabetes()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load diabetes dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: diabetes dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    model = Ridge(alpha=args.alpha, random_state=args.seed)\n",
            "    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
            "    mean_r2 = scores.mean()\n",
            "\n",
            "    print(f\"5-fold CV R² scores: {scores}\")\n",
            "    print(f\"Mean R²: {mean_r2:.4f}\")\n",
            "\n",
            "    if mean_r2 >= 0.35:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: mean R² {mean_r2:.4f} < 0.35\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 32: Moons SVM Margin --- Latency: 12.59s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import make_moons\n",
            "from sklearn.svm import SVC\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as svm_margin.png.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of samples (default: 200).\")\n",
            "    p.add_argument(\"--noise\", type=float, default=0.2, help=\"Noise level (default: 0.2).\")\n",
            "    p.add_argument(\"--C\", type=float, default=1.0, help=\"SVM regularization parameter (default: 1.0).\")\n",
            "    p.add_argument(\"--gamma\", type=float, default=2.0, help=\"RBF kernel gamma (default: 2.0).\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"svm_margin.png\", help=\"Output image path (default: svm_margin.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\n",
            "    \n",
            "    scaler = StandardScaler()\n",
            "    X_scaled = scaler.fit_transform(X)\n",
            "    \n",
            "    clf = SVC(kernel='rbf', C=args.C, gamma=args.gamma)\n",
            "    clf.fit(X_scaled, y)\n",
            "    \n",
            "    x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\n",
            "    y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\n",
            "    h = 0.02\n",
            "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
            "    \n",
            "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
            "    Z = Z.reshape(xx.shape)\n",
            "    \n",
            "    plt.figure(figsize=(10, 8))\n",
            "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 50), cmap='RdBu', alpha=0.6)\n",
            "    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n",
            "    plt.contour(xx, yy, Z, levels=[-1, 1], linewidths=1, colors='black', linestyles='dashed')\n",
            "    \n",
            "    plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c='red', edgecolors='k', marker='o', s=50, label='Class 0')\n",
            "    plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c='blue', edgecolors='k', marker='s', s=50, label='Class 1')\n",
            "    \n",
            "    support_vectors = clf.support_vectors_\n",
            "    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=200, linewidth=1, facecolors='none', edgecolors='green', label='Support Vectors')\n",
            "    \n",
            "    plt.xlabel('Feature 1')\n",
            "    plt.ylabel('Feature 2')\n",
            "    plt.title('SVM Decision Boundary and Margins (RBF Kernel)')\n",
            "    plt.legend()\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "    \n",
            "    import os\n",
            "    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\n",
            "        print(f\"Saved decision boundary plot to {args.output}\")\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file not created or empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 33: Circles Neural Net --- Latency: 9.49s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import make_circles\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.neural_network import MLPClassifier\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy >= 0.87.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=1000, help=\"Number of samples to generate (default: 1000).\")\n",
            "    p.add_argument(\"--noise\", type=float, default=0.1, help=\"Noise level for make_circles (default: 0.1).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y = make_circles(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\n",
            "    \n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    scaler = StandardScaler()\n",
            "    X_train_scaled = scaler.fit_transform(X_train)\n",
            "    X_test_scaled = scaler.transform(X_test)\n",
            "\n",
            "    clf = MLPClassifier(\n",
            "        hidden_layer_sizes=(100, 50),\n",
            "        max_iter=500,\n",
            "        random_state=args.seed,\n",
            "        early_stopping=True,\n",
            "        validation_fraction=0.1\n",
            "    )\n",
            "    clf.fit(X_train_scaled, y_train)\n",
            "\n",
            "    accuracy = clf.score(X_test_scaled, y_test)\n",
            "    print(f\"accuracy={accuracy:.3f}\")\n",
            "\n",
            "    if accuracy >= 0.87:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.87\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 34: Blobs Elbow Method --- Latency: 8.69s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import make_blobs\n",
            "from sklearn.cluster import KMeans\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as elbow_plot.png.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n",
            "    p.add_argument(\"--n-features\", type=int, default=2, help=\"Number of features (default: 2).\")\n",
            "    p.add_argument(\"--centers\", type=int, default=4, help=\"Number of centers for make_blobs (default: 4).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"elbow_plot.png\", help=\"Output filename for elbow plot (default: elbow_plot.png).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y = make_blobs(n_samples=args.n_samples, n_features=args.n_features, centers=args.centers, random_state=args.seed)\n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    inertias = []\n",
            "    k_range = range(1, 9)\n",
            "    for k in k_range:\n",
            "        km = KMeans(n_clusters=k, random_state=args.seed, n_init=10)\n",
            "        km.fit(X)\n",
            "        inertias.append(km.inertia_)\n",
            "\n",
            "    plt.figure(figsize=(8, 5))\n",
            "    plt.plot(list(k_range), inertias, marker='o')\n",
            "    plt.xlabel('Number of clusters (k)')\n",
            "    plt.ylabel('Inertia')\n",
            "    plt.title('Elbow Method for Optimal k')\n",
            "    plt.grid(True)\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if os.path.isfile(args.output):\n",
            "        print(f\"Elbow plot saved to {args.output}\")\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 35: Classification Learning Curve --- Latency: 10.77s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import make_classification\n",
            "from sklearn.model_selection import learning_curve\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate make_classification data, train LogisticRegression, plot learning curve, save as learning_curve.png.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=500, help=\"Number of samples (default: 500).\")\n",
            "    p.add_argument(\"--n-features\", type=int, default=20, help=\"Number of features (default: 20).\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"learning_curve.png\", help=\"Output file path (default: learning_curve.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y = make_classification(\n",
            "        n_samples=args.n_samples,\n",
            "        n_features=args.n_features,\n",
            "        n_informative=max(2, args.n_features // 2),\n",
            "        n_redundant=max(0, args.n_features // 4),\n",
            "        random_state=args.seed\n",
            "    )\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    clf = LogisticRegression(max_iter=300, random_state=args.seed)\n",
            "\n",
            "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
            "    train_sizes_abs, train_scores, val_scores = learning_curve(\n",
            "        clf, X, y,\n",
            "        train_sizes=train_sizes,\n",
            "        cv=5,\n",
            "        scoring='accuracy',\n",
            "        random_state=args.seed,\n",
            "        n_jobs=1\n",
            "    )\n",
            "\n",
            "    train_mean = np.mean(train_scores, axis=1)\n",
            "    train_std = np.std(train_scores, axis=1)\n",
            "    val_mean = np.mean(val_scores, axis=1)\n",
            "    val_std = np.std(val_scores, axis=1)\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    plt.plot(train_sizes_abs, train_mean, 'o-', label='Training score', linewidth=2)\n",
            "    plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
            "    plt.plot(train_sizes_abs, val_mean, 'o-', label='Validation score', linewidth=2)\n",
            "    plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2)\n",
            "    plt.xlabel('Training Set Size')\n",
            "    plt.ylabel('Accuracy')\n",
            "    plt.title('Learning Curve (LogisticRegression)')\n",
            "    plt.legend(loc='best')\n",
            "    plt.grid(True, alpha=0.3)\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\n",
            "        print(f\"Learning curve saved to {args.output}\")\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file not created or empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 36: Regression Residual Analysis --- Latency: 11.04s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import make_regression\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.model_selection import train_test_split\n",
            "import os\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate regression data, train LinearRegression, plot residual histogram, save as resid_hist.png.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples (default: 300).\")\n",
            "    p.add_argument(\"--n-features\", type=int, default=5, help=\"Number of features (default: 5).\")\n",
            "    p.add_argument(\"--noise\", type=float, default=10.0, help=\"Noise level (default: 10.0).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"resid_hist.png\", help=\"Output histogram filename (default: resid_hist.png).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y = make_regression(n_samples=args.n_samples, n_features=args.n_features, noise=args.noise, random_state=args.seed)\n",
            "    \n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\n",
            "    \n",
            "    model = LinearRegression()\n",
            "    model.fit(Xtr, ytr)\n",
            "    \n",
            "    y_pred = model.predict(Xte)\n",
            "    residuals = yte - y_pred\n",
            "    \n",
            "    plt.figure(figsize=(8, 6))\n",
            "    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
            "    plt.xlabel('Residuals')\n",
            "    plt.ylabel('Frequency')\n",
            "    plt.title('Residual Histogram')\n",
            "    plt.grid(True, alpha=0.3)\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "    \n",
            "    if not os.path.exists(args.output):\n",
            "        print(f\"TEST_FAIL: output file {args.output} not created\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    file_size = os.path.getsize(args.output)\n",
            "    if file_size == 0:\n",
            "        print(f\"TEST_FAIL: output file {args.output} is empty\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    print(f\"Residual histogram saved to {args.output} (size: {file_size} bytes)\")\n",
            "    print(\"TEST_PASS\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 37: Synthetic NLP Spam Filter --- Latency: 12.85s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from sklearn.naive_bayes import MultinomialNB\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "def generate_synthetic_emails(n=250, seed=42):\n",
            "    rng = np.random.default_rng(seed)\n",
            "    spam_words = [\"win\", \"free\", \"prize\", \"click\", \"offer\", \"buy\", \"discount\", \"limited\", \"urgent\", \"cash\", \"bonus\", \"guarantee\", \"credit\", \"loan\", \"money\"]\n",
            "    ham_words = [\"meeting\", \"schedule\", \"report\", \"project\", \"team\", \"update\", \"review\", \"discuss\", \"attached\", \"please\", \"thanks\", \"regards\", \"confirm\", \"deadline\", \"agenda\"]\n",
            "    \n",
            "    texts = []\n",
            "    labels = []\n",
            "    \n",
            "    for i in range(n):\n",
            "        is_spam = rng.random() < 0.5\n",
            "        if is_spam:\n",
            "            num_words = rng.integers(5, 15)\n",
            "            words = rng.choice(spam_words, size=num_words, replace=True)\n",
            "            text = \" \".join(words)\n",
            "            labels.append(1)\n",
            "        else:\n",
            "            num_words = rng.integers(5, 15)\n",
            "            words = rng.choice(ham_words, size=num_words, replace=True)\n",
            "            text = \" \".join(words)\n",
            "            labels.append(0)\n",
            "        texts.append(text)\n",
            "    \n",
            "    return texts, labels\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 250 synthetic spam/ham emails, train Naive Bayes with TF-IDF, report accuracy.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=250, help=\"Number of synthetic emails to generate (default: 250).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    texts, labels = generate_synthetic_emails(n=args.n_samples, seed=args.seed)\n",
            "    \n",
            "    if len(texts) == 0 or len(labels) == 0:\n",
            "        print(\"TEST_FAIL: no data generated\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        texts, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\n",
            "    )\n",
            "    \n",
            "    vectorizer = TfidfVectorizer(max_features=100)\n",
            "    X_train_vec = vectorizer.fit_transform(X_train)\n",
            "    X_test_vec = vectorizer.transform(X_test)\n",
            "    \n",
            "    clf = MultinomialNB()\n",
            "    clf.fit(X_train_vec, y_train)\n",
            "    \n",
            "    y_pred = clf.predict(X_test_vec)\n",
            "    acc = accuracy_score(y_test, y_pred)\n",
            "    \n",
            "    print(f\"n_samples={args.n_samples} accuracy={acc:.3f}\")\n",
            "    \n",
            "    if acc >= 0.7:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: accuracy below 0.7\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 38: Synthetic Recommender System --- Latency: 13.77s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.decomposition import NMF\n",
            "from sklearn.metrics import mean_squared_error\n",
            "\n",
            "def generate_synthetic_ratings(n_users=50, n_items=40, n_ratings=200, seed=42):\n",
            "    \"\"\"Generate synthetic user-item ratings matrix.\"\"\"\n",
            "    rng = np.random.default_rng(seed)\n",
            "    \n",
            "    # Create sparse ratings: user_id, item_id, rating\n",
            "    user_ids = rng.integers(0, n_users, size=n_ratings)\n",
            "    item_ids = rng.integers(0, n_items, size=n_ratings)\n",
            "    ratings = rng.uniform(1.0, 5.0, size=n_ratings)\n",
            "    \n",
            "    # Build dense matrix (users x items)\n",
            "    R = np.zeros((n_users, n_items))\n",
            "    for u, i, r in zip(user_ids, item_ids, ratings):\n",
            "        R[u, i] = r\n",
            "    \n",
            "    # Mask: which entries are observed\n",
            "    mask = (R > 0)\n",
            "    \n",
            "    return R, mask, user_ids, item_ids, ratings\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Synthetic recommender with NMF; RMSE ≤ 1.5 for TEST_PASS.\")\n",
            "    p.add_argument(\"--n-users\", type=int, default=50, help=\"Number of users (default: 50).\")\n",
            "    p.add_argument(\"--n-items\", type=int, default=40, help=\"Number of items (default: 40).\")\n",
            "    p.add_argument(\"--n-ratings\", type=int, default=200, help=\"Number of ratings (default: 200).\")\n",
            "    p.add_argument(\"--n-components\", type=int, default=10, help=\"NMF latent factors (default: 10).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "    \n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "    \n",
            "    # Generate synthetic ratings\n",
            "    R, mask, user_ids, item_ids, ratings = generate_synthetic_ratings(\n",
            "        n_users=args.n_users,\n",
            "        n_items=args.n_items,\n",
            "        n_ratings=args.n_ratings,\n",
            "        seed=args.seed\n",
            "    )\n",
            "    \n",
            "    if R is None or mask is None or np.sum(mask) == 0:\n",
            "        print(\"TEST_FAIL: no ratings generated\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    print(f\"Generated {args.n_ratings} ratings for {args.n_users} users and {args.n_items} items\")\n",
            "    \n",
            "    # Apply NMF (Non-negative Matrix Factorization)\n",
            "    # NMF requires non-negative values; our ratings are already ≥ 1.0\n",
            "    model = NMF(n_components=args.n_components, init='random', random_state=args.seed, max_iter=500)\n",
            "    \n",
            "    try:\n",
            "        W = model.fit_transform(R)  # user factors\n",
            "        H = model.components_       # item factors\n",
            "        R_pred = W @ H              # reconstructed matrix\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: NMF failed - {e}\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    # Compute RMSE on observed ratings only\n",
            "    observed_true = R[mask]\n",
            "    observed_pred = R_pred[mask]\n",
            "    \n",
            "    if len(observed_true) == 0:\n",
            "        print(\"TEST_FAIL: no observed ratings to evaluate\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    rmse = np.sqrt(mean_squared_error(observed_true, observed_pred))\n",
            "    print(f\"RMSE on observed ratings: {rmse:.4f}\")\n",
            "    \n",
            "    # Acceptance: RMSE ≤ 1.5\n",
            "    if rmse <= 1.5:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: RMSE {rmse:.4f} > 1.5\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 39: Synthetic Speech Emotion --- Latency: 14.22s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "def generate_synthetic_speech_emotion(n_samples=200, n_features=13, n_classes=4, seed=42):\n",
            "    \"\"\"Generate synthetic speech-like feature vectors with emotion labels.\"\"\"\n",
            "    rng = np.random.default_rng(seed)\n",
            "    \n",
            "    # Emotion classes: 0=neutral, 1=happy, 2=sad, 3=angry\n",
            "    y = rng.integers(0, n_classes, size=n_samples)\n",
            "    \n",
            "    # Generate MFCC-like features (13 coefficients typical for speech)\n",
            "    X = np.zeros((n_samples, n_features))\n",
            "    \n",
            "    for i in range(n_samples):\n",
            "        emotion = y[i]\n",
            "        # Each emotion has characteristic feature patterns\n",
            "        if emotion == 0:  # neutral\n",
            "            X[i] = rng.normal(0.0, 1.0, size=n_features)\n",
            "        elif emotion == 1:  # happy (higher pitch, more energy)\n",
            "            X[i] = rng.normal(1.5, 1.2, size=n_features)\n",
            "        elif emotion == 2:  # sad (lower pitch, less energy)\n",
            "            X[i] = rng.normal(-1.5, 0.8, size=n_features)\n",
            "        elif emotion == 3:  # angry (high energy, variable pitch)\n",
            "            X[i] = rng.normal(0.5, 2.0, size=n_features)\n",
            "    \n",
            "    return X, y\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 200 synthetic speech emotion vectors, train RandomForest, report accuracy.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of synthetic samples (default: 200).\")\n",
            "    p.add_argument(\"--n-features\", type=int, default=13, help=\"Number of features per sample (default: 13).\")\n",
            "    p.add_argument(\"--n-classes\", type=int, default=4, help=\"Number of emotion classes (default: 4).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.25, help=\"Test set fraction (default: 0.25).\")\n",
            "    p.add_argument(\"--n-estimators\", type=int, default=100, help=\"Number of trees in RandomForest (default: 100).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "    \n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "    \n",
            "    # Generate synthetic speech emotion dataset\n",
            "    X, y = generate_synthetic_speech_emotion(\n",
            "        n_samples=args.n_samples,\n",
            "        n_features=args.n_features,\n",
            "        n_classes=args.n_classes,\n",
            "        seed=args.seed\n",
            "    )\n",
            "    \n",
            "    # Validate dataset\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    if len(X) != args.n_samples:\n",
            "        print(f\"TEST_FAIL: expected {args.n_samples} samples, got {len(X)}\")\n",
            "        sys.exit(1)\n",
            "    \n",
            "    # Split dataset\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "    \n",
            "    # Train RandomForest\n",
            "    clf = RandomForestClassifier(\n",
            "        n_estimators=args.n_estimators,\n",
            "        random_state=args.seed,\n",
            "        n_jobs=-1\n",
            "    )\n",
            "    clf.fit(X_train, y_train)\n",
            "    \n",
            "    # Predict and evaluate\n",
            "    y_pred = clf.predict(X_test)\n",
            "    accuracy = accuracy_score(y_test, y_pred)\n",
            "    \n",
            "    print(f\"samples={args.n_samples} features={args.n_features} classes={args.n_classes}\")\n",
            "    print(f\"train_size={len(X_train)} test_size={len(X_test)}\")\n",
            "    print(f\"accuracy={accuracy:.3f}\")\n",
            "    \n",
            "    # Acceptance check\n",
            "    if accuracy >= 0.65:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.65\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 40: Synthetic Financial Forecast --- Latency: 16.89s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "def generate_synthetic_stock_sequences(n_sequences=300, seq_len=20, seed=42):\n",
            "    \"\"\"Generate synthetic stock-like sequences with up/down labels.\"\"\"\n",
            "    rng = np.random.default_rng(seed)\n",
            "    sequences = []\n",
            "    labels = []\n",
            "    \n",
            "    for _ in range(n_sequences):\n",
            "        # Generate a random walk with drift\n",
            "        drift = rng.uniform(-0.02, 0.02)\n",
            "        volatility = rng.uniform(0.01, 0.05)\n",
            "        returns = rng.normal(drift, volatility, seq_len)\n",
            "        prices = 100 * np.exp(np.cumsum(returns))\n",
            "        \n",
            "        # Label: 1 if final price > initial price, else 0\n",
            "        label = 1 if prices[-1] > prices[0] else 0\n",
            "        \n",
            "        sequences.append(prices)\n",
            "        labels.append(label)\n",
            "    \n",
            "    return np.array(sequences), np.array(labels)\n",
            "\n",
            "def extract_features(sequences):\n",
            "    \"\"\"Extract simple features from price sequences.\"\"\"\n",
            "    features = []\n",
            "    for seq in sequences:\n",
            "        # Compute basic statistics\n",
            "        mean_price = np.mean(seq)\n",
            "        std_price = np.std(seq)\n",
            "        min_price = np.min(seq)\n",
            "        max_price = np.max(seq)\n",
            "        price_range = max_price - min_price\n",
            "        \n",
            "        # Compute returns\n",
            "        returns = np.diff(seq) / seq[:-1]\n",
            "        mean_return = np.mean(returns)\n",
            "        std_return = np.std(returns)\n",
            "        \n",
            "        # Trend: slope of linear fit\n",
            "        x = np.arange(len(seq))\n",
            "        trend = np.polyfit(x, seq, 1)[0]\n",
            "        \n",
            "        # Final vs initial price ratio\n",
            "        price_ratio = seq[-1] / seq[0]\n",
            "        \n",
            "        features.append([\n",
            "            mean_price, std_price, min_price, max_price, price_range,\n",
            "            mean_return, std_return, trend, price_ratio\n",
            "        ])\n",
            "    \n",
            "    return np.array(features)\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 300 synthetic stock sequences, predict direction with LogisticRegression.\")\n",
            "    p.add_argument(\"--n-sequences\", type=int, default=300, help=\"Number of synthetic sequences (default: 300).\")\n",
            "    p.add_argument(\"--seq-len\", type=int, default=20, help=\"Length of each sequence (default: 20).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # Validate inputs\n",
            "    if args.n_sequences <= 0:\n",
            "        print(\"TEST_FAIL: n_sequences must be positive\")\n",
            "        sys.exit(1)\n",
            "    if args.seq_len <= 1:\n",
            "        print(\"TEST_FAIL: seq_len must be > 1\")\n",
            "        sys.exit(1)\n",
            "    if not (0 < args.test_size < 1):\n",
            "        print(\"TEST_FAIL: test_size must be in (0, 1)\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Generate synthetic data\n",
            "    sequences, labels = generate_synthetic_stock_sequences(\n",
            "        n_sequences=args.n_sequences,\n",
            "        seq_len=args.seq_len,\n",
            "        seed=args.seed\n",
            "    )\n",
            "    \n",
            "    if len(sequences) == 0 or len(labels) == 0:\n",
            "        print(\"TEST_FAIL: failed to generate synthetic data\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Extract features\n",
            "    X = extract_features(sequences)\n",
            "    y = labels\n",
            "\n",
            "    # Split data\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # Scale features\n",
            "    scaler = StandardScaler()\n",
            "    X_train_scaled = scaler.fit_transform(X_train)\n",
            "    X_test_scaled = scaler.transform(X_test)\n",
            "\n",
            "    # Train LogisticRegression\n",
            "    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\n",
            "    clf.fit(X_train_scaled, y_train)\n",
            "\n",
            "    # Evaluate\n",
            "    accuracy = clf.score(X_test_scaled, y_test)\n",
            "    print(f\"n_sequences={args.n_sequences} seq_len={args.seq_len} accuracy={accuracy:.3f}\")\n",
            "\n",
            "    # Acceptance check\n",
            "    if accuracy >= 0.6:\n",
            "  \n",
            "      print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {accuracy:.3f} < 0.6\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 41: MNIST Autoencoder --- Latency: 19.29s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import os\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def load_mnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
            "        if len(train) == 0:\n",
            "            raise RuntimeError(\"MNIST cache missing and download disabled\")\n",
            "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
            "        return train, True\n",
            "    except Exception:\n",
            "        import torch\n",
            "        from torchvision import transforms\n",
            "        from torchvision.datasets import FakeData\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "        return train, False\n",
            "\n",
            "def main():\n",
            "    import torch\n",
            "    import torch.nn as nn\n",
            "    import torch.optim as optim\n",
            "    from torch.utils.data import DataLoader\n",
            "    import matplotlib\n",
            "    matplotlib.use('Agg')\n",
            "    import matplotlib.pyplot as plt\n",
            "\n",
            "    p = argparse.ArgumentParser(description=\"MNIST autoencoder (opt-in download) or FakeData fallback; saves autoencode.png.\")\n",
            "    p.add_argument(\"--epochs\", type=int, default=3, help=\"Training epochs (default: 3).\")\n",
            "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"autoencode.png\", help=\"Output image path (default: autoencode.png).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    torch.manual_seed(args.seed)\n",
            "\n",
            "    train_ds, real = load_mnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\n",
            "    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
            "\n",
            "    class Autoencoder(nn.Module):\n",
            "        def __init__(self):\n",
            "            super().__init__()\n",
            "            self.encoder = nn.Sequential(\n",
            "                nn.Flatten(),\n",
            "                nn.Linear(28*28, 128),\n",
            "                nn.ReLU(),\n",
            "                nn.Linear(128, 64),\n",
            "                nn.ReLU(),\n",
            "                nn.Linear(64, 32)\n",
            "            )\n",
            "            self.decoder = nn.Sequential(\n",
            "                nn.Linear(32, 64),\n",
            "                nn.ReLU(),\n",
            "                nn.Linear(64, 128),\n",
            "                nn.ReLU(),\n",
            "                nn.Linear(128, 28*28),\n",
            "                nn.Sigmoid()\n",
            "            )\n",
            "        def forward(self, x):\n",
            "            z = self.encoder(x)\n",
            "            recon = self.decoder(z)\n",
            "            return recon.view(-1, 1, 28, 28)\n",
            "\n",
            "    model = Autoencoder()\n",
            "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
            "    loss_fn = nn.MSELoss()\n",
            "\n",
            "    model.train()\n",
            "    for epoch in range(args.epochs):\n",
            "        total_loss = 0.0\n",
            "        for xb, _ in train_loader:\n",
            "            opt.zero_grad()\n",
            "            recon = model(xb)\n",
            "            loss = loss_fn(recon, xb)\n",
            "            loss.backward()\n",
            "            opt.step()\n",
            "            total_loss += loss.item()\n",
            "        avg_loss = total_loss / len(train_loader)\n",
            "        print(f\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\")\n",
            "\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        sample_x, _ = next(iter(DataLoader(train_ds, batch_size=1, shuffle=False)))\n",
            "        sample_recon = model(sample_x)\n",
            "\n",
            "    orig = sample_x[0, 0].cpu().numpy()\n",
            "    recon = sample_recon[0, 0].cpu().numpy()\n",
            "\n",
            "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
            "    axes[0].imshow(orig, cmap='gray')\n",
            "    axes[0].set_title('Original')\n",
            "    axes[0].axis('off')\n",
            "    axes[1].imshow(recon, cmap='gray')\n",
            "    axes[1].set_title('Reconstructed')\n",
            "    axes[1].axis('off')\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output)\n",
            "    plt.close()\n",
            "\n",
            "    print(f\"dataset={'mnist' if real else 'fake'} output={args.output}\")\n",
            "\n",
            "    if o\n",
            "s.path.isfile(args.output):\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 42: FashionMNIST VAE Latent --- Latency: 21.44s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import os\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def load_fashionmnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        train = datasets.FashionMNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
            "        if len(train) == 0:\n",
            "            raise RuntimeError(\"FashionMNIST cache missing and download disabled\")\n",
            "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
            "        return train, True\n",
            "    except Exception:\n",
            "        import torch\n",
            "        from torchvision import transforms\n",
            "        from torchvision.datasets import FakeData\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "        return train, False\n",
            "\n",
            "def main():\n",
            "    import torch\n",
            "    import torch.nn as nn\n",
            "    import torch.optim as optim\n",
            "    from torch.utils.data import DataLoader\n",
            "\n",
            "    p = argparse.ArgumentParser(description=\"FashionMNIST VAE latent space visualization with opt-in download or FakeData fallback.\")\n",
            "    p.add_argument(\"--epochs\", type=int, default=3, help=\"Training epochs (default: 3).\")\n",
            "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
            "    p.add_argument(\"--latent-dim\", type=int, default=2, help=\"Latent dimension (default: 2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit FashionMNIST download if not cached.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"latent_vae.png\", help=\"Output latent space plot filename (default: latent_vae.png).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    torch.manual_seed(args.seed)\n",
            "\n",
            "    train_ds, real = load_fashionmnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\n",
            "    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
            "\n",
            "    class VAE(nn.Module):\n",
            "        def __init__(self, latent_dim=2):\n",
            "            super().__init__()\n",
            "            self.latent_dim = latent_dim\n",
            "            self.encoder = nn.Sequential(\n",
            "                nn.Flatten(),\n",
            "                nn.Linear(28*28, 256), nn.ReLU(),\n",
            "                nn.Linear(256, 128), nn.ReLU()\n",
            "            )\n",
            "            self.fc_mu = nn.Linear(128, latent_dim)\n",
            "            self.fc_logvar = nn.Linear(128, latent_dim)\n",
            "            self.decoder = nn.Sequential(\n",
            "                nn.Linear(latent_dim, 128), nn.ReLU(),\n",
            "                nn.Linear(128, 256), nn.ReLU(),\n",
            "                nn.Linear(256, 28*28), nn.Sigmoid()\n",
            "            )\n",
            "\n",
            "        def encode(self, x):\n",
            "            h = self.encoder(x)\n",
            "            return self.fc_mu(h), self.fc_logvar(h)\n",
            "\n",
            "        def reparameterize(self, mu, logvar):\n",
            "            std = torch.exp(0.5 * logvar)\n",
            "            eps = torch.randn_like(std)\n",
            "            return mu + eps * std\n",
            "\n",
            "        def decode(self, z):\n",
            "            return self.decoder(z).view(-1, 1, 28, 28)\n",
            "\n",
            "        def forward(self, x):\n",
            "            mu, logvar = self.encode(x)\n",
            "            z = self.reparameterize(mu, logvar)\n",
            "            recon = self.decode(z)\n",
            "            return recon, mu, logvar\n",
            "\n",
            "    def vae_loss(recon, x, mu, logvar):\n",
            "        bce = nn.functional.binary_cross_entropy(recon, x, reduction='sum')\n",
            "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
            "        return bce + kld\n",
            "\n",
            "    model = VAE(latent_dim=args.latent_dim)\n",
            "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
            "\n",
            "    model.train()\n",
            "    for epoch in range(args.epochs):\n",
            "        total_loss = 0.0\n",
            "        for xb, _ in train_loader:\n",
            "            opt.zero_grad()\n",
            "            recon, mu, logvar = model(xb)\n",
            "            loss = vae_loss(recon, xb, mu, logvar)\n",
            "            loss.backward()\n",
            "   \n",
            "         opt.step()\n",
            "            total_loss += loss.item()\n",
            "        avg_loss = total_loss / len(train_loader.dataset)\n",
            "        print(f\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\")\n",
            "\n",
            "    model.eval()\n",
            "    latents = []\n",
            "    labels = []\n",
            "    with torch.no_grad():\n",
            "        for xb, yb in train_loader:\n",
            "            mu, _ = model.encode(xb)\n",
            "            latents.append(mu.cpu().numpy())\n",
            "            labels.append(yb.cpu().numpy())\n",
            "    latents = np.concatenate(latents, axis=0)\n",
            "    labels = np.concatenate(labels, axis=0)\n",
            "\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10', alpha=0.6, s=10)\n",
            "    plt.colorbar(scatter, label='Class')\n",
            "    plt.xlabel('Latent Dim 1')\n",
            "    plt.ylabel('Latent Dim 2')\n",
            "    plt.title(f\"VAE Latent Space ({'FashionMNIST' if real else 'FakeData'})\")\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "    print(f\"Saved latent space plot to {args.output}\")\n",
            "\n",
            "    if os.path.isfile(args.output):\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 43: CIFAR10 Transfer Features --- Latency: 18.95s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def load_cifar10_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.Compose([\n",
            "            transforms.ToTensor(),\n",
            "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
            "        ])\n",
            "        train = datasets.CIFAR10(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
            "        test = datasets.CIFAR10(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n",
            "        if len(train) == 0 or len(test) == 0:\n",
            "            raise RuntimeError(\"CIFAR10 cache missing and download disabled\")\n",
            "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
            "        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\n",
            "        return train, test, True\n",
            "    except Exception:\n",
            "        import torch\n",
            "        from torchvision import transforms\n",
            "        from torchvision.datasets import FakeData\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.Compose([\n",
            "            transforms.ToTensor(),\n",
            "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
            "        ])\n",
            "        train = FakeData(size=max_train, image_size=(3, 32, 32), num_classes=10, transform=tfm)\n",
            "        test = FakeData(size=max_test, image_size=(3, 32, 32), num_classes=10, transform=tfm)\n",
            "        return train, test, False\n",
            "\n",
            "def extract_features(model, dataloader, device):\n",
            "    import torch\n",
            "    model.eval()\n",
            "    features_list = []\n",
            "    labels_list = []\n",
            "    with torch.no_grad():\n",
            "        for xb, yb in dataloader:\n",
            "            xb = xb.to(device)\n",
            "            feat = model(xb)\n",
            "            features_list.append(feat.cpu().numpy())\n",
            "            labels_list.append(yb.numpy())\n",
            "    X = np.vstack(features_list)\n",
            "    y = np.concatenate(labels_list)\n",
            "    return X, y\n",
            "\n",
            "def main():\n",
            "    import torch\n",
            "    import torch.nn as nn\n",
            "    from torch.utils.data import DataLoader\n",
            "    from sklearn.linear_model import LogisticRegression\n",
            "\n",
            "    p = argparse.ArgumentParser(description=\"CIFAR10 transfer features (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n",
            "    p.add_argument(\"--max-train\", type=int, default=2000, help=\"Max training samples (default: 2000).\")\n",
            "    p.add_argument(\"--max-test\", type=int, default=500, help=\"Max test samples (default: 500).\")\n",
            "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit CIFAR10 download if not cached.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    train_ds, test_ds, real = load_cifar10_or_fakedata(\n",
            "        max_train=args.max_train,\n",
            "        max_test=args.max_test,\n",
            "        seed=args.seed,\n",
            "        allow_download=args.allow_download\n",
            "    )\n",
            "    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=False)\n",
            "    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\n",
            "\n",
            "    device = torch.device(\"cpu\")\n",
            "\n",
            "    class FeatureExtractor(nn.Module):\n",
            "        def __init__(self):\n",
            "            super().__init__()\n",
            "            self.conv = nn.Sequential(\n",
            "                nn.Conv2d(3, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
            "                nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
            "                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n",
            "            )\n",
            "        def forward(self, x):\n",
            "            x = self.conv(x)\n",
            "            return x.view(x.size(0), -1)\n",
            "\n",
            "    model = FeatureExtractor().to(device)\n",
            "\n",
            "    print(\"Extracting training features...\")\n",
            "    X_train, y_train = extract_features(model, train_loader, device)\n",
            "    print(\"Extracting test features...\")\n",
            "    X_test, y_test = extract_features(model\n",
            ", test_loader, device)\n",
            "\n",
            "    if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
            "        print(\"TEST_FAIL: no samples extracted\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    clf = LogisticRegression(max_iter=500, random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "    acc = clf.score(X_test, y_test)\n",
            "\n",
            "    print(f\"acc={acc:.3f} dataset={'cifar10' if real else 'fake'}\")\n",
            "\n",
            "    threshold = 0.6 if real else 0.4\n",
            "    if acc >= threshold:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 44: ImageFolder Augmentation --- Latency: 15.38s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import os\n",
            "import random\n",
            "import numpy as np\n",
            "from pathlib import Path\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Apply rotation/flips to ImageFolder or FakeData; save augmented grid as augment_grid.png.\")\n",
            "    p.add_argument(\"--imagefolder-path\", type=str, default=None, help=\"Path to ImageFolder root (default: None, use FakeData).\")\n",
            "    p.add_argument(\"--num-images\", type=int, default=20, help=\"Number of images to augment (default: 20).\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"augment_grid.png\", help=\"Output grid filename (default: augment_grid.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit dataset download if needed.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        from torchvision.utils import make_grid\n",
            "        from PIL import Image\n",
            "    except ImportError:\n",
            "        print(\"TEST_FAIL: torchvision or PIL not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    torch.manual_seed(args.seed)\n",
            "\n",
            "    use_imagefolder = False\n",
            "    if args.imagefolder_path and os.path.isdir(args.imagefolder_path):\n",
            "        try:\n",
            "            base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
            "            ds = datasets.ImageFolder(root=args.imagefolder_path, transform=base_tfm)\n",
            "            if len(ds) > 0:\n",
            "                use_imagefolder = True\n",
            "                print(f\"Using ImageFolder from {args.imagefolder_path} with {len(ds)} images\")\n",
            "        except Exception as e:\n",
            "            print(f\"ImageFolder failed: {e}, falling back to FakeData\")\n",
            "\n",
            "    if not use_imagefolder:\n",
            "        print(\"Using FakeData fallback\")\n",
            "        base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
            "        ds = datasets.FakeData(size=args.num_images, image_size=(3, 64, 64), num_classes=10, transform=base_tfm)\n",
            "\n",
            "    num_samples = min(args.num_images, len(ds))\n",
            "    indices = list(range(num_samples))\n",
            "    random.shuffle(indices)\n",
            "    indices = indices[:num_samples]\n",
            "\n",
            "    augmentations = [\n",
            "        transforms.RandomRotation(degrees=30),\n",
            "        transforms.RandomHorizontalFlip(p=1.0),\n",
            "        transforms.RandomVerticalFlip(p=1.0),\n",
            "        transforms.Compose([transforms.RandomRotation(degrees=15), transforms.RandomHorizontalFlip(p=0.5)]),\n",
            "    ]\n",
            "\n",
            "    augmented_images = []\n",
            "    for idx in indices:\n",
            "        img_tensor, _ = ds[idx]\n",
            "        img_pil = transforms.ToPILImage()(img_tensor)\n",
            "        aug = random.choice(augmentations)\n",
            "        aug_pil = aug(img_pil)\n",
            "        aug_tensor = transforms.ToTensor()(aug_pil)\n",
            "        augmented_images.append(aug_tensor)\n",
            "\n",
            "    if len(augmented_images) == 0:\n",
            "        print(\"TEST_FAIL: no images to augment\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    grid_tensor = make_grid(augmented_images, nrow=5, padding=2, normalize=False)\n",
            "    grid_pil = transforms.ToPILImage()(grid_tensor)\n",
            "    grid_pil.save(args.output)\n",
            "    print(f\"Saved augmented grid to {args.output} with {len(augmented_images)} images\")\n",
            "\n",
            "    if os.path.isfile(args.output):\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 45: FakeData Style Transfer --- Latency: 10.19s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from pathlib import Path\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"FakeData style transfer: generate content/style pairs, apply basic transfer, save result.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"style_transfer.png\", help=\"Output image path (default: style_transfer.png).\")\n",
            "    p.add_argument(\"--size\", type=int, default=256, help=\"Image size (default: 256).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision.datasets import FakeData\n",
            "        from torchvision import transforms\n",
            "        from PIL import Image\n",
            "    except ImportError as e:\n",
            "        print(f\"TEST_FAIL: missing dependency {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # generate FakeData content and style images\n",
            "    torch.manual_seed(args.seed)\n",
            "    tfm = transforms.Compose([\n",
            "        transforms.Resize((args.size, args.size)),\n",
            "        transforms.ToTensor()\n",
            "    ])\n",
            "    \n",
            "    fake_ds = FakeData(size=2, image_size=(3, args.size, args.size), transform=tfm)\n",
            "    content_tensor, _ = fake_ds[0]\n",
            "    style_tensor, _ = fake_ds[1]\n",
            "\n",
            "    # basic style transfer: blend content and style with simple weighted average\n",
            "    # convert to numpy for manipulation\n",
            "    content_np = content_tensor.permute(1, 2, 0).numpy()\n",
            "    style_np = style_tensor.permute(1, 2, 0).numpy()\n",
            "    \n",
            "    # simple transfer: 70% content + 30% style\n",
            "    transferred = 0.7 * content_np + 0.3 * style_np\n",
            "    transferred = np.clip(transferred, 0, 1)\n",
            "    \n",
            "    # convert back to PIL and save\n",
            "    transferred_uint8 = (transferred * 255).astype(np.uint8)\n",
            "    result_img = Image.fromarray(transferred_uint8, mode='RGB')\n",
            "    result_img.save(args.output)\n",
            "    \n",
            "    # acceptance check\n",
            "    if Path(args.output).exists():\n",
            "        print(f\"Saved style transfer result to {args.output}\")\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: output file {args.output} not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 46: KMeans CIFAR Segmentation --- Latency: 13.90s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import os\n",
            "import random\n",
            "import numpy as np\n",
            "from PIL import Image\n",
            "\n",
            "def load_cifar_or_fakedata(seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        ds = datasets.CIFAR10(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n",
            "        if len(ds) == 0:\n",
            "            raise RuntimeError(\"CIFAR10 cache missing and download disabled\")\n",
            "        img_tensor, _ = ds[0]\n",
            "        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
            "        return img_np, True\n",
            "    except Exception:\n",
            "        import torch\n",
            "        from torchvision import transforms\n",
            "        from torchvision.datasets import FakeData\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        ds = FakeData(size=1, image_size=(3, 32, 32), num_classes=10, transform=tfm)\n",
            "        img_tensor, _ = ds[0]\n",
            "        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
            "        return img_np, False\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"KMeans CIFAR10 segmentation (opt-in download) or FakeData fallback; save segmented.png; TEST_PASS if file exists.\")\n",
            "    p.add_argument(\"--k\", type=int, default=4, help=\"Number of clusters for KMeans (default: 4).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit CIFAR10 download if not cached.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"segmented.png\", help=\"Output segmented image path (default: segmented.png).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    img, real = load_cifar_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n",
            "    if img is None or img.size == 0:\n",
            "        print(\"TEST_FAIL: image not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    h, w, c = img.shape\n",
            "    pixels = img.reshape(-1, c).astype(np.float32)\n",
            "\n",
            "    from sklearn.cluster import KMeans\n",
            "    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\n",
            "    labels = kmeans.fit_predict(pixels)\n",
            "    centers = kmeans.cluster_centers_.astype(np.uint8)\n",
            "\n",
            "    segmented = centers[labels].reshape(h, w, c)\n",
            "\n",
            "    pil_img = Image.fromarray(segmented, mode=\"RGB\")\n",
            "    pil_img.save(args.output)\n",
            "\n",
            "    print(f\"dataset={'cifar10' if real else 'fake'} k={args.k} saved={args.output}\")\n",
            "\n",
            "    if os.path.isfile(args.output):\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 47: Iris Outlier Detection --- Latency: 10.74s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.ensemble import IsolationForest\n",
            "from sklearn.datasets import load_iris\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as outliers.png.\")\n",
            "    p.add_argument(\"--contamination\", type=float, default=0.1, help=\"Expected proportion of outliers (default: 0.1).\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"outliers.png\", help=\"Output plot filename (default: outliers.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_iris()\n",
            "        X = data.data\n",
            "        feature_names = data.feature_names\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load iris dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: iris dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    iso = IsolationForest(contamination=args.contamination, random_state=args.seed)\n",
            "    iso.fit(X)\n",
            "    predictions = iso.predict(X)\n",
            "\n",
            "    normal_mask = predictions == 1\n",
            "    outlier_mask = predictions == -1\n",
            "\n",
            "    normal_points = X[normal_mask]\n",
            "    outlier_points = X[outlier_mask]\n",
            "\n",
            "    num_outliers = outlier_points.shape[0]\n",
            "    num_normal = normal_points.shape[0]\n",
            "\n",
            "    print(f\"Total samples: {len(X)}\")\n",
            "    print(f\"Normal points: {num_normal}\")\n",
            "    print(f\"Outliers detected: {num_outliers}\")\n",
            "\n",
            "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
            "    ax.scatter(normal_points[:, 0], normal_points[:, 1], c='blue', label='Normal', alpha=0.6, edgecolors='k')\n",
            "    ax.scatter(outlier_points[:, 0], outlier_points[:, 1], c='red', label='Outlier', alpha=0.8, edgecolors='k', s=100)\n",
            "    ax.set_xlabel(feature_names[0])\n",
            "    ax.set_ylabel(feature_names[1])\n",
            "    ax.set_title(\"Iris Outlier Detection (IsolationForest)\")\n",
            "    ax.legend()\n",
            "    ax.grid(True, alpha=0.3)\n",
            "\n",
            "    try:\n",
            "        plt.savefig(args.output, dpi=100, bbox_inches='tight')\n",
            "        print(f\"Plot saved to {args.output}\")\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to save plot: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    import os\n",
            "    if not os.path.isfile(args.output):\n",
            "        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if os.path.getsize(args.output) == 0:\n",
            "        print(f\"TEST_FAIL: output file {args.output} is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    print(\"TEST_PASS\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 48: Wine Anomaly Score --- Latency: 8.25s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_wine\n",
            "from sklearn.svm import OneClassSVM\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as anomaly_scores.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"anomaly_scores.png\", help=\"Output histogram filename (default: anomaly_scores.png).\")\n",
            "    p.add_argument(\"--nu\", type=float, default=0.1, help=\"OneClassSVM nu parameter (default: 0.1).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_wine()\n",
            "        X = data.data\n",
            "        if X is None or len(X) == 0:\n",
            "            raise ValueError(\"wine dataset is empty\")\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: could not load wine dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    scaler = StandardScaler()\n",
            "    X_scaled = scaler.fit_transform(X)\n",
            "\n",
            "    oc_svm = OneClassSVM(nu=args.nu, kernel='rbf', gamma='auto')\n",
            "    oc_svm.fit(X_scaled)\n",
            "\n",
            "    scores = oc_svm.decision_function(X_scaled)\n",
            "\n",
            "    plt.figure(figsize=(8, 5))\n",
            "    plt.hist(scores, bins=30, edgecolor='black', alpha=0.7)\n",
            "    plt.xlabel('Anomaly Score')\n",
            "    plt.ylabel('Frequency')\n",
            "    plt.title('Wine Dataset Anomaly Scores (OneClassSVM)')\n",
            "    plt.grid(True, alpha=0.3)\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100)\n",
            "    plt.close()\n",
            "\n",
            "    import os\n",
            "    if os.path.isfile(args.output):\n",
            "        print(f\"Histogram saved to {args.output}\")\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: output file {args.output} not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 49: Breast Cancer SHAP Values --- Latency: 15.46s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_breast_cancer\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "import shap\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train RandomForest on breast_cancer, compute SHAP values, save waterfall plot.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"shap_waterfall.png\", help=\"Output waterfall plot filename (default: shap_waterfall.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--n-estimators\", type=int, default=50, help=\"Number of trees in RandomForest (default: 50).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    try:\n",
            "        data = load_breast_cancer()\n",
            "        X, y = data.data, data.target\n",
            "        feature_names = data.feature_names\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load breast_cancer dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed, max_depth=5)\n",
            "    clf.fit(X_train, y_train)\n",
            "    acc = clf.score(X_test, y_test)\n",
            "    print(f\"RandomForest accuracy on test set: {acc:.3f}\")\n",
            "\n",
            "    if len(X_test) == 0:\n",
            "        print(\"TEST_FAIL: test set is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    explainer = shap.TreeExplainer(clf)\n",
            "    shap_values = explainer(X_test)\n",
            "\n",
            "    sample_idx = 0\n",
            "    shap.plots.waterfall(shap_values[sample_idx, :, 1], show=False)\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output, dpi=100, bbox_inches='tight')\n",
            "    plt.close()\n",
            "    print(f\"Saved SHAP waterfall plot to {args.output}\")\n",
            "\n",
            "    import os\n",
            "    if os.path.isfile(args.output):\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 50: Digits Grad-CAM Heatmap --- Latency: 22.97s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import os\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train CNN on digits, compute Grad-CAM heatmap, overlay on input, save as gradcam.png.\")\n",
            "    p.add_argument(\"--epochs\", type=int, default=3, help=\"Training epochs (default: 3).\")\n",
            "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"gradcam.png\", help=\"Output heatmap filename (default: gradcam.png).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    import torch\n",
            "    import torch.nn as nn\n",
            "    import torch.optim as optim\n",
            "    from torch.utils.data import DataLoader\n",
            "    from torchvision import datasets, transforms\n",
            "    import cv2\n",
            "\n",
            "    def load_mnist_or_fakedata(seed=42, allow_download=False):\n",
            "        try:\n",
            "            torch.manual_seed(seed)\n",
            "            tfm = transforms.ToTensor()\n",
            "            train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
            "            test = datasets.MNIST(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n",
            "            if len(train) == 0 or len(test) == 0:\n",
            "                raise RuntimeError(\"MNIST cache missing and download disabled\")\n",
            "            train = torch.utils.data.Subset(train, list(range(min(len(train), 2000))))\n",
            "            test = torch.utils.data.Subset(test, list(range(min(len(test), 500))))\n",
            "            return train, test, True\n",
            "        except Exception:\n",
            "            torch.manual_seed(seed)\n",
            "            tfm = transforms.ToTensor()\n",
            "            from torchvision.datasets import FakeData\n",
            "            train = FakeData(size=2000, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "            test = FakeData(size=500, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "            return train, test, False\n",
            "\n",
            "    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n",
            "    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
            "    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\n",
            "\n",
            "    class SimpleCNN(nn.Module):\n",
            "        def __init__(self):\n",
            "            super().__init__()\n",
            "            self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
            "            self.relu1 = nn.ReLU()\n",
            "            self.pool1 = nn.MaxPool2d(2)\n",
            "            self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
            "            self.relu2 = nn.ReLU()\n",
            "            self.pool2 = nn.MaxPool2d(2)\n",
            "            self.flatten = nn.Flatten()\n",
            "            self.fc1 = nn.Linear(32 * 7 * 7, 64)\n",
            "            self.relu3 = nn.ReLU()\n",
            "            self.fc2 = nn.Linear(64, 10)\n",
            "            self.gradients = None\n",
            "            self.activations = None\n",
            "\n",
            "        def forward(self, x):\n",
            "            x = self.conv1(x)\n",
            "            x = self.relu1(x)\n",
            "            x = self.pool1(x)\n",
            "            x = self.conv2(x)\n",
            "            x = self.relu2(x)\n",
            "            x = self.pool2(x)\n",
            "            if x.requires_grad:\n",
            "                x.register_hook(self.save_gradient)\n",
            "            self.activations = x\n",
            "            x = self.flatten(x)\n",
            "            x = self.fc1(x)\n",
            "            x = self.relu3(x)\n",
            "            x = self.fc2(x)\n",
            "            return x\n",
            "\n",
            "        def save_gradient(self, grad):\n",
            "            self.gradients = grad\n",
            "\n",
            "    model = SimpleCNN()\n",
            "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
            "    loss_fn = nn.CrossEntropyLoss()\n",
            "\n",
            "    model.train()\n",
            "    for epoch in range(args.epochs):\n",
            "        for xb, yb in train_loader:\n",
            "            opt.zero_grad()\n",
            "            logits = model(xb)\n",
            "            loss = loss_fn(logits, yb)\n",
            "            loss.backward()\n",
            "            opt.step()\n",
            "\n",
            "    model.eval()\n",
            "    correct = total = 0\n",
            "    with torch.no_grad():\n",
            "        for xb, yb in test_loader:\n",
            "\n",
            "            pred = model(xb).argmax(1)\n",
            "            correct += (pred == yb).sum().item()\n",
            "            total += yb.numel()\n",
            "    acc = correct / max(total, 1)\n",
            "    print(f\"acc={acc:.3f} dataset={'mnist' if real else 'fake'}\")\n",
            "\n",
            "    test_iter = iter(test_loader)\n",
            "    xb, yb = next(test_iter)\n",
            "    img = xb[0:1]\n",
            "    label = yb[0].item()\n",
            "\n",
            "    model.zero_grad()\n",
            "    img.requires_grad = True\n",
            "    output = model(img)\n",
            "    pred_class = output.argmax(1).item()\n",
            "    score = output[0, pred_class]\n",
            "    score.backward()\n",
            "\n",
            "    gradients = model.gradients\n",
            "    activations = model.activations\n",
            "\n",
            "    if gradients is None or activations is None:\n",
            "        print(\"TEST_FAIL: gradients or activations not captured\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
            "    for i in range(activations.shape[1]):\n",
            "        activations[:, i, :, :] *= pooled_gradients[i]\n",
            "\n",
            "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
            "    heatmap = torch.clamp(heatmap, min=0)\n",
            "    heatmap /= (torch.max(heatmap) + 1e-8)\n",
            "    heatmap_np = heatmap.detach().cpu().numpy()\n",
            "\n",
            "    heatmap_resized = cv2.resize(heatmap_np, (28, 28))\n",
            "    heatmap_uint8 = np.uint8(255 * heatmap_resized)\n",
            "    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
            "\n",
            "    img_np = img.squeeze().detach().cpu().numpy()\n",
            "    img_uint8 = np.uint8(255 * img_np)\n",
            "    img_color = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2BGR)\n",
            "\n",
            "    overlay = cv2.addWeighted(img_color, 0.5, heatmap_color, 0.5, 0)\n",
            "    cv2.imwrite(args.output, overlay)\n",
            "\n",
            "    if os.path.exists(args.output):\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file not created\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            ">>> anthropic/claude-sonnet-4.5 • JSON with title, description, code:\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Iris KNN Classifier\",\n",
            "    \"description\": \"Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.9.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Iris KNN classifier with k=3; prints TEST_PASS if accuracy >= 0.9.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=3, help=\\\"Number of neighbors for KNN (default: 3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # load iris dataset (no download required)\\n    try:\\n        data = load_iris()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # split into train/test\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train k-NN classifier\\n    knn = KNeighborsClassifier(n_neighbors=args.k)\\n    knn.fit(X_train, y_train)\\n\\n    # evaluate on test set\\n    accuracy = knn.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {accuracy:.3f}\\\")\\n\\n    # acceptance check\\n    if accuracy >= 0.9:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} < 0.9\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Logistic Regression\",\n",
            "    \"description\": \"Use sklearn wine dataset, scale features, train logistic regression, and report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Wine dataset logistic regression with feature scaling; TEST_PASS if accuracy >= 0.92.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\\n    clf.fit(X_train_scaled, y_train)\\n\\n    accuracy = clf.score(X_test_scaled, y_test)\\n    print(f\\\"Test accuracy: {accuracy:.4f}\\\")\\n\\n    if accuracy >= 0.92:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accu\n",
            "racy {accuracy:.4f} < 0.92\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SVM\",\n",
            "    \"description\": \"Train a linear SVM on sklearn breast_cancer dataset, report test accuracy. Print TEST_PASS if accuracy ≥ 0.93.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train linear SVM on breast_cancer dataset; report test accuracy.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--max-iter\\\", type=int, default=1000, help=\\\"Max iterations for LinearSVC (default: 1000).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"svm\\\", LinearSVC(max_iter=args.max_iter, random_state=args.seed))\\n    ])\\n\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    print(f\\\"test_accuracy={acc:.4f}\\\")\\n\\n    if acc >= 0.93:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.93\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Linear Classifier\",\n",
            "    \"description\": \"Classify sklearn digits using LogisticRegression, report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Classify sklearn digits using LogisticRegression; TEST_PASS if accuracy >= 0.92.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--max-iter\\\", type=int, default=1000, help=\\\"Max iterations for LogisticRegression (default: 1000).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"lr\\\", LogisticRegression(max_iter=args.max_iter, random_state=args.seed))\\n    ])\\n\\n    clf.fit(X_train, y_train)\\n\n",
            "    acc = clf.score(X_test, y_test)\\n\\n    print(f\\\"Test accuracy: {acc:.4f}\\\")\\n\\n    if acc >= 0.92:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.92\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Ridge Regression\",\n",
            "    \"description\": \"Fit Ridge regression on sklearn diabetes dataset, compute R² score. Print TEST_PASS if R² ≥ 0.4.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.metrics import r2_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Ridge regression on sklearn diabetes dataset; TEST_PASS if R² ≥ 0.4.\\\")\\n    p.add_argument(\\\"--alpha\\\", type=float, default=1.0, help=\\\"Ridge regularization strength (default: 1.0).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: diabetes dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    model = Ridge(alpha=args.alpha, random_state=args.seed)\\n    model.fit(X_train, y_train)\\n\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"R² score: {r2:.4f}\\\")\\n\\n    if r2 >= 0.4:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R² score {r2:.4f} below threshold 0.4\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons Decision Tree\",\n",
            "    \"description\": \"Generate make_moons data (n=300), train decision tree, plot decision boundary, save as 'moons_tree.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_moons data, train decision tree, plot decision boundary, save as moons_tree.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.2, help=\\\"Noise level for make_moons (default: 0.2).\\\")\\n    p.add_argument(\\\"--max-depth\\\", type=int, default=5, help=\\\"Max depth of decision tree (default: 5).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"moons_tree.png\\\", help=\\\"Output filename for decision boundary plot (default: moons_tree.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Generate make_moons data\\n    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # Train decision tree\\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\\n    clf.fit(X, y)\\n\\n    #\n",
            " Create decision boundary plot\\n    h = 0.02\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    \\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap=plt.cm.RdYlBu)\\n    plt.xlabel(\\\"Feature 1\\\")\\n    plt.ylabel(\\\"Feature 2\\\")\\n    plt.title(\\\"Decision Tree Boundary on make_moons\\\")\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    # Acceptance check\\n    import os\\n    if not os.path.exists(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n    \\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    acc = clf.score(X, y)\\n    print(f\\\"Training accuracy: {acc:.3f}\\\")\\n    print(f\\\"Decision boundary saved to {args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles Random Forest\",\n",
            "    \"description\": \"Generate make_circles data (n=300), train random forest, report accuracy. Print TEST_PASS if accuracy ≥ 0.88.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_circles data (n=300), train random forest, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=100, help=\\\"Number of trees in random forest (default: 100).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_circles(n_samples=args.n_samples, noise=0.1, factor=0.5, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\\n    \\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\\n    clf.fit(Xtr, ytr)\\n    \\n    acc = clf.score(Xte, yte)\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n    \\n    if acc >= 0.88:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold 0.88\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Blobs KMeans Clustering\",\n",
            "    \"description\": \"Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score. Print TEST_PASS if silhouette ≥ 0.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import silhouette_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of samples (default: 200).\\\")\\n    p.add_argument(\\\"--centers\\\", type=int, default=4, help=\\\"Number of cluster centers (default: 4).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, defau\n",
            "lt=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples <= 0:\\n        print(\\\"TEST_FAIL: n_samples must be positive\\\")\\n        sys.exit(1)\\n    if args.centers <= 0:\\n        print(\\\"TEST_FAIL: centers must be positive\\\")\\n        sys.exit(1)\\n\\n    X, y_true = make_blobs(n_samples=args.n_samples, centers=args.centers, random_state=args.seed)\\n    \\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    kmeans = KMeans(n_clusters=args.centers, random_state=args.seed, n_init=10)\\n    y_pred = kmeans.fit_predict(X)\\n\\n    sil = silhouette_score(X, y_pred)\\n    print(f\\\"n_samples={args.n_samples} centers={args.centers} silhouette={sil:.3f}\\\")\\n\\n    if sil >= 0.5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: silhouette score {sil:.3f} below threshold 0.5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Classification Synthetic Data\",\n",
            "    \"description\": \"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.85.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=500, help=\\\"Number of samples (default: 500).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=4, help=\\\"Number of features (default: 4).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_classification(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_informative=max(2, args.n_features // 2),\\n        n_redundant=0,\\n        n_clusters_per_class=1,\\n        random_state=args.seed\\n    )\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    y_pred = clf.predict(X_test)\\n    acc = accuracy_score(y_test, y_pred)\\n\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.85:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.85\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Regression Synthetic Data\",\n",
            "    \"description\": \"Generate make_regression data (n=300), fit LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate synthetic regression data (n=300), fit LinearRegression, compute R2; TEST_PASS if R2 >= 0.3.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, he\n",
            "lp=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=10, help=\\\"Number of features (default: 10).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=10.0, help=\\\"Standard deviation of Gaussian noise (default: 10.0).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Generate synthetic regression data\\n    X, y = make_regression(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        noise=args.noise,\\n        random_state=args.seed\\n    )\\n\\n    # Validate dataset\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    # Fit LinearRegression\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    # Predict and compute R²\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"n_samples={args.n_samples} n_features={args.n_features} noise={args.noise}\\\")\\n    print(f\\\"R2={r2:.4f}\\\")\\n\\n    # Acceptance check\\n    if r2 >= 0.3:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R2={r2:.4f} below threshold 0.3\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"PCA on Digits\",\n",
            "    \"description\": \"Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as 'digits_pca.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.decomposition import PCA\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as digits_pca.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"digits_pca.png\\\", help=\\\"Output PNG file path (default: digits_pca.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X = data.data\\n        y = data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    pca = PCA(n_components=2, random_state=args.seed)\\n    X_pca = pca.fit_transform(X)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\\n    plt.colorbar(scatter, label='Digit')\\n    plt.xlabel('First Principal Component')\\n    plt.ylabel('Second Principal Component')\\n    plt.title('PCA on Digits Dataset')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Saved PCA scatter plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Pairplot Visualization\",\n",
            "    \"description\": \"Plot pairwise feature scatt\n",
            "er plots for iris dataset using matplotlib, save as 'iris_pairplot.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_iris_or_synthetic(seed=42):\\n    try:\\n        from sklearn.datasets import load_iris\\n        data = load_iris()\\n        X = data.data\\n        y = data.target\\n        feature_names = data.feature_names\\n        used = \\\"iris\\\"\\n    except Exception:\\n        rng = np.random.default_rng(seed)\\n        n = 150\\n        c = rng.integers(0, 3, size=n)\\n        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\\n        y = c\\n        feature_names = [\\\"sepal_length\\\", \\\"sepal_width\\\", \\\"petal_length\\\", \\\"petal_width\\\"]\\n        used = \\\"synthetic\\\"\\n    return X, y, feature_names, used\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Plot pairwise feature scatter plots for iris dataset; save as iris_pairplot.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"iris_pairplot.png\\\", help=\\\"Output filename (default: iris_pairplot.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y, feature_names, used = load_iris_or_synthetic(args.seed)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    n_features = X.shape[1]\\n    fig, axes = plt.subplots(n_features, n_features, figsize=(12, 12))\\n    \\n    colors = ['red', 'green', 'blue']\\n    for i in range(n_features):\\n        for j in range(n_features):\\n            ax = axes[i, j]\\n            if i == j:\\n                for cls in np.unique(y):\\n                    ax.hist(X[y == cls, i], bins=15, alpha=0.6, color=colors[cls % len(colors)], label=f\\\"Class {cls}\\\")\\n                ax.set_ylabel(\\\"Frequency\\\")\\n            else:\\n                for cls in np.unique(y):\\n                    ax.scatter(X[y == cls, j], X[y == cls, i], alpha=0.6, s=10, color=colors[cls % len(colors)], label=f\\\"Class {cls}\\\")\\n            \\n            if i == n_features - 1:\\n                ax.set_xlabel(feature_names[j])\\n            else:\\n                ax.set_xticklabels([])\\n            \\n            if j == 0:\\n                ax.set_ylabel(feature_names[i])\\n            else:\\n                ax.set_yticklabels([])\\n            \\n            if i == 0 and j == n_features - 1:\\n                ax.legend(loc='upper right', fontsize=8)\\n\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    print(f\\\"dataset={used} saved={args.output}\\\")\\n\\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file missing or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Feature Importance\",\n",
            "    \"description\": \"Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart as 'wine_importance.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"wine_importance.png\\\", help=\\\"Output filename for feature importance bar chart (default: wine_importance.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=i\n",
            "nt, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=100, help=\\\"Number of trees in RandomForest (default: 100).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X, y = data.data, data.target\\n        feature_names = data.feature_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: could not load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    importances = clf.feature_importances_\\n    if importances is None or len(importances) == 0:\\n        print(\\\"TEST_FAIL: feature importances are empty\\\")\\n        sys.exit(1)\\n\\n    indices = np.argsort(importances)[::-1]\\n\\n    plt.figure(figsize=(10, 6))\\n    plt.bar(range(len(importances)), importances[indices], align='center')\\n    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')\\n    plt.xlabel('Feature')\\n    plt.ylabel('Importance')\\n    plt.title('Wine Dataset Feature Importances')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    if os.path.getsize(args.output) == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"accuracy={acc:.3f} output={args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Confusion Matrix\",\n",
            "    \"description\": \"Train LogisticRegression on breast_cancer dataset, plot confusion matrix, save as 'bc_confusion.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train LogisticRegression on breast_cancer, plot confusion matrix, save as bc_confusion.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"bc_confusion.png\\\", help=\\\"Output confusion matrix image path (default: bc_confusion.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = LogisticRegression(max_iter=\n",
            "5000, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n    y_pred = clf.predict(X_test)\\n\\n    cm = confusion_matrix(y_test, y_pred)\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\\n    \\n    fig, ax = plt.subplots(figsize=(8, 6))\\n    disp.plot(ax=ax, cmap='Blues')\\n    plt.title('Breast Cancer Confusion Matrix')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    if os.path.getsize(args.output) == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"Confusion matrix saved to {args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Confusion Heatmap\",\n",
            "    \"description\": \"Train SVC on digits dataset, plot confusion matrix heatmap, save as 'digits_heatmap.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nfrom sklearn.metrics import confusion_matrix\\nimport seaborn as sns\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train SVC on digits dataset, plot confusion matrix heatmap, save as digits_heatmap.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"digits_heatmap.png\\\", help=\\\"Output heatmap filename (default: digits_heatmap.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.3, help=\\\"Test set fraction (default: 0.3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\\n\\n    clf = SVC(kernel='rbf', gamma='scale', random_state=args.seed)\\n    clf.fit(Xtr, ytr)\\n    ypred = clf.predict(Xte)\\n\\n    cm = confusion_matrix(yte, ypred)\\n\\n    plt.figure(figsize=(8, 6))\\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\\n    plt.xlabel('Predicted')\\n    plt.ylabel('Actual')\\n    plt.title('Digits Confusion Matrix')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"Confusion matrix heatmap saved to {args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Residual Plot\",\n",
            "    \"description\": \"Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as 'residuals_plot.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error\\nimport os\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as residua\n",
            "ls_plot.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"residuals_plot.png\\\", help=\\\"Output filename for residual plot (default: residuals_plot.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: diabetes dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    y_pred = model.predict(X_test)\\n    residuals = y_test - y_pred\\n    mse = mean_squared_error(y_test, y_pred)\\n\\n    print(f\\\"Test MSE: {mse:.2f}\\\")\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.scatter(y_pred, residuals, alpha=0.6, edgecolors='k')\\n    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\\n    plt.xlabel('Predicted Values')\\n    plt.ylabel('Residuals')\\n    plt.title('Residual Plot: Diabetes Dataset')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n\\n    try:\\n        plt.savefig(args.output, dpi=100)\\n        print(f\\\"Residual plot saved to {args.output}\\\")\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Audio Classification\",\n",
            "    \"description\": \"Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_audio(n_samples=250, n_features=20, seed=42):\\n    rng = np.random.default_rng(seed)\\n    X = []\\n    y = []\\n    for i in range(n_samples):\\n        if i < n_samples // 2:\\n            # high pitch: higher frequency components\\n            freq_base = rng.uniform(800, 1200)\\n            label = 1\\n        else:\\n            # low pitch: lower frequency components\\n            freq_base = rng.uniform(100, 400)\\n            label = 0\\n        # simulate audio features (e.g., spectral coefficients)\\n        features = rng.normal(freq_base, 50, size=n_features)\\n        X.append(features)\\n        y.append(label)\\n    return np.array(X), np.array(y)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=250, help=\\\"Number of synthetic audio samples (default: 250).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=20, help=\\\"Number of features per sample (default: 20).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(arg\n",
            "s.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # generate synthetic audio dataset\\n    X, y = generate_synthetic_audio(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train MLP\\n    clf = MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=300, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    # predict and evaluate\\n    y_pred = clf.predict(X_test)\\n    acc = accuracy_score(y_test, y_pred)\\n\\n    print(f\\\"dataset=synthetic_audio n_samples={args.n_samples} accuracy={acc:.3f}\\\")\\n\\n    # acceptance: accuracy >= 0.7\\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Time Series Forecast\",\n",
            "    \"description\": \"Generate 200 synthetic time series samples, predict next value using lag features, compute R². Print TEST_PASS if R² ≥ 0.25.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\n\\ndef generate_synthetic_time_series(n_samples=200, seed=42):\\n    \\\"\\\"\\\"Generate synthetic time series with trend, seasonality, and noise.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    t = np.arange(n_samples)\\n    trend = 0.05 * t\\n    seasonality = 10 * np.sin(2 * np.pi * t / 20)\\n    noise = rng.normal(0, 2, size=n_samples)\\n    series = trend + seasonality + noise\\n    return series\\n\\ndef create_lag_features(series, n_lags=3):\\n    \\\"\\\"\\\"Create lag features for time series prediction.\\\"\\\"\\\"\\n    X = []\\n    y = []\\n    for i in range(n_lags, len(series)):\\n        X.append(series[i-n_lags:i])\\n        y.append(series[i])\\n    return np.array(X), np.array(y)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 200 synthetic time series samples, predict next value using lag features, compute R².\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of time series samples (default: 200).\\\")\\n    p.add_argument(\\\"--n-lags\\\", type=int, default=3, help=\\\"Number of lag features (default: 3).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples < 50:\\n        print(\\\"TEST_FAIL: n_samples must be at least 50\\\")\\n        sys.exit(1)\\n    if args.n_lags < 1:\\n        print(\\\"TEST_FAIL: n_lags must be at least 1\\\")\\n        sys.exit(1)\\n    if not (0 < args.test_size < 1):\\n        print(\\\"TEST_FAIL: test_size must be between 0 and 1\\\")\\n        sys.exit(1)\\n\\n    series = generate_synthetic_time_series(n_samples=args.n_samples, seed=args.seed)\\n    X, y = create_lag_features(series, n_lags=args.n_lags)\\n\\n    if len(X) == 0 or len(y) == 0:\\n        print(\\\"TEST_FAIL: insufficient data after creating lag features\\\")\\n        sys.exit(1)\\n\\n    split_idx = int(len(X) * (1 - args.test_size))\\n    if split_idx < 1 or split_idx >= len(X):\\n        print(\\\"TEST_FAIL: invalid train/test split\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test = X[:split_idx], X[split_idx:]\\n    y_train, y_test = y[:split_idx], y[split_idx:]\\n\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"n_sa\n",
            "mples={args.n_samples} n_lags={args.n_lags} r2={r2:.4f}\\\")\\n\\n    if r2 >= 0.25:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R² {r2:.4f} below threshold 0.25\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Tabular Regression\",\n",
            "    \"description\": \"Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n\\ndef generate_synthetic_tabular(n_samples=300, n_features=5, seed=42):\\n    rng = np.random.default_rng(seed)\\n    X = rng.normal(0, 1, size=(n_samples, n_features))\\n    true_coef = rng.uniform(-2, 2, size=n_features)\\n    y = X @ true_coef + rng.normal(0, 0.5, size=n_samples)\\n    return X, y\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R².\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of synthetic samples (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=5, help=\\\"Number of features (default: 5).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = generate_synthetic_tabular(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n    \\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\\n    \\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n    \\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n    \\n    print(f\\\"n_samples={args.n_samples} n_features={args.n_features} R²={r2:.3f}\\\")\\n    \\n    if r2 >= 0.3:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R²={r2:.3f} below threshold 0.3\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Text Sentiment\",\n",
            "    \"description\": \"Create 200 short synthetic sentences labeled positive or negative, vectorize with CountVectorizer, train a LogisticRegression, and print accuracy; print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n\\ndef generate_synthetic_sentences(n=200, seed=42):\\n    rng = np.random.default_rng(seed)\\n    positive_templates = [\\n        \\\"I love this product\\\",\\n        \\\"This is amazing\\\",\\n        \\\"Great experience\\\",\\n        \\\"Wonderful service\\\",\\n        \\\"Highly recommend\\\",\\n        \\\"Excellent quality\\\",\\n        \\\"Very satisfied\\\",\\n        \\\"Best purchase ever\\\",\\n        \\\"Fantastic results\\\",\\n        \\\"Really enjoyed it\\\"\\n    ]\\n    negative_templates = [\\n        \\\"I hate this product\\\",\\n        \\\"This is terrible\\\",\\n        \\\"Bad experience\\\",\\n        \\\"Poor service\\\",\\n        \\\"Do not recommend\\\",\\n        \\\"Awful quality\\\",\\n        \\\"Very disappointed\\\",\\n        \\\"Worst purchase ever\\\",\\n        \\\"Horrible results\\\",\\n        \\\"Really disliked it\\\"\\n    ]\\n    sentences = []\\n    labels = []\\n    for i in range(n):\\n        if i % 2 == 0:\\n            template = positive_templates[rng.integers(0, len(positive_tem\n",
            "plates))]\\n            label = 1\\n        else:\\n            template = negative_templates[rng.integers(0, len(negative_templates))]\\n            label = 0\\n        sentences.append(template)\\n        labels.append(label)\\n    return sentences, np.array(labels)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 200 synthetic sentiment sentences, vectorize with CountVectorizer, train LogisticRegression, print TEST_PASS if accuracy >= 0.7.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of synthetic sentences (default: 200).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    sentences, labels = generate_synthetic_sentences(n=args.n_samples, seed=args.seed)\\n    if len(sentences) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: no sentences generated\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        sentences, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\\n    )\\n\\n    vectorizer = CountVectorizer()\\n    X_train_vec = vectorizer.fit_transform(X_train)\\n    X_test_vec = vectorizer.transform(X_test)\\n\\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\\n    clf.fit(X_train_vec, y_train)\\n\\n    acc = clf.score(X_test_vec, y_test)\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST CNN Classifier\",\n",
            "    \"description\": \"If MNIST available, train simple CNN (2 conv layers), else generate FakeData (32x32 grayscale), train CNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.85 or fallback ≥ 0.6.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_mnist_or_fakedata(seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([transforms.ToTensor()])\\n        train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.MNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n        return train, test, True\\n    except Exception:\\n        import torch\\n        from torchvision.datasets import FakeData\\n        from torchvision import transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([transforms.ToTensor()])\\n        train = FakeData(size=1000, image_size=(1, 32, 32), num_classes=10, transform=tfm)\\n        test = FakeData(size=200, image_size=(1, 32, 32), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n\\n    p = argparse.ArgumentParser(description=\\\"MNIST CNN classifier with FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=2, help=\\\"Training epochs (default: 2).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\n",
            "\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    class SimpleCNN(nn.Module):\\n        def __init__(self, input_size=28):\\n            super().__init__()\\n            self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\\n            self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\\n            self.pool = nn.MaxPool2d(2, 2)\\n            self.relu = nn.ReLU()\\n            conv_out_size = (input_size // 4) * (input_size // 4) * 32\\n            self.fc1 = nn.Linear(conv_out_size, 64)\\n            self.fc2 = nn.Linear(64, 10)\\n\\n        def forward(self, x):\\n            x = self.pool(self.relu(self.conv1(x)))\\n            x = self.pool(self.relu(self.conv2(x)))\\n            x = x.view(x.size(0), -1)\\n            x = self.relu(self.fc1(x))\\n            x = self.fc2(x)\\n            return x\\n\\n    input_size = 28 if real else 32\\n    model = SimpleCNN(input_size=input_size)\\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\\n    criterion = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            optimizer.zero_grad()\\n            outputs = model(xb)\\n            loss = criterion(outputs, yb)\\n            loss.backward()\\n            optimizer.step()\\n\\n    model.eval()\\n    correct = 0\\n    total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            outputs = model(xb)\\n            _, predicted = torch.max(outputs.data, 1)\\n            total += yb.size(0)\\n            correct += (predicted == yb).sum().item()\\n\\n    accuracy = correct / max(total, 1)\\n    dataset_name = \\\"MNIST\\\" if real else \\\"FakeData\\\"\\n    print(f\\\"dataset={dataset_name} accuracy={accuracy:.3f}\\\")\\n\\n    threshold = 0.85 if real else 0.6\\n    if accuracy >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} below threshold {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST KNN\",\n",
            "    \"description\": \"If FashionMNIST available, flatten images, train KNN (k=5), else use FakeData, train KNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.7 or fallback ≥ 0.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_fashionmnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.FashionMNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.FashionMNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"FashionMNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\\n        return train, test, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        test = FakeData(size=max_test, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"FashionMNIST KNN (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argume\n",
            "nt(\\\"--k\\\", type=int, default=5, help=\\\"Number of neighbors for KNN (default: 5).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit FashionMNIST download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    train_ds, test_ds, real = load_fashionmnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n\\n    X_train = []\\n    y_train = []\\n    for img, label in train_ds:\\n        flat = img.numpy().flatten()\\n        X_train.append(flat)\\n        y_train.append(label)\\n    X_train = np.array(X_train)\\n    y_train = np.array(y_train)\\n\\n    X_test = []\\n    y_test = []\\n    for img, label in test_ds:\\n        flat = img.numpy().flatten()\\n        X_test.append(flat)\\n        y_test.append(label)\\n    X_test = np.array(X_test)\\n    y_test = np.array(y_test)\\n\\n    if len(X_train) == 0 or len(X_test) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    from sklearn.neighbors import KNeighborsClassifier\\n    knn = KNeighborsClassifier(n_neighbors=args.k)\\n    knn.fit(X_train, y_train)\\n    acc = knn.score(X_test, y_test)\\n\\n    print(f\\\"dataset={'fashionmnist' if real else 'fake'} k={args.k} accuracy={acc:.3f}\\\")\\n\\n    threshold = 0.7 if real else 0.5\\n    if acc >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"CIFAR10 Class Balance\",\n",
            "    \"description\": \"If CIFAR10 available, count class frequencies, else generate FakeData with 10 classes, count, save bar chart as 'class_balance.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_cifar10_or_fakedata(seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets\\n        torch.manual_seed(seed)\\n        ds = datasets.CIFAR10(root=\\\"./data\\\", train=True, download=bool(allow_download))\\n        if len(ds) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        labels = [y for _, y in ds]\\n        return labels, True\\n    except Exception:\\n        import torch\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        ds = FakeData(size=1000, image_size=(3, 32, 32), num_classes=10)\\n        labels = [y for _, y in ds]\\n        return labels, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Count CIFAR10 class frequencies or FakeData fallback; save bar chart; seeds in main.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"class_balance.png\\\", help=\\\"Output bar chart filename (default: class_balance.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    labels, real = load_cifar10_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    if labels is None or len(labels) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    counts = np.bincount(labels, minlength=10)\\n    print(f\\\"dataset={'cifar10' if real else 'fake'} class_counts={counts.tolist()}\\\")\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.bar(range(10), counts, color='steelblue')\\n    plt.xlabel(\\\"Class\\\")\\n    plt.ylabel(\\\"Fr\n",
            "equency\\\")\\n    plt.title(\\\"Class Balance\\\")\\n    plt.xticks(range(10))\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"ImageFolder Histogram\",\n",
            "    \"description\": \"If ImageFolder available, compute average RGB histogram, else generate 50 FakeData images, compute histogram, save as 'histogram.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_imagefolder_or_fakedata(root_dir, num_fake=50, seed=42, allow_download=False):\\n    \\\"\\\"\\\"\\n    Try to load ImageFolder from root_dir if it exists and contains images.\\n    Otherwise fall back to FakeData.\\n    Returns (dataset, is_real).\\n    \\\"\\\"\\\"\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        \\n        # Check if root_dir exists and has subdirectories with images\\n        if os.path.isdir(root_dir):\\n            # Try to load ImageFolder\\n            tfm = transforms.ToTensor()\\n            ds = datasets.ImageFolder(root=root_dir, transform=tfm)\\n            if len(ds) > 0:\\n                return ds, True\\n        \\n        # Fallback to FakeData\\n        raise RuntimeError(\\\"ImageFolder not available or empty\\\")\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        ds = FakeData(size=num_fake, image_size=(3, 64, 64), num_classes=2, transform=tfm)\\n        return ds, False\\n\\ndef compute_rgb_histogram(dataset, num_samples=None):\\n    \\\"\\\"\\\"\\n    Compute average RGB histogram across dataset.\\n    Returns (r_hist, g_hist, b_hist) each of shape (256,).\\n    \\\"\\\"\\\"\\n    if num_samples is None:\\n        num_samples = len(dataset)\\n    else:\\n        num_samples = min(num_samples, len(dataset))\\n    \\n    r_sum = np.zeros(256, dtype=np.float64)\\n    g_sum = np.zeros(256, dtype=np.float64)\\n    b_sum = np.zeros(256, dtype=np.float64)\\n    \\n    for i in range(num_samples):\\n        img_tensor, _ = dataset[i]\\n        # img_tensor is (C, H, W) in [0, 1]\\n        img_np = (img_tensor.numpy() * 255).astype(np.uint8)\\n        \\n        r_hist, _ = np.histogram(img_np[0].flatten(), bins=256, range=(0, 256))\\n        g_hist, _ = np.histogram(img_np[1].flatten(), bins=256, range=(0, 256))\\n        b_hist, _ = np.histogram(img_np[2].flatten(), bins=256, range=(0, 256))\\n        \\n        r_sum += r_hist\\n        g_sum += g_hist\\n        b_sum += b_hist\\n    \\n    # Average\\n    r_avg = r_sum / num_samples\\n    g_avg = g_sum / num_samples\\n    b_avg = b_sum / num_samples\\n    \\n    return r_avg, g_avg, b_avg\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Compute average RGB histogram from ImageFolder or FakeData and save as histogram.png.\\\")\\n    p.add_argument(\\\"--root-dir\\\", type=str, default=\\\"./imagefolder_data\\\", help=\\\"Path to ImageFolder root directory (default: ./imagefolder_data).\\\")\\n    p.add_argument(\\\"--num-fake\\\", type=int, default=50, help=\\\"Number of FakeData images if ImageFolder unavailable (default: 50).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"histogram.png\\\", help=\\\"Output histogram file path (default: histogram.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit dataset download if needed.\\\")\\n    args = p.parse_args()\\n    \\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        t\n",
            "orch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n    \\n    # Load dataset\\n    dataset, is_real = load_imagefolder_or_fakedata(\\n        root_dir=args.root_dir,\\n        num_fake=args.num_fake,\\n        seed=args.seed,\\n        allow_download=args.allow_download\\n    )\\n    \\n    if dataset is None or len(dataset) == 0:\\n        print(\\\"TEST_FAIL: dataset not available or empty\\\")\\n        sys.exit(1)\\n    \\n    print(f\\\"Using {'ImageFolder' if is_real else 'FakeData'} with {len(dataset)} images\\\")\\n    \\n    # Compute histogram\\n    r_hist, g_hist, b_hist = compute_rgb_histogram(dataset)\\n    \\n    # Plot and save\\n    fig, ax = plt.subplots(figsize=(10, 6))\\n    bins = np.arange(256)\\n    ax.plot(bins, r_hist, color='red', alpha=0.7, label='Red')\\n    ax.plot(bins, g_hist, color='green', alpha=0.7, label='Green')\\n    ax.plot(bins, b_hist, color='blue', alpha=0.7, label='Blue')\\n    ax.set_xlabel('Pixel Value')\\n    ax.set_ylabel('Average Frequency')\\n    ax.set_title('Average RGB Histogram')\\n    ax.legend()\\n    ax.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    \\n    # Acceptance check\\n    if os.path.isfile(args.output):\\n        file_size = os.path.getsize(args.output)\\n        if file_size > 0:\\n            print(f\\\"Histogram saved to {args.output} ({file_size} bytes)\\\")\\n            print(\\\"TEST_PASS\\\")\\n        else:\\n            print(\\\"TEST_FAIL: output file is empty\\\")\\n            sys.exit(1)\\n    else:\\n        print(\\\"TEST_FAIL: output file does not exist\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FakeData GAN Discriminator\",\n",
            "    \"description\": \"Generate FakeData images, train simple CNN discriminator, report accuracy distinguishing real/fake. Print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train CNN discriminator on FakeData (real vs fake labels); print TEST_PASS if accuracy >= 0.7.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=64, help=\\\"Batch size (default: 64).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-real\\\", type=int, default=500, help=\\\"Number of real samples (default: 500).\\\")\\n    p.add_argument(\\\"--n-fake\\\", type=int, default=500, help=\\\"Number of fake samples (default: 500).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader, TensorDataset\\n    from torchvision.datasets import FakeData\\n    from torchvision import transforms\\n\\n    tfm = transforms.ToTensor()\\n    real_ds = FakeData(size=args.n_real, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n    fake_ds = FakeData(size=args.n_fake, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n\\n    real_images = []\\n    fake_images = []\\n    for i in range(args.n_real):\\n        img, _ = real_ds[i]\\n        real_images.append(img)\\n    for i in range(args.n_fake):\\n        img, _ = fake_ds[i]\\n        fake_images.append(img)\\n\\n    real_images = torch.stack(real_images)\\n    fake_images = torch.stack(fake_images)\\n    real_labels = torch.ones(args.n_real, dtype=torch.long)\\n    fake_labels = torch.zeros(args.n_fake, dtype=torch.long)\\n\\n    X = torch.cat([real_images, fake_images], dim=0)\\n    y = torch.cat([real_labels, fake_labels], dim=0)\\n\\n    perm = torch.randperm(len(X))\\n    X = X[perm]\\n    y = y[perm]\\n\\n    split = int(0.8 * len(X))\\n    X_train, X_test = X[:split], X[split:]\\n    y_train, y_test = y[:split], y[split:]\\n\\n \n",
            "   train_ds = TensorDataset(X_train, y_train)\\n    test_ds = TensorDataset(X_test, y_test)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    class Discriminator(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.net = nn.Sequential(\\n                nn.Conv2d(1, 16, 3, 1, 1), nn.ReLU(),\\n                nn.MaxPool2d(2),\\n                nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(),\\n                nn.MaxPool2d(2),\\n                nn.Flatten(),\\n                nn.Linear(32 * 7 * 7, 64), nn.ReLU(),\\n                nn.Linear(64, 2)\\n            )\\n        def forward(self, x):\\n            return self.net(x)\\n\\n    model = Discriminator()\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n    loss_fn = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            opt.zero_grad()\\n            logits = model(xb)\\n            loss = loss_fn(logits, yb)\\n            loss.backward()\\n            opt.step()\\n\\n    model.eval()\\n    correct = 0\\n    total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            pred = model(xb).argmax(1)\\n            correct += (pred == yb).sum().item()\\n            total += yb.numel()\\n\\n    acc = correct / max(total, 1)\\n    print(f\\\"discriminator_accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} < 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"KMeans on CIFAR Colors\",\n",
            "    \"description\": \"If CIFAR10 available, reshape to pixels, run KMeans (k=5) on RGB values, else use FakeData, compute cluster inertia. Print TEST_PASS if inertia < 1.0e5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_cifar_or_fakedata(max_samples=5000, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.CIFAR10(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        subset = torch.utils.data.Subset(train, list(range(min(len(train), max_samples))))\\n        loader = torch.utils.data.DataLoader(subset, batch_size=max_samples, shuffle=False)\\n        for xb, _ in loader:\\n            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\\n            return pixels, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        fake = FakeData(size=max_samples, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        loader = torch.utils.data.DataLoader(fake, batch_size=max_samples, shuffle=False)\\n        for xb, _ in loader:\\n            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\\n            return pixels, False\\n    return None, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"KMeans on CIFAR10 RGB pixels (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=5, help=\\\"Number of clusters (default: 5).\\\")\\n    p.add_argument(\\\"--max-samples\\\", type=int, default=5000, help=\\\"Max images to load (default: 5000).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manu\n",
            "al_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    pixels, real = load_cifar_or_fakedata(max_samples=args.max_samples, seed=args.seed, allow_download=args.allow_download)\\n    if pixels is None or len(pixels) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    from sklearn.cluster import KMeans\\n    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\\n    kmeans.fit(pixels)\\n    inertia = kmeans.inertia_\\n\\n    print(f\\\"dataset={'cifar10' if real else 'fake'} k={args.k} inertia={inertia:.2e}\\\")\\n\\n    if inertia < 1.0e5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: inertia >= 1.0e5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Decision Boundary\",\n",
            "    \"description\": \"Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as 'iris_boundary.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as iris_boundary.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"iris_boundary.png\\\", help=\\\"Output image path (default: iris_boundary.png).\\\")\\n    p.add_argument(\\\"--max-depth\\\", type=int, default=3, help=\\\"Max depth of decision tree (default: 3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_iris()\\n        X = data.data[:, :2]\\n        y = data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: could not load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\\n    clf.fit(X, y)\\n\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    h = 0.02\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolor='k', cmap='viridis')\\n    plt.xlabel(data.feature_names[0])\\n    plt.ylabel(data.feature_names[1])\\n    plt.title(\\\"Iris Decision Boundary (first two features)\\\")\\n    plt.colorbar(scatter)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Decision boundary saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine PCA Scatter\",\n",
            "    \"description\": \"Apply PCA to wine dataset (2 components), scatter plot colored by target, save as 'wine_pca.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply PCA to wine dataset (2 components), scatter plot colored by target, save as wine_pca.png.\\\")\\\n",
            "n    p.add_argument(\\\"--output\\\", type=str, default=\\\"wine_pca.png\\\", help=\\\"Output filename (default: wine_pca.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X = data.data\\n        y = data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    pca = PCA(n_components=2, random_state=args.seed)\\n    X_pca = pca.fit_transform(X_scaled)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, edgecolors='k')\\n    plt.colorbar(scatter, label='Wine Class')\\n    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\\n    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\\n    plt.title('Wine Dataset PCA (2 Components)')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(f\\\"Saved PCA scatter plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist or is empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer ROC Curve\",\n",
            "    \"description\": \"Train LogisticRegression on breast_cancer, compute ROC curve, save as 'roc_curve.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import roc_curve, auc\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train LogisticRegression on breast_cancer, compute ROC curve, save as roc_curve.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"roc_curve.png\\\", help=\\\"Output ROC curve image path (default: roc_curve.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.3, help=\\\"Test set fraction (default: 0.3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = LogisticRegression(max_iter=5000, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    y_scores = clf.predict_proba(X_test)[:, 1]\\n\\n    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\\n    roc_auc = auc(fpr, tpr)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\\n    plt.xlim([0.0, 1.0])\\n    plt.ylim([0.0, 1.05])\\n    plt.xlabel('False Positive Rate')\\n    \n",
            "plt.ylabel('True Positive Rate')\\n    plt.title('Receiver Operating Characteristic - Breast Cancer')\\n    plt.legend(loc='lower right')\\n    plt.grid(alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"ROC curve saved to {args.output}, AUC={roc_auc:.3f}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits t-SNE Plot\",\n",
            "    \"description\": \"Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as 'tsne_digits.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.manifold import TSNE\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as tsne_digits.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"tsne_digits.png\\\", help=\\\"Output PNG file path (default: tsne_digits.png).\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=500, help=\\\"Number of samples to use (default: 500).\\\")\\n    p.add_argument(\\\"--perplexity\\\", type=float, default=30.0, help=\\\"t-SNE perplexity (default: 30.0).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    n = min(args.n_samples, len(X))\\n    indices = np.random.choice(len(X), size=n, replace=False)\\n    X_sub = X[indices]\\n    y_sub = y[indices]\\n\\n    try:\\n        tsne = TSNE(n_components=2, perplexity=args.perplexity, random_state=args.seed)\\n        X_embedded = tsne.fit_transform(X_sub)\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: t-SNE failed: {e}\\\")\\n        sys.exit(1)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_sub, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\\n    plt.colorbar(scatter, label='Digit')\\n    plt.title('t-SNE of Digits Dataset')\\n    plt.xlabel('t-SNE Component 1')\\n    plt.ylabel('t-SNE Component 2')\\n    plt.tight_layout()\\n\\n    try:\\n        plt.savefig(args.output, dpi=100)\\n        plt.close()\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Saved t-SNE plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file does not exist\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Cross Validation\",\n",
            "    \"description\": \"Run 5-fold CV on diabetes dataset with Ridge regression, compute mean R². Print TEST_PASS if mean R² ≥ 0.35.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.model_selection import cross_val_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Run 5-fold CV on diabetes dataset with Ridge regression; print TEST_PASS if mean R² ≥ 0.35.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--alpha\\\", type=float, defa\n",
            "ult=1.0, help=\\\"Ridge regularization strength (default: 1.0).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: diabetes dataset is empty\\\")\\n        sys.exit(1)\\n\\n    model = Ridge(alpha=args.alpha, random_state=args.seed)\\n    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\\n    mean_r2 = scores.mean()\\n\\n    print(f\\\"5-fold CV R² scores: {scores}\\\")\\n    print(f\\\"Mean R²: {mean_r2:.4f}\\\")\\n\\n    if mean_r2 >= 0.35:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: mean R² {mean_r2:.4f} < 0.35\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons SVM Margin\",\n",
            "    \"description\": \"Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as 'svm_margin.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.svm import SVC\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as svm_margin.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of samples (default: 200).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.2, help=\\\"Noise level (default: 0.2).\\\")\\n    p.add_argument(\\\"--C\\\", type=float, default=1.0, help=\\\"SVM regularization parameter (default: 1.0).\\\")\\n    p.add_argument(\\\"--gamma\\\", type=float, default=2.0, help=\\\"RBF kernel gamma (default: 2.0).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"svm_margin.png\\\", help=\\\"Output image path (default: svm_margin.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n    \\n    clf = SVC(kernel='rbf', C=args.C, gamma=args.gamma)\\n    clf.fit(X_scaled, y)\\n    \\n    x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\\n    y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\\n    h = 0.02\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    \\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n    \\n    plt.figure(figsize=(10, 8))\\n    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 50), cmap='RdBu', alpha=0.6)\\n    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\\n    plt.contour(xx, yy, Z, levels=[-1, 1], linewidths=1, colors='black', linestyles='dashed')\\n    \\n    plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c='red', edgecolors='k', marker='o', s=50, label='Class 0')\\n    plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c='blue', edgecolors='k', marker='s', s=50, label='Class 1')\\n    \\n    support_vectors = clf.support_vectors_\\n    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=200, linewidth=1, facecolors='none', edgecolors='green', label='Support Vectors')\\n    \\n    plt.xlabel('Feature 1')\\n    plt.ylabel('Feature 2')\\n    plt.title('SVM Decision Boundary and Margins (RBF Kerne\n",
            "l)')\\n    plt.legend()\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    \\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(f\\\"Saved decision boundary plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles Neural Net\",\n",
            "    \"description\": \"Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.87.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy >= 0.87.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=1000, help=\\\"Number of samples to generate (default: 1000).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.1, help=\\\"Noise level for make_circles (default: 0.1).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_circles(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    clf = MLPClassifier(\\n        hidden_layer_sizes=(100, 50),\\n        max_iter=500,\\n        random_state=args.seed,\\n        early_stopping=True,\\n        validation_fraction=0.1\\n    )\\n    clf.fit(X_train_scaled, y_train)\\n\\n    accuracy = clf.score(X_test_scaled, y_test)\\n    print(f\\\"accuracy={accuracy:.3f}\\\")\\n\\n    if accuracy >= 0.87:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.87\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Blobs Elbow Method\",\n",
            "    \"description\": \"Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as 'elbow_plot.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as elbow_plot.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=2, help=\\\"Number of features (default: 2).\\\")\\n    p.add_argument(\\\"--centers\\\", type=int, default=4, help=\\\"Number of centers for make_blobs (default: 4).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"elbow_plot.png\\\", help=\\\"Output filename for elbow plot (default: elbow_plot.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n   \n",
            "     import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_blobs(n_samples=args.n_samples, n_features=args.n_features, centers=args.centers, random_state=args.seed)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    inertias = []\\n    k_range = range(1, 9)\\n    for k in k_range:\\n        km = KMeans(n_clusters=k, random_state=args.seed, n_init=10)\\n        km.fit(X)\\n        inertias.append(km.inertia_)\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.plot(list(k_range), inertias, marker='o')\\n    plt.xlabel('Number of clusters (k)')\\n    plt.ylabel('Inertia')\\n    plt.title('Elbow Method for Optimal k')\\n    plt.grid(True)\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Elbow plot saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Classification Learning Curve\",\n",
            "    \"description\": \"Generate make_classification data, train LogisticRegression, plot learning curve, save as 'learning_curve.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import learning_curve\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_classification data, train LogisticRegression, plot learning curve, save as learning_curve.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=500, help=\\\"Number of samples (default: 500).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=20, help=\\\"Number of features (default: 20).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"learning_curve.png\\\", help=\\\"Output file path (default: learning_curve.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_classification(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_informative=max(2, args.n_features // 2),\\n        n_redundant=max(0, args.n_features // 4),\\n        random_state=args.seed\\n    )\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\\n\\n    train_sizes = np.linspace(0.1, 1.0, 10)\\n    train_sizes_abs, train_scores, val_scores = learning_curve(\\n        clf, X, y,\\n        train_sizes=train_sizes,\\n        cv=5,\\n        scoring='accuracy',\\n        random_state=args.seed,\\n        n_jobs=1\\n    )\\n\\n    train_mean = np.mean(train_scores, axis=1)\\n    train_std = np.std(train_scores, axis=1)\\n    val_mean = np.mean(val_scores, axis=1)\\n    val_std = np.std(val_scores, axis=1)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.plot(train_sizes_abs, train_mean, 'o-', label='Training score', linewidth=2)\\n    plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2)\\n    plt.plot(train_sizes_abs, val_mean, 'o-', label='Validation score', linewidth=2)\\n    plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2)\\n    plt.xlabel('Training Set Size')\\n    plt.ylabel('Accuracy')\\n    plt.title('Learning Curve (LogisticRegression)')\\n    plt.legend(loc='best')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import o\n",
            "s\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(f\\\"Learning curve saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Regression Residual Analysis\",\n",
            "    \"description\": \"Generate make_regression data, train LinearRegression, plot residual histogram, save as 'resid_hist.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nimport os\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate regression data, train LinearRegression, plot residual histogram, save as resid_hist.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=5, help=\\\"Number of features (default: 5).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=10.0, help=\\\"Noise level (default: 10.0).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"resid_hist.png\\\", help=\\\"Output histogram filename (default: resid_hist.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_regression(n_samples=args.n_samples, n_features=args.n_features, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\\n    \\n    model = LinearRegression()\\n    model.fit(Xtr, ytr)\\n    \\n    y_pred = model.predict(Xte)\\n    residuals = yte - y_pred\\n    \\n    plt.figure(figsize=(8, 6))\\n    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\\n    plt.xlabel('Residuals')\\n    plt.ylabel('Frequency')\\n    plt.title('Residual Histogram')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    \\n    if not os.path.exists(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n    \\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n    \\n    print(f\\\"Residual histogram saved to {args.output} (size: {file_size} bytes)\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic NLP Spam Filter\",\n",
            "    \"description\": \"Generate 250 synthetic email texts labeled spam/ham, vectorize with TF-IDF, train Naive Bayes, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_emails(n=250, seed=42):\\n    rng = np.random.default_rng(seed)\\n    spam_words = [\\\"win\\\", \\\"free\\\", \\\"prize\\\", \\\"click\\\", \\\"offer\\\", \\\"buy\\\", \\\"discount\\\", \\\"limited\\\", \\\"urgent\\\", \\\"cash\\\", \\\"bonus\\\", \\\"guarantee\\\", \\\"credit\\\", \\\"loan\\\", \\\"money\\\"]\\n    ham_words = [\\\"meeting\\\", \\\"schedule\\\", \\\"report\\\", \\\"project\\\", \\\"team\\\", \\\"update\\\", \\\"rev\n",
            "iew\\\", \\\"discuss\\\", \\\"attached\\\", \\\"please\\\", \\\"thanks\\\", \\\"regards\\\", \\\"confirm\\\", \\\"deadline\\\", \\\"agenda\\\"]\\n    \\n    texts = []\\n    labels = []\\n    \\n    for i in range(n):\\n        is_spam = rng.random() < 0.5\\n        if is_spam:\\n            num_words = rng.integers(5, 15)\\n            words = rng.choice(spam_words, size=num_words, replace=True)\\n            text = \\\" \\\".join(words)\\n            labels.append(1)\\n        else:\\n            num_words = rng.integers(5, 15)\\n            words = rng.choice(ham_words, size=num_words, replace=True)\\n            text = \\\" \\\".join(words)\\n            labels.append(0)\\n        texts.append(text)\\n    \\n    return texts, labels\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 250 synthetic spam/ham emails, train Naive Bayes with TF-IDF, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=250, help=\\\"Number of synthetic emails to generate (default: 250).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    texts, labels = generate_synthetic_emails(n=args.n_samples, seed=args.seed)\\n    \\n    if len(texts) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: no data generated\\\")\\n        sys.exit(1)\\n    \\n    X_train, X_test, y_train, y_test = train_test_split(\\n        texts, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\\n    )\\n    \\n    vectorizer = TfidfVectorizer(max_features=100)\\n    X_train_vec = vectorizer.fit_transform(X_train)\\n    X_test_vec = vectorizer.transform(X_test)\\n    \\n    clf = MultinomialNB()\\n    clf.fit(X_train_vec, y_train)\\n    \\n    y_pred = clf.predict(X_test_vec)\\n    acc = accuracy_score(y_test, y_pred)\\n    \\n    print(f\\\"n_samples={args.n_samples} accuracy={acc:.3f}\\\")\\n    \\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Recommender System\",\n",
            "    \"description\": \"Generate 200 synthetic user-item ratings, apply matrix factorization (NMF), compute RMSE. Print TEST_PASS if RMSE ≤ 1.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.decomposition import NMF\\nfrom sklearn.metrics import mean_squared_error\\n\\ndef generate_synthetic_ratings(n_users=50, n_items=40, n_ratings=200, seed=42):\\n    \\\"\\\"\\\"Generate synthetic user-item ratings matrix.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    \\n    # Create sparse ratings: user_id, item_id, rating\\n    user_ids = rng.integers(0, n_users, size=n_ratings)\\n    item_ids = rng.integers(0, n_items, size=n_ratings)\\n    ratings = rng.uniform(1.0, 5.0, size=n_ratings)\\n    \\n    # Build dense matrix (users x items)\\n    R = np.zeros((n_users, n_items))\\n    for u, i, r in zip(user_ids, item_ids, ratings):\\n        R[u, i] = r\\n    \\n    # Mask: which entries are observed\\n    mask = (R > 0)\\n    \\n    return R, mask, user_ids, item_ids, ratings\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Synthetic recommender with NMF; RMSE ≤ 1.5 for TEST_PASS.\\\")\\n    p.add_argument(\\\"--n-users\\\", type=int, default=50, help=\\\"Number of users (default: 50).\\\")\\n    p.add_argument(\\\"--n-items\\\", type=int, default=40, help=\\\"Number of items (default: 40).\\\")\\n    p.add_argument(\\\"--n-ratings\\\", type=int, default=200, help=\\\"Number of ratings (default: 200).\\\")\\n    p.add_argument(\\\"--n-components\\\", type=int, default=10, help=\\\"NMF latent factors (default: 10).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args\n",
            "()\\n    \\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n    \\n    # Generate synthetic ratings\\n    R, mask, user_ids, item_ids, ratings = generate_synthetic_ratings(\\n        n_users=args.n_users,\\n        n_items=args.n_items,\\n        n_ratings=args.n_ratings,\\n        seed=args.seed\\n    )\\n    \\n    if R is None or mask is None or np.sum(mask) == 0:\\n        print(\\\"TEST_FAIL: no ratings generated\\\")\\n        sys.exit(1)\\n    \\n    print(f\\\"Generated {args.n_ratings} ratings for {args.n_users} users and {args.n_items} items\\\")\\n    \\n    # Apply NMF (Non-negative Matrix Factorization)\\n    # NMF requires non-negative values; our ratings are already ≥ 1.0\\n    model = NMF(n_components=args.n_components, init='random', random_state=args.seed, max_iter=500)\\n    \\n    try:\\n        W = model.fit_transform(R)  # user factors\\n        H = model.components_       # item factors\\n        R_pred = W @ H              # reconstructed matrix\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: NMF failed - {e}\\\")\\n        sys.exit(1)\\n    \\n    # Compute RMSE on observed ratings only\\n    observed_true = R[mask]\\n    observed_pred = R_pred[mask]\\n    \\n    if len(observed_true) == 0:\\n        print(\\\"TEST_FAIL: no observed ratings to evaluate\\\")\\n        sys.exit(1)\\n    \\n    rmse = np.sqrt(mean_squared_error(observed_true, observed_pred))\\n    print(f\\\"RMSE on observed ratings: {rmse:.4f}\\\")\\n    \\n    # Acceptance: RMSE ≤ 1.5\\n    if rmse <= 1.5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: RMSE {rmse:.4f} > 1.5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Speech Emotion\",\n",
            "    \"description\": \"Generate 200 synthetic speech-like feature vectors labeled emotion, train RandomForest, report accuracy. Print TEST_PASS if accuracy ≥ 0.65.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_speech_emotion(n_samples=200, n_features=13, n_classes=4, seed=42):\\n    \\\"\\\"\\\"Generate synthetic speech-like feature vectors with emotion labels.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    \\n    # Emotion classes: 0=neutral, 1=happy, 2=sad, 3=angry\\n    y = rng.integers(0, n_classes, size=n_samples)\\n    \\n    # Generate MFCC-like features (13 coefficients typical for speech)\\n    X = np.zeros((n_samples, n_features))\\n    \\n    for i in range(n_samples):\\n        emotion = y[i]\\n        # Each emotion has characteristic feature patterns\\n        if emotion == 0:  # neutral\\n            X[i] = rng.normal(0.0, 1.0, size=n_features)\\n        elif emotion == 1:  # happy (higher pitch, more energy)\\n            X[i] = rng.normal(1.5, 1.2, size=n_features)\\n        elif emotion == 2:  # sad (lower pitch, less energy)\\n            X[i] = rng.normal(-1.5, 0.8, size=n_features)\\n        elif emotion == 3:  # angry (high energy, variable pitch)\\n            X[i] = rng.normal(0.5, 2.0, size=n_features)\\n    \\n    return X, y\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 200 synthetic speech emotion vectors, train RandomForest, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of synthetic samples (default: 200).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=13, help=\\\"Number of features per sample (default: 13).\\\")\\n    p.add_argument(\\\"--n-classes\\\", type=int, default=4, help=\\\"Number of emotion classes (default: 4).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.25, help=\\\"Test set fraction (default: 0.25).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=100, help=\\\"Number of trees in RandomForest (def\n",
            "ault: 100).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n    \\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n    \\n    # Generate synthetic speech emotion dataset\\n    X, y = generate_synthetic_speech_emotion(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_classes=args.n_classes,\\n        seed=args.seed\\n    )\\n    \\n    # Validate dataset\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n    \\n    if len(X) != args.n_samples:\\n        print(f\\\"TEST_FAIL: expected {args.n_samples} samples, got {len(X)}\\\")\\n        sys.exit(1)\\n    \\n    # Split dataset\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n    \\n    # Train RandomForest\\n    clf = RandomForestClassifier(\\n        n_estimators=args.n_estimators,\\n        random_state=args.seed,\\n        n_jobs=-1\\n    )\\n    clf.fit(X_train, y_train)\\n    \\n    # Predict and evaluate\\n    y_pred = clf.predict(X_test)\\n    accuracy = accuracy_score(y_test, y_pred)\\n    \\n    print(f\\\"samples={args.n_samples} features={args.n_features} classes={args.n_classes}\\\")\\n    print(f\\\"train_size={len(X_train)} test_size={len(X_test)}\\\")\\n    print(f\\\"accuracy={accuracy:.3f}\\\")\\n    \\n    # Acceptance check\\n    if accuracy >= 0.65:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.65\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Financial Forecast\",\n",
            "    \"description\": \"Generate 300 synthetic stock-like sequences, predict direction (up/down), train LogisticRegression, report accuracy. Print TEST_PASS if accuracy ≥ 0.6.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef generate_synthetic_stock_sequences(n_sequences=300, seq_len=20, seed=42):\\n    \\\"\\\"\\\"Generate synthetic stock-like sequences with up/down labels.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    sequences = []\\n    labels = []\\n    \\n    for _ in range(n_sequences):\\n        # Generate a random walk with drift\\n        drift = rng.uniform(-0.02, 0.02)\\n        volatility = rng.uniform(0.01, 0.05)\\n        returns = rng.normal(drift, volatility, seq_len)\\n        prices = 100 * np.exp(np.cumsum(returns))\\n        \\n        # Label: 1 if final price > initial price, else 0\\n        label = 1 if prices[-1] > prices[0] else 0\\n        \\n        sequences.append(prices)\\n        labels.append(label)\\n    \\n    return np.array(sequences), np.array(labels)\\n\\ndef extract_features(sequences):\\n    \\\"\\\"\\\"Extract simple features from price sequences.\\\"\\\"\\\"\\n    features = []\\n    for seq in sequences:\\n        # Compute basic statistics\\n        mean_price = np.mean(seq)\\n        std_price = np.std(seq)\\n        min_price = np.min(seq)\\n        max_price = np.max(seq)\\n        price_range = max_price - min_price\\n        \\n        # Compute returns\\n        returns = np.diff(seq) / seq[:-1]\\n        mean_return = np.mean(returns)\\n        std_return = np.std(returns)\\n        \\n        # Trend: slope of linear fit\\n        x = np.arange(len(seq))\\n        trend = np.polyfit(x, seq, 1)[0]\\n        \\n        # Final vs initial price ratio\\n        price_ratio = seq[-1] / seq[0]\\n        \\n        features.append([\\n            mean_price, std_price, min_price, max_price, price_range,\\n            mean_return, std_return, trend, price_ratio\\n        ])\\n    \\n    return np.array(features)\\n\\ndef main():\n",
            "\\n    p = argparse.ArgumentParser(description=\\\"Generate 300 synthetic stock sequences, predict direction with LogisticRegression.\\\")\\n    p.add_argument(\\\"--n-sequences\\\", type=int, default=300, help=\\\"Number of synthetic sequences (default: 300).\\\")\\n    p.add_argument(\\\"--seq-len\\\", type=int, default=20, help=\\\"Length of each sequence (default: 20).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Validate inputs\\n    if args.n_sequences <= 0:\\n        print(\\\"TEST_FAIL: n_sequences must be positive\\\")\\n        sys.exit(1)\\n    if args.seq_len <= 1:\\n        print(\\\"TEST_FAIL: seq_len must be > 1\\\")\\n        sys.exit(1)\\n    if not (0 < args.test_size < 1):\\n        print(\\\"TEST_FAIL: test_size must be in (0, 1)\\\")\\n        sys.exit(1)\\n\\n    # Generate synthetic data\\n    sequences, labels = generate_synthetic_stock_sequences(\\n        n_sequences=args.n_sequences,\\n        seq_len=args.seq_len,\\n        seed=args.seed\\n    )\\n    \\n    if len(sequences) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: failed to generate synthetic data\\\")\\n        sys.exit(1)\\n\\n    # Extract features\\n    X = extract_features(sequences)\\n    y = labels\\n\\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # Scale features\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    # Train LogisticRegression\\n    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\\n    clf.fit(X_train_scaled, y_train)\\n\\n    # Evaluate\\n    accuracy = clf.score(X_test_scaled, y_test)\\n    print(f\\\"n_sequences={args.n_sequences} seq_len={args.seq_len} accuracy={accuracy:.3f}\\\")\\n\\n    # Acceptance check\\n    if accuracy >= 0.6:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} < 0.6\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST Autoencoder\",\n",
            "    \"description\": \"If MNIST available, train simple autoencoder, else use FakeData, reconstruct sample image, save original/recon as 'autoencode.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\n\\ndef load_mnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0:\\n            raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        return train, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n    import matplotlib\\n    matplotlib.use('Agg')\\n    import matplotlib.pyplot as plt\\n\\n    p = argparse.ArgumentParser(description=\\\"MNIST autoencoder (opt-in download) or FakeData fallback; saves autoencode.png.\\\")\\n    p.add_argument(\\\"--epo\n",
            "chs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"autoencode.png\\\", help=\\\"Output image path (default: autoencode.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, real = load_mnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n\\n    class Autoencoder(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.encoder = nn.Sequential(\\n                nn.Flatten(),\\n                nn.Linear(28*28, 128),\\n                nn.ReLU(),\\n                nn.Linear(128, 64),\\n                nn.ReLU(),\\n                nn.Linear(64, 32)\\n            )\\n            self.decoder = nn.Sequential(\\n                nn.Linear(32, 64),\\n                nn.ReLU(),\\n                nn.Linear(64, 128),\\n                nn.ReLU(),\\n                nn.Linear(128, 28*28),\\n                nn.Sigmoid()\\n            )\\n        def forward(self, x):\\n            z = self.encoder(x)\\n            recon = self.decoder(z)\\n            return recon.view(-1, 1, 28, 28)\\n\\n    model = Autoencoder()\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n    loss_fn = nn.MSELoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        total_loss = 0.0\\n        for xb, _ in train_loader:\\n            opt.zero_grad()\\n            recon = model(xb)\\n            loss = loss_fn(recon, xb)\\n            loss.backward()\\n            opt.step()\\n            total_loss += loss.item()\\n        avg_loss = total_loss / len(train_loader)\\n        print(f\\\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\\\")\\n\\n    model.eval()\\n    with torch.no_grad():\\n        sample_x, _ = next(iter(DataLoader(train_ds, batch_size=1, shuffle=False)))\\n        sample_recon = model(sample_x)\\n\\n    orig = sample_x[0, 0].cpu().numpy()\\n    recon = sample_recon[0, 0].cpu().numpy()\\n\\n    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\\n    axes[0].imshow(orig, cmap='gray')\\n    axes[0].set_title('Original')\\n    axes[0].axis('off')\\n    axes[1].imshow(recon, cmap='gray')\\n    axes[1].set_title('Reconstructed')\\n    axes[1].axis('off')\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    print(f\\\"dataset={'mnist' if real else 'fake'} output={args.output}\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST VAE Latent\",\n",
            "    \"description\": \"If FashionMNIST available, train basic VAE encoder, else use FakeData, visualize latent space, save as 'latent_vae.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_fashionmnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.FashionMNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0:\\n            raise RuntimeError(\\\"FashionMNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        return train, True\\n    except Exception:\\n        im\n",
            "port torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n\\n    p = argparse.ArgumentParser(description=\\\"FashionMNIST VAE latent space visualization with opt-in download or FakeData fallback.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--latent-dim\\\", type=int, default=2, help=\\\"Latent dimension (default: 2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit FashionMNIST download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"latent_vae.png\\\", help=\\\"Output latent space plot filename (default: latent_vae.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, real = load_fashionmnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n\\n    class VAE(nn.Module):\\n        def __init__(self, latent_dim=2):\\n            super().__init__()\\n            self.latent_dim = latent_dim\\n            self.encoder = nn.Sequential(\\n                nn.Flatten(),\\n                nn.Linear(28*28, 256), nn.ReLU(),\\n                nn.Linear(256, 128), nn.ReLU()\\n            )\\n            self.fc_mu = nn.Linear(128, latent_dim)\\n            self.fc_logvar = nn.Linear(128, latent_dim)\\n            self.decoder = nn.Sequential(\\n                nn.Linear(latent_dim, 128), nn.ReLU(),\\n                nn.Linear(128, 256), nn.ReLU(),\\n                nn.Linear(256, 28*28), nn.Sigmoid()\\n            )\\n\\n        def encode(self, x):\\n            h = self.encoder(x)\\n            return self.fc_mu(h), self.fc_logvar(h)\\n\\n        def reparameterize(self, mu, logvar):\\n            std = torch.exp(0.5 * logvar)\\n            eps = torch.randn_like(std)\\n            return mu + eps * std\\n\\n        def decode(self, z):\\n            return self.decoder(z).view(-1, 1, 28, 28)\\n\\n        def forward(self, x):\\n            mu, logvar = self.encode(x)\\n            z = self.reparameterize(mu, logvar)\\n            recon = self.decode(z)\\n            return recon, mu, logvar\\n\\n    def vae_loss(recon, x, mu, logvar):\\n        bce = nn.functional.binary_cross_entropy(recon, x, reduction='sum')\\n        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\\n        return bce + kld\\n\\n    model = VAE(latent_dim=args.latent_dim)\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        total_loss = 0.0\\n        for xb, _ in train_loader:\\n            opt.zero_grad()\\n            recon, mu, logvar = model(xb)\\n            loss = vae_loss(recon, xb, mu, logvar)\\n            loss.backward()\\n            opt.step()\\n            total_loss += loss.item()\\n        avg_loss = total_loss / len(train_loader.dataset)\\n        print(f\\\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\\\")\\n\\n    model.eval()\\n    latents = []\\n    labels = []\\n    with torch.no_grad():\\n        for xb, yb in train_loader:\\n            mu, _ = model.encode(xb)\\n            latents.append(mu.cpu().numpy())\\n            labels.append(yb.cpu().numpy())\\n    latents = np.concatenate(latents, axis=0)\\n    labels = np.concatenate(labels, axis=0)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap\n",
            "='tab10', alpha=0.6, s=10)\\n    plt.colorbar(scatter, label='Class')\\n    plt.xlabel('Latent Dim 1')\\n    plt.ylabel('Latent Dim 2')\\n    plt.title(f\\\"VAE Latent Space ({'FashionMNIST' if real else 'FakeData'})\\\")\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    print(f\\\"Saved latent space plot to {args.output}\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"CIFAR10 Transfer Features\",\n",
            "    \"description\": \"If CIFAR10 available, extract features via pretrained CNN, else use FakeData, train LogisticRegression on features, report accuracy. Print TEST_PASS if accuracy ≥ 0.6 or fallback ≥ 0.4.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_cifar10_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n        ])\\n        train = datasets.CIFAR10(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.CIFAR10(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\\n        return train, test, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n        ])\\n        train = FakeData(size=max_train, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        test = FakeData(size=max_test, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef extract_features(model, dataloader, device):\\n    import torch\\n    model.eval()\\n    features_list = []\\n    labels_list = []\\n    with torch.no_grad():\\n        for xb, yb in dataloader:\\n            xb = xb.to(device)\\n            feat = model(xb)\\n            features_list.append(feat.cpu().numpy())\\n            labels_list.append(yb.numpy())\\n    X = np.vstack(features_list)\\n    y = np.concatenate(labels_list)\\n    return X, y\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    from torch.utils.data import DataLoader\\n    from sklearn.linear_model import LogisticRegression\\n\\n    p = argparse.ArgumentParser(description=\\\"CIFAR10 transfer features (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--max-train\\\", type=int, default=2000, help=\\\"Max training samples (default: 2000).\\\")\\n    p.add_argument(\\\"--max-test\\\", type=int, default=500, help=\\\"Max test samples (default: 500).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    train_ds, test_ds, real = load_cifar10_or_fakedata(\\n        max_train=args.max_train,\\n        max_test=args.max_test,\\n        seed=args.seed,\\n       \n",
            " allow_download=args.allow_download\\n    )\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=False)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    device = torch.device(\\\"cpu\\\")\\n\\n    class FeatureExtractor(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.conv = nn.Sequential(\\n                nn.Conv2d(3, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\\n                nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\\n                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\\n            )\\n        def forward(self, x):\\n            x = self.conv(x)\\n            return x.view(x.size(0), -1)\\n\\n    model = FeatureExtractor().to(device)\\n\\n    print(\\\"Extracting training features...\\\")\\n    X_train, y_train = extract_features(model, train_loader, device)\\n    print(\\\"Extracting test features...\\\")\\n    X_test, y_test = extract_features(model, test_loader, device)\\n\\n    if X_train.shape[0] == 0 or X_test.shape[0] == 0:\\n        print(\\\"TEST_FAIL: no samples extracted\\\")\\n        sys.exit(1)\\n\\n    clf = LogisticRegression(max_iter=500, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    print(f\\\"acc={acc:.3f} dataset={'cifar10' if real else 'fake'}\\\")\\n\\n    threshold = 0.6 if real else 0.4\\n    if acc >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"ImageFolder Augmentation\",\n",
            "    \"description\": \"If ImageFolder available, apply rotation/flips to 20 images, else generate FakeData, augment, save augmented grid as 'augment_grid.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nfrom pathlib import Path\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply rotation/flips to ImageFolder or FakeData; save augmented grid as augment_grid.png.\\\")\\n    p.add_argument(\\\"--imagefolder-path\\\", type=str, default=None, help=\\\"Path to ImageFolder root (default: None, use FakeData).\\\")\\n    p.add_argument(\\\"--num-images\\\", type=int, default=20, help=\\\"Number of images to augment (default: 20).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"augment_grid.png\\\", help=\\\"Output grid filename (default: augment_grid.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit dataset download if needed.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        from torchvision.utils import make_grid\\n        from PIL import Image\\n    except ImportError:\\n        print(\\\"TEST_FAIL: torchvision or PIL not available\\\")\\n        sys.exit(1)\\n\\n    torch.manual_seed(args.seed)\\n\\n    use_imagefolder = False\\n    if args.imagefolder_path and os.path.isdir(args.imagefolder_path):\\n        try:\\n            base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\\n            ds = datasets.ImageFolder(root=args.imagefolder_path, transform=base_tfm)\\n            if len(ds) > 0:\\n                use_imagefolder = True\\n                print(f\\\"Using ImageFolder from {args.imagefolder_path} with {len(ds)} images\\\")\\n        except Exception as e:\\n            print(f\\\"ImageFolder failed: {e}, falling back to FakeData\\\")\\n\\n    if not use_imagefolder:\\n        print(\\\"Using FakeData fallback\\\")\\n        base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\\n        ds = datasets.FakeData(size=args.num_i\n",
            "mages, image_size=(3, 64, 64), num_classes=10, transform=base_tfm)\\n\\n    num_samples = min(args.num_images, len(ds))\\n    indices = list(range(num_samples))\\n    random.shuffle(indices)\\n    indices = indices[:num_samples]\\n\\n    augmentations = [\\n        transforms.RandomRotation(degrees=30),\\n        transforms.RandomHorizontalFlip(p=1.0),\\n        transforms.RandomVerticalFlip(p=1.0),\\n        transforms.Compose([transforms.RandomRotation(degrees=15), transforms.RandomHorizontalFlip(p=0.5)]),\\n    ]\\n\\n    augmented_images = []\\n    for idx in indices:\\n        img_tensor, _ = ds[idx]\\n        img_pil = transforms.ToPILImage()(img_tensor)\\n        aug = random.choice(augmentations)\\n        aug_pil = aug(img_pil)\\n        aug_tensor = transforms.ToTensor()(aug_pil)\\n        augmented_images.append(aug_tensor)\\n\\n    if len(augmented_images) == 0:\\n        print(\\\"TEST_FAIL: no images to augment\\\")\\n        sys.exit(1)\\n\\n    grid_tensor = make_grid(augmented_images, nrow=5, padding=2, normalize=False)\\n    grid_pil = transforms.ToPILImage()(grid_tensor)\\n    grid_pil.save(args.output)\\n    print(f\\\"Saved augmented grid to {args.output} with {len(augmented_images)} images\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FakeData Style Transfer\",\n",
            "    \"description\": \"Generate FakeData content/style pairs, apply basic style transfer algorithm, save result as 'style_transfer.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom pathlib import Path\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"FakeData style transfer: generate content/style pairs, apply basic transfer, save result.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"style_transfer.png\\\", help=\\\"Output image path (default: style_transfer.png).\\\")\\n    p.add_argument(\\\"--size\\\", type=int, default=256, help=\\\"Image size (default: 256).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        import torch\\n        from torchvision.datasets import FakeData\\n        from torchvision import transforms\\n        from PIL import Image\\n    except ImportError as e:\\n        print(f\\\"TEST_FAIL: missing dependency {e}\\\")\\n        sys.exit(1)\\n\\n    # generate FakeData content and style images\\n    torch.manual_seed(args.seed)\\n    tfm = transforms.Compose([\\n        transforms.Resize((args.size, args.size)),\\n        transforms.ToTensor()\\n    ])\\n    \\n    fake_ds = FakeData(size=2, image_size=(3, args.size, args.size), transform=tfm)\\n    content_tensor, _ = fake_ds[0]\\n    style_tensor, _ = fake_ds[1]\\n\\n    # basic style transfer: blend content and style with simple weighted average\\n    # convert to numpy for manipulation\\n    content_np = content_tensor.permute(1, 2, 0).numpy()\\n    style_np = style_tensor.permute(1, 2, 0).numpy()\\n    \\n    # simple transfer: 70% content + 30% style\\n    transferred = 0.7 * content_np + 0.3 * style_np\\n    transferred = np.clip(transferred, 0, 1)\\n    \\n    # convert back to PIL and save\\n    transferred_uint8 = (transferred * 255).astype(np.uint8)\\n    result_img = Image.fromarray(transferred_uint8, mode='RGB')\\n    result_img.save(args.output)\\n    \\n    # acceptance check\\n    if Path(args.output).exists():\\n        print(f\\\"Saved style transfer result to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\":\n",
            " \"KMeans CIFAR Segmentation\",\n",
            "    \"description\": \"If CIFAR10 available, segment image via KMeans clustering, else use FakeData, save segmented image as 'segmented.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nfrom PIL import Image\\n\\ndef load_cifar_or_fakedata(seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        ds = datasets.CIFAR10(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(ds) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        img_tensor, _ = ds[0]\\n        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\\n        return img_np, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        ds = FakeData(size=1, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        img_tensor, _ = ds[0]\\n        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\\n        return img_np, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"KMeans CIFAR10 segmentation (opt-in download) or FakeData fallback; save segmented.png; TEST_PASS if file exists.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=4, help=\\\"Number of clusters for KMeans (default: 4).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"segmented.png\\\", help=\\\"Output segmented image path (default: segmented.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    img, real = load_cifar_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    if img is None or img.size == 0:\\n        print(\\\"TEST_FAIL: image not available\\\")\\n        sys.exit(1)\\n\\n    h, w, c = img.shape\\n    pixels = img.reshape(-1, c).astype(np.float32)\\n\\n    from sklearn.cluster import KMeans\\n    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\\n    labels = kmeans.fit_predict(pixels)\\n    centers = kmeans.cluster_centers_.astype(np.uint8)\\n\\n    segmented = centers[labels].reshape(h, w, c)\\n\\n    pil_img = Image.fromarray(segmented, mode=\\\"RGB\\\")\\n    pil_img.save(args.output)\\n\\n    print(f\\\"dataset={'cifar10' if real else 'fake'} k={args.k} saved={args.output}\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Outlier Detection\",\n",
            "    \"description\": \"Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as 'outliers.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.datasets import load_iris\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as outliers.png.\\\")\\n    p.add_argument(\\\"--contamination\\\", type=float, default=0.1, help=\\\"Expected proportion of outliers (default: 0.1).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"outliers.png\\\", help=\\\"Output plot filename (default: outliers.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random se\n",
            "ed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_iris()\\n        X = data.data\\n        feature_names = data.feature_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    iso = IsolationForest(contamination=args.contamination, random_state=args.seed)\\n    iso.fit(X)\\n    predictions = iso.predict(X)\\n\\n    normal_mask = predictions == 1\\n    outlier_mask = predictions == -1\\n\\n    normal_points = X[normal_mask]\\n    outlier_points = X[outlier_mask]\\n\\n    num_outliers = outlier_points.shape[0]\\n    num_normal = normal_points.shape[0]\\n\\n    print(f\\\"Total samples: {len(X)}\\\")\\n    print(f\\\"Normal points: {num_normal}\\\")\\n    print(f\\\"Outliers detected: {num_outliers}\\\")\\n\\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\\n    ax.scatter(normal_points[:, 0], normal_points[:, 1], c='blue', label='Normal', alpha=0.6, edgecolors='k')\\n    ax.scatter(outlier_points[:, 0], outlier_points[:, 1], c='red', label='Outlier', alpha=0.8, edgecolors='k', s=100)\\n    ax.set_xlabel(feature_names[0])\\n    ax.set_ylabel(feature_names[1])\\n    ax.set_title(\\\"Iris Outlier Detection (IsolationForest)\\\")\\n    ax.legend()\\n    ax.grid(True, alpha=0.3)\\n\\n    try:\\n        plt.savefig(args.output, dpi=100, bbox_inches='tight')\\n        print(f\\\"Plot saved to {args.output}\\\")\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    if os.path.getsize(args.output) == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Anomaly Score\",\n",
            "    \"description\": \"Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as 'anomaly_scores.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as anomaly_scores.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"anomaly_scores.png\\\", help=\\\"Output histogram filename (default: anomaly_scores.png).\\\")\\n    p.add_argument(\\\"--nu\\\", type=float, default=0.1, help=\\\"OneClassSVM nu parameter (default: 0.1).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X = data.data\\n        if X is None or len(X) == 0:\\n            raise ValueError(\\\"wine dataset is empty\\\")\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: could not load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    oc_svm = OneClassSVM(nu=args.nu, kernel='rbf', gamma='auto')\\n    oc_svm.fit(X_scaled)\\n\\n    scores = oc_svm.decision_function(X_scaled)\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.hist(scores, bins=30, edgecolor='black', alpha=0.7)\\n    plt.xlabel('Anomaly Score')\\n    plt.ylabel('Frequency')\\n    plt.title('Wine Dataset Anom\n",
            "aly Scores (OneClassSVM)')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Histogram saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SHAP Values\",\n",
            "    \"description\": \"Train RandomForest on breast_cancer, compute SHAP values for one sample, save waterfall plot as 'shap_waterfall.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nimport shap\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train RandomForest on breast_cancer, compute SHAP values, save waterfall plot.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"shap_waterfall.png\\\", help=\\\"Output waterfall plot filename (default: shap_waterfall.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=50, help=\\\"Number of trees in RandomForest (default: 50).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n        feature_names = data.feature_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed, max_depth=5)\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"RandomForest accuracy on test set: {acc:.3f}\\\")\\n\\n    if len(X_test) == 0:\\n        print(\\\"TEST_FAIL: test set is empty\\\")\\n        sys.exit(1)\\n\\n    explainer = shap.TreeExplainer(clf)\\n    shap_values = explainer(X_test)\\n\\n    sample_idx = 0\\n    shap.plots.waterfall(shap_values[sample_idx, :, 1], show=False)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100, bbox_inches='tight')\\n    plt.close()\\n    print(f\\\"Saved SHAP waterfall plot to {args.output}\\\")\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file does not exist\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Grad-CAM Heatmap\",\n",
            "    \"description\": \"Train simple CNN on digits, compute Grad-CAM heatmap for one image, overlay on input, save as 'gradcam.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train CNN on digits, compute Grad-CAM heatmap, overlay on input, save as gradcam.png.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", hel\n",
            "p=\\\"Permit MNIST download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"gradcam.png\\\", help=\\\"Output heatmap filename (default: gradcam.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n    from torchvision import datasets, transforms\\n    import cv2\\n\\n    def load_mnist_or_fakedata(seed=42, allow_download=False):\\n        try:\\n            torch.manual_seed(seed)\\n            tfm = transforms.ToTensor()\\n            train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n            test = datasets.MNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n            if len(train) == 0 or len(test) == 0:\\n                raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n            train = torch.utils.data.Subset(train, list(range(min(len(train), 2000))))\\n            test = torch.utils.data.Subset(test, list(range(min(len(test), 500))))\\n            return train, test, True\\n        except Exception:\\n            torch.manual_seed(seed)\\n            tfm = transforms.ToTensor()\\n            from torchvision.datasets import FakeData\\n            train = FakeData(size=2000, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n            test = FakeData(size=500, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n            return train, test, False\\n\\n    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    class SimpleCNN(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\\n            self.relu1 = nn.ReLU()\\n            self.pool1 = nn.MaxPool2d(2)\\n            self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\\n            self.relu2 = nn.ReLU()\\n            self.pool2 = nn.MaxPool2d(2)\\n            self.flatten = nn.Flatten()\\n            self.fc1 = nn.Linear(32 * 7 * 7, 64)\\n            self.relu3 = nn.ReLU()\\n            self.fc2 = nn.Linear(64, 10)\\n            self.gradients = None\\n            self.activations = None\\n\\n        def forward(self, x):\\n            x = self.conv1(x)\\n            x = self.relu1(x)\\n            x = self.pool1(x)\\n            x = self.conv2(x)\\n            x = self.relu2(x)\\n            x = self.pool2(x)\\n            if x.requires_grad:\\n                x.register_hook(self.save_gradient)\\n            self.activations = x\\n            x = self.flatten(x)\\n            x = self.fc1(x)\\n            x = self.relu3(x)\\n            x = self.fc2(x)\\n            return x\\n\\n        def save_gradient(self, grad):\\n            self.gradients = grad\\n\\n    model = SimpleCNN()\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n    loss_fn = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            opt.zero_grad()\\n            logits = model(xb)\\n            loss = loss_fn(logits, yb)\\n            loss.backward()\\n            opt.step()\\n\\n    model.eval()\\n    correct = total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            pred = model(xb).argmax(1)\\n            correct += (pred == yb).sum().item()\\n            total += yb.numel()\\n    acc = correct / max(total, 1)\\n    print(f\\\"acc={acc:.3f} dataset={'mnist' if real else 'fake'}\\\")\\n\\n    test_iter = iter(test_loader)\\n    xb, yb = next(test_iter)\\n    img = xb[0:1]\\n    label = yb[0].item()\\n\\n    model.zero_grad()\\n    img.requires_grad = True\\n    output = model(img)\\n    pred_class =\n",
            " output.argmax(1).item()\\n    score = output[0, pred_class]\\n    score.backward()\\n\\n    gradients = model.gradients\\n    activations = model.activations\\n\\n    if gradients is None or activations is None:\\n        print(\\\"TEST_FAIL: gradients or activations not captured\\\")\\n        sys.exit(1)\\n\\n    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\\n    for i in range(activations.shape[1]):\\n        activations[:, i, :, :] *= pooled_gradients[i]\\n\\n    heatmap = torch.mean(activations, dim=1).squeeze()\\n    heatmap = torch.clamp(heatmap, min=0)\\n    heatmap /= (torch.max(heatmap) + 1e-8)\\n    heatmap_np = heatmap.detach().cpu().numpy()\\n\\n    heatmap_resized = cv2.resize(heatmap_np, (28, 28))\\n    heatmap_uint8 = np.uint8(255 * heatmap_resized)\\n    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\\n\\n    img_np = img.squeeze().detach().cpu().numpy()\\n    img_uint8 = np.uint8(255 * img_np)\\n    img_color = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2BGR)\\n\\n    overlay = cv2.addWeighted(img_color, 0.5, heatmap_color, 0.5, 0)\\n    cv2.imwrite(args.output, overlay)\\n\\n    if os.path.exists(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(result[\"items_with_code\"], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "quVj0XmEp_95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c9937c-775d-4952-fc5b-49e20bba9c42"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Iris KNN Classifier\",\n",
            "    \"description\": \"Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.9.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Iris KNN classifier with k=3; prints TEST_PASS if accuracy >= 0.9.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=3, help=\\\"Number of neighbors for KNN (default: 3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # load iris dataset (no download required)\\n    try:\\n        data = load_iris()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # split into train/test\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train k-NN classifier\\n    knn = KNeighborsClassifier(n_neighbors=args.k)\\n    knn.fit(X_train, y_train)\\n\\n    # evaluate on test set\\n    accuracy = knn.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {accuracy:.3f}\\\")\\n\\n    # acceptance check\\n    if accuracy >= 0.9:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} < 0.9\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Logistic Regression\",\n",
            "    \"description\": \"Use sklearn wine dataset, scale features, train logistic regression, and report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Wine dataset logistic regression with feature scaling; TEST_PASS if accuracy >= 0.92.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\\n    clf.fit(X_train_scaled, y_train)\\n\\n    accuracy = clf.score(X_test_scaled, y_test)\\n    print(f\\\"Test accuracy: {accuracy:.4f}\\\")\\n\\n    if accuracy >= 0.92:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.4f} < 0.92\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SVM\",\n",
            "    \"description\": \"Train a linear SVM on sklearn breast_cancer dataset, report test accuracy. Print TEST_PASS if accuracy ≥ 0.93.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train linear SVM on breast_cancer dataset; report test accuracy.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--max-iter\\\", type=int, default=1000, help=\\\"Max iterations for LinearSVC (default: 1000).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"svm\\\", LinearSVC(max_iter=args.max_iter, random_state=args.seed))\\n    ])\\n\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    print(f\\\"test_accuracy={acc:.4f}\\\")\\n\\n    if acc >= 0.93:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.93\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Linear Classifier\",\n",
            "    \"description\": \"Classify sklearn digits using LogisticRegression, report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Classify sklearn digits using LogisticRegression; TEST_PASS if accuracy >= 0.92.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--max-iter\\\", type=int, default=1000, help=\\\"Max iterations for LogisticRegression (default: 1000).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"lr\\\", LogisticRegression(max_iter=args.max_iter, random_state=args.seed))\\n    ])\\n\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    print(f\\\"Test accuracy: {acc:.4f}\\\")\\n\\n    if acc >= 0.92:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.92\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Ridge Regression\",\n",
            "    \"description\": \"Fit Ridge regression on sklearn diabetes dataset, compute R² score. Print TEST_PASS if R² ≥ 0.4.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.metrics import r2_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Ridge regression on sklearn diabetes dataset; TEST_PASS if R² ≥ 0.4.\\\")\\n    p.add_argument(\\\"--alpha\\\", type=float, default=1.0, help=\\\"Ridge regularization strength (default: 1.0).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: diabetes dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    model = Ridge(alpha=args.alpha, random_state=args.seed)\\n    model.fit(X_train, y_train)\\n\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"R² score: {r2:.4f}\\\")\\n\\n    if r2 >= 0.4:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R² score {r2:.4f} below threshold 0.4\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons Decision Tree\",\n",
            "    \"description\": \"Generate make_moons data (n=300), train decision tree, plot decision boundary, save as 'moons_tree.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_moons data, train decision tree, plot decision boundary, save as moons_tree.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.2, help=\\\"Noise level for make_moons (default: 0.2).\\\")\\n    p.add_argument(\\\"--max-depth\\\", type=int, default=5, help=\\\"Max depth of decision tree (default: 5).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"moons_tree.png\\\", help=\\\"Output filename for decision boundary plot (default: moons_tree.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Generate make_moons data\\n    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # Train decision tree\\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\\n    clf.fit(X, y)\\n\\n    # Create decision boundary plot\\n    h = 0.02\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    \\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap=plt.cm.RdYlBu)\\n    plt.xlabel(\\\"Feature 1\\\")\\n    plt.ylabel(\\\"Feature 2\\\")\\n    plt.title(\\\"Decision Tree Boundary on make_moons\\\")\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    # Acceptance check\\n    import os\\n    if not os.path.exists(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n    \\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    acc = clf.score(X, y)\\n    print(f\\\"Training accuracy: {acc:.3f}\\\")\\n    print(f\\\"Decision boundary saved to {args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles Random Forest\",\n",
            "    \"description\": \"Generate make_circles data (n=300), train random forest, report accuracy. Print TEST_PASS if accuracy ≥ 0.88.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_circles data (n=300), train random forest, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=100, help=\\\"Number of trees in random forest (default: 100).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_circles(n_samples=args.n_samples, noise=0.1, factor=0.5, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\\n    \\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\\n    clf.fit(Xtr, ytr)\\n    \\n    acc = clf.score(Xte, yte)\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n    \\n    if acc >= 0.88:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold 0.88\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Blobs KMeans Clustering\",\n",
            "    \"description\": \"Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score. Print TEST_PASS if silhouette ≥ 0.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import silhouette_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of samples (default: 200).\\\")\\n    p.add_argument(\\\"--centers\\\", type=int, default=4, help=\\\"Number of cluster centers (default: 4).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples <= 0:\\n        print(\\\"TEST_FAIL: n_samples must be positive\\\")\\n        sys.exit(1)\\n    if args.centers <= 0:\\n        print(\\\"TEST_FAIL: centers must be positive\\\")\\n        sys.exit(1)\\n\\n    X, y_true = make_blobs(n_samples=args.n_samples, centers=args.centers, random_state=args.seed)\\n    \\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    kmeans = KMeans(n_clusters=args.centers, random_state=args.seed, n_init=10)\\n    y_pred = kmeans.fit_predict(X)\\n\\n    sil = silhouette_score(X, y_pred)\\n    print(f\\\"n_samples={args.n_samples} centers={args.centers} silhouette={sil:.3f}\\\")\\n\\n    if sil >= 0.5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: silhouette score {sil:.3f} below threshold 0.5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Classification Synthetic Data\",\n",
            "    \"description\": \"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.85.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=500, help=\\\"Number of samples (default: 500).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=4, help=\\\"Number of features (default: 4).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_classification(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_informative=max(2, args.n_features // 2),\\n        n_redundant=0,\\n        n_clusters_per_class=1,\\n        random_state=args.seed\\n    )\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    y_pred = clf.predict(X_test)\\n    acc = accuracy_score(y_test, y_pred)\\n\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.85:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.85\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Regression Synthetic Data\",\n",
            "    \"description\": \"Generate make_regression data (n=300), fit LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate synthetic regression data (n=300), fit LinearRegression, compute R2; TEST_PASS if R2 >= 0.3.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=10, help=\\\"Number of features (default: 10).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=10.0, help=\\\"Standard deviation of Gaussian noise (default: 10.0).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Generate synthetic regression data\\n    X, y = make_regression(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        noise=args.noise,\\n        random_state=args.seed\\n    )\\n\\n    # Validate dataset\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    # Fit LinearRegression\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    # Predict and compute R²\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"n_samples={args.n_samples} n_features={args.n_features} noise={args.noise}\\\")\\n    print(f\\\"R2={r2:.4f}\\\")\\n\\n    # Acceptance check\\n    if r2 >= 0.3:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R2={r2:.4f} below threshold 0.3\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"PCA on Digits\",\n",
            "    \"description\": \"Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as 'digits_pca.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.decomposition import PCA\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as digits_pca.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"digits_pca.png\\\", help=\\\"Output PNG file path (default: digits_pca.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X = data.data\\n        y = data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    pca = PCA(n_components=2, random_state=args.seed)\\n    X_pca = pca.fit_transform(X)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\\n    plt.colorbar(scatter, label='Digit')\\n    plt.xlabel('First Principal Component')\\n    plt.ylabel('Second Principal Component')\\n    plt.title('PCA on Digits Dataset')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Saved PCA scatter plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Pairplot Visualization\",\n",
            "    \"description\": \"Plot pairwise feature scatter plots for iris dataset using matplotlib, save as 'iris_pairplot.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_iris_or_synthetic(seed=42):\\n    try:\\n        from sklearn.datasets import load_iris\\n        data = load_iris()\\n        X = data.data\\n        y = data.target\\n        feature_names = data.feature_names\\n        used = \\\"iris\\\"\\n    except Exception:\\n        rng = np.random.default_rng(seed)\\n        n = 150\\n        c = rng.integers(0, 3, size=n)\\n        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\\n        y = c\\n        feature_names = [\\\"sepal_length\\\", \\\"sepal_width\\\", \\\"petal_length\\\", \\\"petal_width\\\"]\\n        used = \\\"synthetic\\\"\\n    return X, y, feature_names, used\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Plot pairwise feature scatter plots for iris dataset; save as iris_pairplot.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"iris_pairplot.png\\\", help=\\\"Output filename (default: iris_pairplot.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y, feature_names, used = load_iris_or_synthetic(args.seed)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    n_features = X.shape[1]\\n    fig, axes = plt.subplots(n_features, n_features, figsize=(12, 12))\\n    \\n    colors = ['red', 'green', 'blue']\\n    for i in range(n_features):\\n        for j in range(n_features):\\n            ax = axes[i, j]\\n            if i == j:\\n                for cls in np.unique(y):\\n                    ax.hist(X[y == cls, i], bins=15, alpha=0.6, color=colors[cls % len(colors)], label=f\\\"Class {cls}\\\")\\n                ax.set_ylabel(\\\"Frequency\\\")\\n            else:\\n                for cls in np.unique(y):\\n                    ax.scatter(X[y == cls, j], X[y == cls, i], alpha=0.6, s=10, color=colors[cls % len(colors)], label=f\\\"Class {cls}\\\")\\n            \\n            if i == n_features - 1:\\n                ax.set_xlabel(feature_names[j])\\n            else:\\n                ax.set_xticklabels([])\\n            \\n            if j == 0:\\n                ax.set_ylabel(feature_names[i])\\n            else:\\n                ax.set_yticklabels([])\\n            \\n            if i == 0 and j == n_features - 1:\\n                ax.legend(loc='upper right', fontsize=8)\\n\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    print(f\\\"dataset={used} saved={args.output}\\\")\\n\\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file missing or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Feature Importance\",\n",
            "    \"description\": \"Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart as 'wine_importance.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"wine_importance.png\\\", help=\\\"Output filename for feature importance bar chart (default: wine_importance.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=100, help=\\\"Number of trees in RandomForest (default: 100).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X, y = data.data, data.target\\n        feature_names = data.feature_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: could not load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    importances = clf.feature_importances_\\n    if importances is None or len(importances) == 0:\\n        print(\\\"TEST_FAIL: feature importances are empty\\\")\\n        sys.exit(1)\\n\\n    indices = np.argsort(importances)[::-1]\\n\\n    plt.figure(figsize=(10, 6))\\n    plt.bar(range(len(importances)), importances[indices], align='center')\\n    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')\\n    plt.xlabel('Feature')\\n    plt.ylabel('Importance')\\n    plt.title('Wine Dataset Feature Importances')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    if os.path.getsize(args.output) == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"accuracy={acc:.3f} output={args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Confusion Matrix\",\n",
            "    \"description\": \"Train LogisticRegression on breast_cancer dataset, plot confusion matrix, save as 'bc_confusion.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train LogisticRegression on breast_cancer, plot confusion matrix, save as bc_confusion.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"bc_confusion.png\\\", help=\\\"Output confusion matrix image path (default: bc_confusion.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = LogisticRegression(max_iter=5000, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n    y_pred = clf.predict(X_test)\\n\\n    cm = confusion_matrix(y_test, y_pred)\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\\n    \\n    fig, ax = plt.subplots(figsize=(8, 6))\\n    disp.plot(ax=ax, cmap='Blues')\\n    plt.title('Breast Cancer Confusion Matrix')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    if os.path.getsize(args.output) == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"Confusion matrix saved to {args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Confusion Heatmap\",\n",
            "    \"description\": \"Train SVC on digits dataset, plot confusion matrix heatmap, save as 'digits_heatmap.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nfrom sklearn.metrics import confusion_matrix\\nimport seaborn as sns\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train SVC on digits dataset, plot confusion matrix heatmap, save as digits_heatmap.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"digits_heatmap.png\\\", help=\\\"Output heatmap filename (default: digits_heatmap.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.3, help=\\\"Test set fraction (default: 0.3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\\n\\n    clf = SVC(kernel='rbf', gamma='scale', random_state=args.seed)\\n    clf.fit(Xtr, ytr)\\n    ypred = clf.predict(Xte)\\n\\n    cm = confusion_matrix(yte, ypred)\\n\\n    plt.figure(figsize=(8, 6))\\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\\n    plt.xlabel('Predicted')\\n    plt.ylabel('Actual')\\n    plt.title('Digits Confusion Matrix')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"Confusion matrix heatmap saved to {args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Residual Plot\",\n",
            "    \"description\": \"Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as 'residuals_plot.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error\\nimport os\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as residuals_plot.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"residuals_plot.png\\\", help=\\\"Output filename for residual plot (default: residuals_plot.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: diabetes dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    y_pred = model.predict(X_test)\\n    residuals = y_test - y_pred\\n    mse = mean_squared_error(y_test, y_pred)\\n\\n    print(f\\\"Test MSE: {mse:.2f}\\\")\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.scatter(y_pred, residuals, alpha=0.6, edgecolors='k')\\n    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\\n    plt.xlabel('Predicted Values')\\n    plt.ylabel('Residuals')\\n    plt.title('Residual Plot: Diabetes Dataset')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n\\n    try:\\n        plt.savefig(args.output, dpi=100)\\n        print(f\\\"Residual plot saved to {args.output}\\\")\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Audio Classification\",\n",
            "    \"description\": \"Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_audio(n_samples=250, n_features=20, seed=42):\\n    rng = np.random.default_rng(seed)\\n    X = []\\n    y = []\\n    for i in range(n_samples):\\n        if i < n_samples // 2:\\n            # high pitch: higher frequency components\\n            freq_base = rng.uniform(800, 1200)\\n            label = 1\\n        else:\\n            # low pitch: lower frequency components\\n            freq_base = rng.uniform(100, 400)\\n            label = 0\\n        # simulate audio features (e.g., spectral coefficients)\\n        features = rng.normal(freq_base, 50, size=n_features)\\n        X.append(features)\\n        y.append(label)\\n    return np.array(X), np.array(y)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=250, help=\\\"Number of synthetic audio samples (default: 250).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=20, help=\\\"Number of features per sample (default: 20).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # generate synthetic audio dataset\\n    X, y = generate_synthetic_audio(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train MLP\\n    clf = MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=300, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    # predict and evaluate\\n    y_pred = clf.predict(X_test)\\n    acc = accuracy_score(y_test, y_pred)\\n\\n    print(f\\\"dataset=synthetic_audio n_samples={args.n_samples} accuracy={acc:.3f}\\\")\\n\\n    # acceptance: accuracy >= 0.7\\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Time Series Forecast\",\n",
            "    \"description\": \"Generate 200 synthetic time series samples, predict next value using lag features, compute R². Print TEST_PASS if R² ≥ 0.25.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\n\\ndef generate_synthetic_time_series(n_samples=200, seed=42):\\n    \\\"\\\"\\\"Generate synthetic time series with trend, seasonality, and noise.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    t = np.arange(n_samples)\\n    trend = 0.05 * t\\n    seasonality = 10 * np.sin(2 * np.pi * t / 20)\\n    noise = rng.normal(0, 2, size=n_samples)\\n    series = trend + seasonality + noise\\n    return series\\n\\ndef create_lag_features(series, n_lags=3):\\n    \\\"\\\"\\\"Create lag features for time series prediction.\\\"\\\"\\\"\\n    X = []\\n    y = []\\n    for i in range(n_lags, len(series)):\\n        X.append(series[i-n_lags:i])\\n        y.append(series[i])\\n    return np.array(X), np.array(y)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 200 synthetic time series samples, predict next value using lag features, compute R².\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of time series samples (default: 200).\\\")\\n    p.add_argument(\\\"--n-lags\\\", type=int, default=3, help=\\\"Number of lag features (default: 3).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples < 50:\\n        print(\\\"TEST_FAIL: n_samples must be at least 50\\\")\\n        sys.exit(1)\\n    if args.n_lags < 1:\\n        print(\\\"TEST_FAIL: n_lags must be at least 1\\\")\\n        sys.exit(1)\\n    if not (0 < args.test_size < 1):\\n        print(\\\"TEST_FAIL: test_size must be between 0 and 1\\\")\\n        sys.exit(1)\\n\\n    series = generate_synthetic_time_series(n_samples=args.n_samples, seed=args.seed)\\n    X, y = create_lag_features(series, n_lags=args.n_lags)\\n\\n    if len(X) == 0 or len(y) == 0:\\n        print(\\\"TEST_FAIL: insufficient data after creating lag features\\\")\\n        sys.exit(1)\\n\\n    split_idx = int(len(X) * (1 - args.test_size))\\n    if split_idx < 1 or split_idx >= len(X):\\n        print(\\\"TEST_FAIL: invalid train/test split\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test = X[:split_idx], X[split_idx:]\\n    y_train, y_test = y[:split_idx], y[split_idx:]\\n\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"n_samples={args.n_samples} n_lags={args.n_lags} r2={r2:.4f}\\\")\\n\\n    if r2 >= 0.25:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R² {r2:.4f} below threshold 0.25\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Tabular Regression\",\n",
            "    \"description\": \"Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n\\ndef generate_synthetic_tabular(n_samples=300, n_features=5, seed=42):\\n    rng = np.random.default_rng(seed)\\n    X = rng.normal(0, 1, size=(n_samples, n_features))\\n    true_coef = rng.uniform(-2, 2, size=n_features)\\n    y = X @ true_coef + rng.normal(0, 0.5, size=n_samples)\\n    return X, y\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R².\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of synthetic samples (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=5, help=\\\"Number of features (default: 5).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = generate_synthetic_tabular(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n    \\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\\n    \\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n    \\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n    \\n    print(f\\\"n_samples={args.n_samples} n_features={args.n_features} R²={r2:.3f}\\\")\\n    \\n    if r2 >= 0.3:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R²={r2:.3f} below threshold 0.3\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Text Sentiment\",\n",
            "    \"description\": \"Create 200 short synthetic sentences labeled positive or negative, vectorize with CountVectorizer, train a LogisticRegression, and print accuracy; print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n\\ndef generate_synthetic_sentences(n=200, seed=42):\\n    rng = np.random.default_rng(seed)\\n    positive_templates = [\\n        \\\"I love this product\\\",\\n        \\\"This is amazing\\\",\\n        \\\"Great experience\\\",\\n        \\\"Wonderful service\\\",\\n        \\\"Highly recommend\\\",\\n        \\\"Excellent quality\\\",\\n        \\\"Very satisfied\\\",\\n        \\\"Best purchase ever\\\",\\n        \\\"Fantastic results\\\",\\n        \\\"Really enjoyed it\\\"\\n    ]\\n    negative_templates = [\\n        \\\"I hate this product\\\",\\n        \\\"This is terrible\\\",\\n        \\\"Bad experience\\\",\\n        \\\"Poor service\\\",\\n        \\\"Do not recommend\\\",\\n        \\\"Awful quality\\\",\\n        \\\"Very disappointed\\\",\\n        \\\"Worst purchase ever\\\",\\n        \\\"Horrible results\\\",\\n        \\\"Really disliked it\\\"\\n    ]\\n    sentences = []\\n    labels = []\\n    for i in range(n):\\n        if i % 2 == 0:\\n            template = positive_templates[rng.integers(0, len(positive_templates))]\\n            label = 1\\n        else:\\n            template = negative_templates[rng.integers(0, len(negative_templates))]\\n            label = 0\\n        sentences.append(template)\\n        labels.append(label)\\n    return sentences, np.array(labels)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 200 synthetic sentiment sentences, vectorize with CountVectorizer, train LogisticRegression, print TEST_PASS if accuracy >= 0.7.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of synthetic sentences (default: 200).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    sentences, labels = generate_synthetic_sentences(n=args.n_samples, seed=args.seed)\\n    if len(sentences) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: no sentences generated\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        sentences, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\\n    )\\n\\n    vectorizer = CountVectorizer()\\n    X_train_vec = vectorizer.fit_transform(X_train)\\n    X_test_vec = vectorizer.transform(X_test)\\n\\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\\n    clf.fit(X_train_vec, y_train)\\n\\n    acc = clf.score(X_test_vec, y_test)\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST CNN Classifier\",\n",
            "    \"description\": \"If MNIST available, train simple CNN (2 conv layers), else generate FakeData (32x32 grayscale), train CNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.85 or fallback ≥ 0.6.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_mnist_or_fakedata(seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([transforms.ToTensor()])\\n        train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.MNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n        return train, test, True\\n    except Exception:\\n        import torch\\n        from torchvision.datasets import FakeData\\n        from torchvision import transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([transforms.ToTensor()])\\n        train = FakeData(size=1000, image_size=(1, 32, 32), num_classes=10, transform=tfm)\\n        test = FakeData(size=200, image_size=(1, 32, 32), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n\\n    p = argparse.ArgumentParser(description=\\\"MNIST CNN classifier with FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=2, help=\\\"Training epochs (default: 2).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    class SimpleCNN(nn.Module):\\n        def __init__(self, input_size=28):\\n            super().__init__()\\n            self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\\n            self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\\n            self.pool = nn.MaxPool2d(2, 2)\\n            self.relu = nn.ReLU()\\n            conv_out_size = (input_size // 4) * (input_size // 4) * 32\\n            self.fc1 = nn.Linear(conv_out_size, 64)\\n            self.fc2 = nn.Linear(64, 10)\\n\\n        def forward(self, x):\\n            x = self.pool(self.relu(self.conv1(x)))\\n            x = self.pool(self.relu(self.conv2(x)))\\n            x = x.view(x.size(0), -1)\\n            x = self.relu(self.fc1(x))\\n            x = self.fc2(x)\\n            return x\\n\\n    input_size = 28 if real else 32\\n    model = SimpleCNN(input_size=input_size)\\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\\n    criterion = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            optimizer.zero_grad()\\n            outputs = model(xb)\\n            loss = criterion(outputs, yb)\\n            loss.backward()\\n            optimizer.step()\\n\\n    model.eval()\\n    correct = 0\\n    total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            outputs = model(xb)\\n            _, predicted = torch.max(outputs.data, 1)\\n            total += yb.size(0)\\n            correct += (predicted == yb).sum().item()\\n\\n    accuracy = correct / max(total, 1)\\n    dataset_name = \\\"MNIST\\\" if real else \\\"FakeData\\\"\\n    print(f\\\"dataset={dataset_name} accuracy={accuracy:.3f}\\\")\\n\\n    threshold = 0.85 if real else 0.6\\n    if accuracy >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} below threshold {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST KNN\",\n",
            "    \"description\": \"If FashionMNIST available, flatten images, train KNN (k=5), else use FakeData, train KNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.7 or fallback ≥ 0.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_fashionmnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.FashionMNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.FashionMNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"FashionMNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\\n        return train, test, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        test = FakeData(size=max_test, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"FashionMNIST KNN (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=5, help=\\\"Number of neighbors for KNN (default: 5).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit FashionMNIST download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    train_ds, test_ds, real = load_fashionmnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n\\n    X_train = []\\n    y_train = []\\n    for img, label in train_ds:\\n        flat = img.numpy().flatten()\\n        X_train.append(flat)\\n        y_train.append(label)\\n    X_train = np.array(X_train)\\n    y_train = np.array(y_train)\\n\\n    X_test = []\\n    y_test = []\\n    for img, label in test_ds:\\n        flat = img.numpy().flatten()\\n        X_test.append(flat)\\n        y_test.append(label)\\n    X_test = np.array(X_test)\\n    y_test = np.array(y_test)\\n\\n    if len(X_train) == 0 or len(X_test) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    from sklearn.neighbors import KNeighborsClassifier\\n    knn = KNeighborsClassifier(n_neighbors=args.k)\\n    knn.fit(X_train, y_train)\\n    acc = knn.score(X_test, y_test)\\n\\n    print(f\\\"dataset={'fashionmnist' if real else 'fake'} k={args.k} accuracy={acc:.3f}\\\")\\n\\n    threshold = 0.7 if real else 0.5\\n    if acc >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"CIFAR10 Class Balance\",\n",
            "    \"description\": \"If CIFAR10 available, count class frequencies, else generate FakeData with 10 classes, count, save bar chart as 'class_balance.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_cifar10_or_fakedata(seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets\\n        torch.manual_seed(seed)\\n        ds = datasets.CIFAR10(root=\\\"./data\\\", train=True, download=bool(allow_download))\\n        if len(ds) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        labels = [y for _, y in ds]\\n        return labels, True\\n    except Exception:\\n        import torch\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        ds = FakeData(size=1000, image_size=(3, 32, 32), num_classes=10)\\n        labels = [y for _, y in ds]\\n        return labels, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Count CIFAR10 class frequencies or FakeData fallback; save bar chart; seeds in main.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"class_balance.png\\\", help=\\\"Output bar chart filename (default: class_balance.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    labels, real = load_cifar10_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    if labels is None or len(labels) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    counts = np.bincount(labels, minlength=10)\\n    print(f\\\"dataset={'cifar10' if real else 'fake'} class_counts={counts.tolist()}\\\")\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.bar(range(10), counts, color='steelblue')\\n    plt.xlabel(\\\"Class\\\")\\n    plt.ylabel(\\\"Frequency\\\")\\n    plt.title(\\\"Class Balance\\\")\\n    plt.xticks(range(10))\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"ImageFolder Histogram\",\n",
            "    \"description\": \"If ImageFolder available, compute average RGB histogram, else generate 50 FakeData images, compute histogram, save as 'histogram.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_imagefolder_or_fakedata(root_dir, num_fake=50, seed=42, allow_download=False):\\n    \\\"\\\"\\\"\\n    Try to load ImageFolder from root_dir if it exists and contains images.\\n    Otherwise fall back to FakeData.\\n    Returns (dataset, is_real).\\n    \\\"\\\"\\\"\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        \\n        # Check if root_dir exists and has subdirectories with images\\n        if os.path.isdir(root_dir):\\n            # Try to load ImageFolder\\n            tfm = transforms.ToTensor()\\n            ds = datasets.ImageFolder(root=root_dir, transform=tfm)\\n            if len(ds) > 0:\\n                return ds, True\\n        \\n        # Fallback to FakeData\\n        raise RuntimeError(\\\"ImageFolder not available or empty\\\")\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        ds = FakeData(size=num_fake, image_size=(3, 64, 64), num_classes=2, transform=tfm)\\n        return ds, False\\n\\ndef compute_rgb_histogram(dataset, num_samples=None):\\n    \\\"\\\"\\\"\\n    Compute average RGB histogram across dataset.\\n    Returns (r_hist, g_hist, b_hist) each of shape (256,).\\n    \\\"\\\"\\\"\\n    if num_samples is None:\\n        num_samples = len(dataset)\\n    else:\\n        num_samples = min(num_samples, len(dataset))\\n    \\n    r_sum = np.zeros(256, dtype=np.float64)\\n    g_sum = np.zeros(256, dtype=np.float64)\\n    b_sum = np.zeros(256, dtype=np.float64)\\n    \\n    for i in range(num_samples):\\n        img_tensor, _ = dataset[i]\\n        # img_tensor is (C, H, W) in [0, 1]\\n        img_np = (img_tensor.numpy() * 255).astype(np.uint8)\\n        \\n        r_hist, _ = np.histogram(img_np[0].flatten(), bins=256, range=(0, 256))\\n        g_hist, _ = np.histogram(img_np[1].flatten(), bins=256, range=(0, 256))\\n        b_hist, _ = np.histogram(img_np[2].flatten(), bins=256, range=(0, 256))\\n        \\n        r_sum += r_hist\\n        g_sum += g_hist\\n        b_sum += b_hist\\n    \\n    # Average\\n    r_avg = r_sum / num_samples\\n    g_avg = g_sum / num_samples\\n    b_avg = b_sum / num_samples\\n    \\n    return r_avg, g_avg, b_avg\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Compute average RGB histogram from ImageFolder or FakeData and save as histogram.png.\\\")\\n    p.add_argument(\\\"--root-dir\\\", type=str, default=\\\"./imagefolder_data\\\", help=\\\"Path to ImageFolder root directory (default: ./imagefolder_data).\\\")\\n    p.add_argument(\\\"--num-fake\\\", type=int, default=50, help=\\\"Number of FakeData images if ImageFolder unavailable (default: 50).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"histogram.png\\\", help=\\\"Output histogram file path (default: histogram.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit dataset download if needed.\\\")\\n    args = p.parse_args()\\n    \\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n    \\n    # Load dataset\\n    dataset, is_real = load_imagefolder_or_fakedata(\\n        root_dir=args.root_dir,\\n        num_fake=args.num_fake,\\n        seed=args.seed,\\n        allow_download=args.allow_download\\n    )\\n    \\n    if dataset is None or len(dataset) == 0:\\n        print(\\\"TEST_FAIL: dataset not available or empty\\\")\\n        sys.exit(1)\\n    \\n    print(f\\\"Using {'ImageFolder' if is_real else 'FakeData'} with {len(dataset)} images\\\")\\n    \\n    # Compute histogram\\n    r_hist, g_hist, b_hist = compute_rgb_histogram(dataset)\\n    \\n    # Plot and save\\n    fig, ax = plt.subplots(figsize=(10, 6))\\n    bins = np.arange(256)\\n    ax.plot(bins, r_hist, color='red', alpha=0.7, label='Red')\\n    ax.plot(bins, g_hist, color='green', alpha=0.7, label='Green')\\n    ax.plot(bins, b_hist, color='blue', alpha=0.7, label='Blue')\\n    ax.set_xlabel('Pixel Value')\\n    ax.set_ylabel('Average Frequency')\\n    ax.set_title('Average RGB Histogram')\\n    ax.legend()\\n    ax.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    \\n    # Acceptance check\\n    if os.path.isfile(args.output):\\n        file_size = os.path.getsize(args.output)\\n        if file_size > 0:\\n            print(f\\\"Histogram saved to {args.output} ({file_size} bytes)\\\")\\n            print(\\\"TEST_PASS\\\")\\n        else:\\n            print(\\\"TEST_FAIL: output file is empty\\\")\\n            sys.exit(1)\\n    else:\\n        print(\\\"TEST_FAIL: output file does not exist\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FakeData GAN Discriminator\",\n",
            "    \"description\": \"Generate FakeData images, train simple CNN discriminator, report accuracy distinguishing real/fake. Print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train CNN discriminator on FakeData (real vs fake labels); print TEST_PASS if accuracy >= 0.7.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=64, help=\\\"Batch size (default: 64).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-real\\\", type=int, default=500, help=\\\"Number of real samples (default: 500).\\\")\\n    p.add_argument(\\\"--n-fake\\\", type=int, default=500, help=\\\"Number of fake samples (default: 500).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader, TensorDataset\\n    from torchvision.datasets import FakeData\\n    from torchvision import transforms\\n\\n    tfm = transforms.ToTensor()\\n    real_ds = FakeData(size=args.n_real, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n    fake_ds = FakeData(size=args.n_fake, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n\\n    real_images = []\\n    fake_images = []\\n    for i in range(args.n_real):\\n        img, _ = real_ds[i]\\n        real_images.append(img)\\n    for i in range(args.n_fake):\\n        img, _ = fake_ds[i]\\n        fake_images.append(img)\\n\\n    real_images = torch.stack(real_images)\\n    fake_images = torch.stack(fake_images)\\n    real_labels = torch.ones(args.n_real, dtype=torch.long)\\n    fake_labels = torch.zeros(args.n_fake, dtype=torch.long)\\n\\n    X = torch.cat([real_images, fake_images], dim=0)\\n    y = torch.cat([real_labels, fake_labels], dim=0)\\n\\n    perm = torch.randperm(len(X))\\n    X = X[perm]\\n    y = y[perm]\\n\\n    split = int(0.8 * len(X))\\n    X_train, X_test = X[:split], X[split:]\\n    y_train, y_test = y[:split], y[split:]\\n\\n    train_ds = TensorDataset(X_train, y_train)\\n    test_ds = TensorDataset(X_test, y_test)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    class Discriminator(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.net = nn.Sequential(\\n                nn.Conv2d(1, 16, 3, 1, 1), nn.ReLU(),\\n                nn.MaxPool2d(2),\\n                nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(),\\n                nn.MaxPool2d(2),\\n                nn.Flatten(),\\n                nn.Linear(32 * 7 * 7, 64), nn.ReLU(),\\n                nn.Linear(64, 2)\\n            )\\n        def forward(self, x):\\n            return self.net(x)\\n\\n    model = Discriminator()\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n    loss_fn = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            opt.zero_grad()\\n            logits = model(xb)\\n            loss = loss_fn(logits, yb)\\n            loss.backward()\\n            opt.step()\\n\\n    model.eval()\\n    correct = 0\\n    total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            pred = model(xb).argmax(1)\\n            correct += (pred == yb).sum().item()\\n            total += yb.numel()\\n\\n    acc = correct / max(total, 1)\\n    print(f\\\"discriminator_accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} < 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"KMeans on CIFAR Colors\",\n",
            "    \"description\": \"If CIFAR10 available, reshape to pixels, run KMeans (k=5) on RGB values, else use FakeData, compute cluster inertia. Print TEST_PASS if inertia < 1.0e5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_cifar_or_fakedata(max_samples=5000, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.CIFAR10(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        subset = torch.utils.data.Subset(train, list(range(min(len(train), max_samples))))\\n        loader = torch.utils.data.DataLoader(subset, batch_size=max_samples, shuffle=False)\\n        for xb, _ in loader:\\n            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\\n            return pixels, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        fake = FakeData(size=max_samples, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        loader = torch.utils.data.DataLoader(fake, batch_size=max_samples, shuffle=False)\\n        for xb, _ in loader:\\n            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\\n            return pixels, False\\n    return None, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"KMeans on CIFAR10 RGB pixels (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=5, help=\\\"Number of clusters (default: 5).\\\")\\n    p.add_argument(\\\"--max-samples\\\", type=int, default=5000, help=\\\"Max images to load (default: 5000).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    pixels, real = load_cifar_or_fakedata(max_samples=args.max_samples, seed=args.seed, allow_download=args.allow_download)\\n    if pixels is None or len(pixels) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    from sklearn.cluster import KMeans\\n    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\\n    kmeans.fit(pixels)\\n    inertia = kmeans.inertia_\\n\\n    print(f\\\"dataset={'cifar10' if real else 'fake'} k={args.k} inertia={inertia:.2e}\\\")\\n\\n    if inertia < 1.0e5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: inertia >= 1.0e5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Decision Boundary\",\n",
            "    \"description\": \"Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as 'iris_boundary.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as iris_boundary.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"iris_boundary.png\\\", help=\\\"Output image path (default: iris_boundary.png).\\\")\\n    p.add_argument(\\\"--max-depth\\\", type=int, default=3, help=\\\"Max depth of decision tree (default: 3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_iris()\\n        X = data.data[:, :2]\\n        y = data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: could not load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\\n    clf.fit(X, y)\\n\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    h = 0.02\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolor='k', cmap='viridis')\\n    plt.xlabel(data.feature_names[0])\\n    plt.ylabel(data.feature_names[1])\\n    plt.title(\\\"Iris Decision Boundary (first two features)\\\")\\n    plt.colorbar(scatter)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Decision boundary saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine PCA Scatter\",\n",
            "    \"description\": \"Apply PCA to wine dataset (2 components), scatter plot colored by target, save as 'wine_pca.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply PCA to wine dataset (2 components), scatter plot colored by target, save as wine_pca.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"wine_pca.png\\\", help=\\\"Output filename (default: wine_pca.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X = data.data\\n        y = data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    pca = PCA(n_components=2, random_state=args.seed)\\n    X_pca = pca.fit_transform(X_scaled)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, edgecolors='k')\\n    plt.colorbar(scatter, label='Wine Class')\\n    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\\n    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\\n    plt.title('Wine Dataset PCA (2 Components)')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(f\\\"Saved PCA scatter plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist or is empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer ROC Curve\",\n",
            "    \"description\": \"Train LogisticRegression on breast_cancer, compute ROC curve, save as 'roc_curve.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import roc_curve, auc\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train LogisticRegression on breast_cancer, compute ROC curve, save as roc_curve.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"roc_curve.png\\\", help=\\\"Output ROC curve image path (default: roc_curve.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.3, help=\\\"Test set fraction (default: 0.3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = LogisticRegression(max_iter=5000, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    y_scores = clf.predict_proba(X_test)[:, 1]\\n\\n    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\\n    roc_auc = auc(fpr, tpr)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\\n    plt.xlim([0.0, 1.0])\\n    plt.ylim([0.0, 1.05])\\n    plt.xlabel('False Positive Rate')\\n    plt.ylabel('True Positive Rate')\\n    plt.title('Receiver Operating Characteristic - Breast Cancer')\\n    plt.legend(loc='lower right')\\n    plt.grid(alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"ROC curve saved to {args.output}, AUC={roc_auc:.3f}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits t-SNE Plot\",\n",
            "    \"description\": \"Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as 'tsne_digits.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.manifold import TSNE\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as tsne_digits.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"tsne_digits.png\\\", help=\\\"Output PNG file path (default: tsne_digits.png).\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=500, help=\\\"Number of samples to use (default: 500).\\\")\\n    p.add_argument(\\\"--perplexity\\\", type=float, default=30.0, help=\\\"t-SNE perplexity (default: 30.0).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    n = min(args.n_samples, len(X))\\n    indices = np.random.choice(len(X), size=n, replace=False)\\n    X_sub = X[indices]\\n    y_sub = y[indices]\\n\\n    try:\\n        tsne = TSNE(n_components=2, perplexity=args.perplexity, random_state=args.seed)\\n        X_embedded = tsne.fit_transform(X_sub)\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: t-SNE failed: {e}\\\")\\n        sys.exit(1)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_sub, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\\n    plt.colorbar(scatter, label='Digit')\\n    plt.title('t-SNE of Digits Dataset')\\n    plt.xlabel('t-SNE Component 1')\\n    plt.ylabel('t-SNE Component 2')\\n    plt.tight_layout()\\n\\n    try:\\n        plt.savefig(args.output, dpi=100)\\n        plt.close()\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Saved t-SNE plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file does not exist\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Cross Validation\",\n",
            "    \"description\": \"Run 5-fold CV on diabetes dataset with Ridge regression, compute mean R². Print TEST_PASS if mean R² ≥ 0.35.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.model_selection import cross_val_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Run 5-fold CV on diabetes dataset with Ridge regression; print TEST_PASS if mean R² ≥ 0.35.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--alpha\\\", type=float, default=1.0, help=\\\"Ridge regularization strength (default: 1.0).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: diabetes dataset is empty\\\")\\n        sys.exit(1)\\n\\n    model = Ridge(alpha=args.alpha, random_state=args.seed)\\n    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\\n    mean_r2 = scores.mean()\\n\\n    print(f\\\"5-fold CV R² scores: {scores}\\\")\\n    print(f\\\"Mean R²: {mean_r2:.4f}\\\")\\n\\n    if mean_r2 >= 0.35:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: mean R² {mean_r2:.4f} < 0.35\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons SVM Margin\",\n",
            "    \"description\": \"Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as 'svm_margin.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.svm import SVC\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as svm_margin.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of samples (default: 200).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.2, help=\\\"Noise level (default: 0.2).\\\")\\n    p.add_argument(\\\"--C\\\", type=float, default=1.0, help=\\\"SVM regularization parameter (default: 1.0).\\\")\\n    p.add_argument(\\\"--gamma\\\", type=float, default=2.0, help=\\\"RBF kernel gamma (default: 2.0).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"svm_margin.png\\\", help=\\\"Output image path (default: svm_margin.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n    \\n    clf = SVC(kernel='rbf', C=args.C, gamma=args.gamma)\\n    clf.fit(X_scaled, y)\\n    \\n    x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\\n    y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\\n    h = 0.02\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    \\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n    \\n    plt.figure(figsize=(10, 8))\\n    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 50), cmap='RdBu', alpha=0.6)\\n    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\\n    plt.contour(xx, yy, Z, levels=[-1, 1], linewidths=1, colors='black', linestyles='dashed')\\n    \\n    plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c='red', edgecolors='k', marker='o', s=50, label='Class 0')\\n    plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c='blue', edgecolors='k', marker='s', s=50, label='Class 1')\\n    \\n    support_vectors = clf.support_vectors_\\n    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=200, linewidth=1, facecolors='none', edgecolors='green', label='Support Vectors')\\n    \\n    plt.xlabel('Feature 1')\\n    plt.ylabel('Feature 2')\\n    plt.title('SVM Decision Boundary and Margins (RBF Kernel)')\\n    plt.legend()\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    \\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(f\\\"Saved decision boundary plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles Neural Net\",\n",
            "    \"description\": \"Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.87.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy >= 0.87.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=1000, help=\\\"Number of samples to generate (default: 1000).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.1, help=\\\"Noise level for make_circles (default: 0.1).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_circles(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    clf = MLPClassifier(\\n        hidden_layer_sizes=(100, 50),\\n        max_iter=500,\\n        random_state=args.seed,\\n        early_stopping=True,\\n        validation_fraction=0.1\\n    )\\n    clf.fit(X_train_scaled, y_train)\\n\\n    accuracy = clf.score(X_test_scaled, y_test)\\n    print(f\\\"accuracy={accuracy:.3f}\\\")\\n\\n    if accuracy >= 0.87:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.87\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Blobs Elbow Method\",\n",
            "    \"description\": \"Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as 'elbow_plot.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as elbow_plot.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=2, help=\\\"Number of features (default: 2).\\\")\\n    p.add_argument(\\\"--centers\\\", type=int, default=4, help=\\\"Number of centers for make_blobs (default: 4).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"elbow_plot.png\\\", help=\\\"Output filename for elbow plot (default: elbow_plot.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_blobs(n_samples=args.n_samples, n_features=args.n_features, centers=args.centers, random_state=args.seed)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    inertias = []\\n    k_range = range(1, 9)\\n    for k in k_range:\\n        km = KMeans(n_clusters=k, random_state=args.seed, n_init=10)\\n        km.fit(X)\\n        inertias.append(km.inertia_)\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.plot(list(k_range), inertias, marker='o')\\n    plt.xlabel('Number of clusters (k)')\\n    plt.ylabel('Inertia')\\n    plt.title('Elbow Method for Optimal k')\\n    plt.grid(True)\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Elbow plot saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Classification Learning Curve\",\n",
            "    \"description\": \"Generate make_classification data, train LogisticRegression, plot learning curve, save as 'learning_curve.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import learning_curve\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_classification data, train LogisticRegression, plot learning curve, save as learning_curve.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=500, help=\\\"Number of samples (default: 500).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=20, help=\\\"Number of features (default: 20).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"learning_curve.png\\\", help=\\\"Output file path (default: learning_curve.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_classification(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_informative=max(2, args.n_features // 2),\\n        n_redundant=max(0, args.n_features // 4),\\n        random_state=args.seed\\n    )\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\\n\\n    train_sizes = np.linspace(0.1, 1.0, 10)\\n    train_sizes_abs, train_scores, val_scores = learning_curve(\\n        clf, X, y,\\n        train_sizes=train_sizes,\\n        cv=5,\\n        scoring='accuracy',\\n        random_state=args.seed,\\n        n_jobs=1\\n    )\\n\\n    train_mean = np.mean(train_scores, axis=1)\\n    train_std = np.std(train_scores, axis=1)\\n    val_mean = np.mean(val_scores, axis=1)\\n    val_std = np.std(val_scores, axis=1)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.plot(train_sizes_abs, train_mean, 'o-', label='Training score', linewidth=2)\\n    plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2)\\n    plt.plot(train_sizes_abs, val_mean, 'o-', label='Validation score', linewidth=2)\\n    plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2)\\n    plt.xlabel('Training Set Size')\\n    plt.ylabel('Accuracy')\\n    plt.title('Learning Curve (LogisticRegression)')\\n    plt.legend(loc='best')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(f\\\"Learning curve saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Regression Residual Analysis\",\n",
            "    \"description\": \"Generate make_regression data, train LinearRegression, plot residual histogram, save as 'resid_hist.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nimport os\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate regression data, train LinearRegression, plot residual histogram, save as resid_hist.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=5, help=\\\"Number of features (default: 5).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=10.0, help=\\\"Noise level (default: 10.0).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"resid_hist.png\\\", help=\\\"Output histogram filename (default: resid_hist.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_regression(n_samples=args.n_samples, n_features=args.n_features, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\\n    \\n    model = LinearRegression()\\n    model.fit(Xtr, ytr)\\n    \\n    y_pred = model.predict(Xte)\\n    residuals = yte - y_pred\\n    \\n    plt.figure(figsize=(8, 6))\\n    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\\n    plt.xlabel('Residuals')\\n    plt.ylabel('Frequency')\\n    plt.title('Residual Histogram')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    \\n    if not os.path.exists(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n    \\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n    \\n    print(f\\\"Residual histogram saved to {args.output} (size: {file_size} bytes)\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic NLP Spam Filter\",\n",
            "    \"description\": \"Generate 250 synthetic email texts labeled spam/ham, vectorize with TF-IDF, train Naive Bayes, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_emails(n=250, seed=42):\\n    rng = np.random.default_rng(seed)\\n    spam_words = [\\\"win\\\", \\\"free\\\", \\\"prize\\\", \\\"click\\\", \\\"offer\\\", \\\"buy\\\", \\\"discount\\\", \\\"limited\\\", \\\"urgent\\\", \\\"cash\\\", \\\"bonus\\\", \\\"guarantee\\\", \\\"credit\\\", \\\"loan\\\", \\\"money\\\"]\\n    ham_words = [\\\"meeting\\\", \\\"schedule\\\", \\\"report\\\", \\\"project\\\", \\\"team\\\", \\\"update\\\", \\\"review\\\", \\\"discuss\\\", \\\"attached\\\", \\\"please\\\", \\\"thanks\\\", \\\"regards\\\", \\\"confirm\\\", \\\"deadline\\\", \\\"agenda\\\"]\\n    \\n    texts = []\\n    labels = []\\n    \\n    for i in range(n):\\n        is_spam = rng.random() < 0.5\\n        if is_spam:\\n            num_words = rng.integers(5, 15)\\n            words = rng.choice(spam_words, size=num_words, replace=True)\\n            text = \\\" \\\".join(words)\\n            labels.append(1)\\n        else:\\n            num_words = rng.integers(5, 15)\\n            words = rng.choice(ham_words, size=num_words, replace=True)\\n            text = \\\" \\\".join(words)\\n            labels.append(0)\\n        texts.append(text)\\n    \\n    return texts, labels\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 250 synthetic spam/ham emails, train Naive Bayes with TF-IDF, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=250, help=\\\"Number of synthetic emails to generate (default: 250).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    texts, labels = generate_synthetic_emails(n=args.n_samples, seed=args.seed)\\n    \\n    if len(texts) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: no data generated\\\")\\n        sys.exit(1)\\n    \\n    X_train, X_test, y_train, y_test = train_test_split(\\n        texts, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\\n    )\\n    \\n    vectorizer = TfidfVectorizer(max_features=100)\\n    X_train_vec = vectorizer.fit_transform(X_train)\\n    X_test_vec = vectorizer.transform(X_test)\\n    \\n    clf = MultinomialNB()\\n    clf.fit(X_train_vec, y_train)\\n    \\n    y_pred = clf.predict(X_test_vec)\\n    acc = accuracy_score(y_test, y_pred)\\n    \\n    print(f\\\"n_samples={args.n_samples} accuracy={acc:.3f}\\\")\\n    \\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Recommender System\",\n",
            "    \"description\": \"Generate 200 synthetic user-item ratings, apply matrix factorization (NMF), compute RMSE. Print TEST_PASS if RMSE ≤ 1.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.decomposition import NMF\\nfrom sklearn.metrics import mean_squared_error\\n\\ndef generate_synthetic_ratings(n_users=50, n_items=40, n_ratings=200, seed=42):\\n    \\\"\\\"\\\"Generate synthetic user-item ratings matrix.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    \\n    # Create sparse ratings: user_id, item_id, rating\\n    user_ids = rng.integers(0, n_users, size=n_ratings)\\n    item_ids = rng.integers(0, n_items, size=n_ratings)\\n    ratings = rng.uniform(1.0, 5.0, size=n_ratings)\\n    \\n    # Build dense matrix (users x items)\\n    R = np.zeros((n_users, n_items))\\n    for u, i, r in zip(user_ids, item_ids, ratings):\\n        R[u, i] = r\\n    \\n    # Mask: which entries are observed\\n    mask = (R > 0)\\n    \\n    return R, mask, user_ids, item_ids, ratings\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Synthetic recommender with NMF; RMSE ≤ 1.5 for TEST_PASS.\\\")\\n    p.add_argument(\\\"--n-users\\\", type=int, default=50, help=\\\"Number of users (default: 50).\\\")\\n    p.add_argument(\\\"--n-items\\\", type=int, default=40, help=\\\"Number of items (default: 40).\\\")\\n    p.add_argument(\\\"--n-ratings\\\", type=int, default=200, help=\\\"Number of ratings (default: 200).\\\")\\n    p.add_argument(\\\"--n-components\\\", type=int, default=10, help=\\\"NMF latent factors (default: 10).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n    \\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n    \\n    # Generate synthetic ratings\\n    R, mask, user_ids, item_ids, ratings = generate_synthetic_ratings(\\n        n_users=args.n_users,\\n        n_items=args.n_items,\\n        n_ratings=args.n_ratings,\\n        seed=args.seed\\n    )\\n    \\n    if R is None or mask is None or np.sum(mask) == 0:\\n        print(\\\"TEST_FAIL: no ratings generated\\\")\\n        sys.exit(1)\\n    \\n    print(f\\\"Generated {args.n_ratings} ratings for {args.n_users} users and {args.n_items} items\\\")\\n    \\n    # Apply NMF (Non-negative Matrix Factorization)\\n    # NMF requires non-negative values; our ratings are already ≥ 1.0\\n    model = NMF(n_components=args.n_components, init='random', random_state=args.seed, max_iter=500)\\n    \\n    try:\\n        W = model.fit_transform(R)  # user factors\\n        H = model.components_       # item factors\\n        R_pred = W @ H              # reconstructed matrix\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: NMF failed - {e}\\\")\\n        sys.exit(1)\\n    \\n    # Compute RMSE on observed ratings only\\n    observed_true = R[mask]\\n    observed_pred = R_pred[mask]\\n    \\n    if len(observed_true) == 0:\\n        print(\\\"TEST_FAIL: no observed ratings to evaluate\\\")\\n        sys.exit(1)\\n    \\n    rmse = np.sqrt(mean_squared_error(observed_true, observed_pred))\\n    print(f\\\"RMSE on observed ratings: {rmse:.4f}\\\")\\n    \\n    # Acceptance: RMSE ≤ 1.5\\n    if rmse <= 1.5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: RMSE {rmse:.4f} > 1.5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Speech Emotion\",\n",
            "    \"description\": \"Generate 200 synthetic speech-like feature vectors labeled emotion, train RandomForest, report accuracy. Print TEST_PASS if accuracy ≥ 0.65.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_speech_emotion(n_samples=200, n_features=13, n_classes=4, seed=42):\\n    \\\"\\\"\\\"Generate synthetic speech-like feature vectors with emotion labels.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    \\n    # Emotion classes: 0=neutral, 1=happy, 2=sad, 3=angry\\n    y = rng.integers(0, n_classes, size=n_samples)\\n    \\n    # Generate MFCC-like features (13 coefficients typical for speech)\\n    X = np.zeros((n_samples, n_features))\\n    \\n    for i in range(n_samples):\\n        emotion = y[i]\\n        # Each emotion has characteristic feature patterns\\n        if emotion == 0:  # neutral\\n            X[i] = rng.normal(0.0, 1.0, size=n_features)\\n        elif emotion == 1:  # happy (higher pitch, more energy)\\n            X[i] = rng.normal(1.5, 1.2, size=n_features)\\n        elif emotion == 2:  # sad (lower pitch, less energy)\\n            X[i] = rng.normal(-1.5, 0.8, size=n_features)\\n        elif emotion == 3:  # angry (high energy, variable pitch)\\n            X[i] = rng.normal(0.5, 2.0, size=n_features)\\n    \\n    return X, y\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 200 synthetic speech emotion vectors, train RandomForest, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of synthetic samples (default: 200).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=13, help=\\\"Number of features per sample (default: 13).\\\")\\n    p.add_argument(\\\"--n-classes\\\", type=int, default=4, help=\\\"Number of emotion classes (default: 4).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.25, help=\\\"Test set fraction (default: 0.25).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=100, help=\\\"Number of trees in RandomForest (default: 100).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n    \\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n    \\n    # Generate synthetic speech emotion dataset\\n    X, y = generate_synthetic_speech_emotion(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_classes=args.n_classes,\\n        seed=args.seed\\n    )\\n    \\n    # Validate dataset\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n    \\n    if len(X) != args.n_samples:\\n        print(f\\\"TEST_FAIL: expected {args.n_samples} samples, got {len(X)}\\\")\\n        sys.exit(1)\\n    \\n    # Split dataset\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n    \\n    # Train RandomForest\\n    clf = RandomForestClassifier(\\n        n_estimators=args.n_estimators,\\n        random_state=args.seed,\\n        n_jobs=-1\\n    )\\n    clf.fit(X_train, y_train)\\n    \\n    # Predict and evaluate\\n    y_pred = clf.predict(X_test)\\n    accuracy = accuracy_score(y_test, y_pred)\\n    \\n    print(f\\\"samples={args.n_samples} features={args.n_features} classes={args.n_classes}\\\")\\n    print(f\\\"train_size={len(X_train)} test_size={len(X_test)}\\\")\\n    print(f\\\"accuracy={accuracy:.3f}\\\")\\n    \\n    # Acceptance check\\n    if accuracy >= 0.65:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.65\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Financial Forecast\",\n",
            "    \"description\": \"Generate 300 synthetic stock-like sequences, predict direction (up/down), train LogisticRegression, report accuracy. Print TEST_PASS if accuracy ≥ 0.6.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef generate_synthetic_stock_sequences(n_sequences=300, seq_len=20, seed=42):\\n    \\\"\\\"\\\"Generate synthetic stock-like sequences with up/down labels.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    sequences = []\\n    labels = []\\n    \\n    for _ in range(n_sequences):\\n        # Generate a random walk with drift\\n        drift = rng.uniform(-0.02, 0.02)\\n        volatility = rng.uniform(0.01, 0.05)\\n        returns = rng.normal(drift, volatility, seq_len)\\n        prices = 100 * np.exp(np.cumsum(returns))\\n        \\n        # Label: 1 if final price > initial price, else 0\\n        label = 1 if prices[-1] > prices[0] else 0\\n        \\n        sequences.append(prices)\\n        labels.append(label)\\n    \\n    return np.array(sequences), np.array(labels)\\n\\ndef extract_features(sequences):\\n    \\\"\\\"\\\"Extract simple features from price sequences.\\\"\\\"\\\"\\n    features = []\\n    for seq in sequences:\\n        # Compute basic statistics\\n        mean_price = np.mean(seq)\\n        std_price = np.std(seq)\\n        min_price = np.min(seq)\\n        max_price = np.max(seq)\\n        price_range = max_price - min_price\\n        \\n        # Compute returns\\n        returns = np.diff(seq) / seq[:-1]\\n        mean_return = np.mean(returns)\\n        std_return = np.std(returns)\\n        \\n        # Trend: slope of linear fit\\n        x = np.arange(len(seq))\\n        trend = np.polyfit(x, seq, 1)[0]\\n        \\n        # Final vs initial price ratio\\n        price_ratio = seq[-1] / seq[0]\\n        \\n        features.append([\\n            mean_price, std_price, min_price, max_price, price_range,\\n            mean_return, std_return, trend, price_ratio\\n        ])\\n    \\n    return np.array(features)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 300 synthetic stock sequences, predict direction with LogisticRegression.\\\")\\n    p.add_argument(\\\"--n-sequences\\\", type=int, default=300, help=\\\"Number of synthetic sequences (default: 300).\\\")\\n    p.add_argument(\\\"--seq-len\\\", type=int, default=20, help=\\\"Length of each sequence (default: 20).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Validate inputs\\n    if args.n_sequences <= 0:\\n        print(\\\"TEST_FAIL: n_sequences must be positive\\\")\\n        sys.exit(1)\\n    if args.seq_len <= 1:\\n        print(\\\"TEST_FAIL: seq_len must be > 1\\\")\\n        sys.exit(1)\\n    if not (0 < args.test_size < 1):\\n        print(\\\"TEST_FAIL: test_size must be in (0, 1)\\\")\\n        sys.exit(1)\\n\\n    # Generate synthetic data\\n    sequences, labels = generate_synthetic_stock_sequences(\\n        n_sequences=args.n_sequences,\\n        seq_len=args.seq_len,\\n        seed=args.seed\\n    )\\n    \\n    if len(sequences) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: failed to generate synthetic data\\\")\\n        sys.exit(1)\\n\\n    # Extract features\\n    X = extract_features(sequences)\\n    y = labels\\n\\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # Scale features\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    # Train LogisticRegression\\n    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\\n    clf.fit(X_train_scaled, y_train)\\n\\n    # Evaluate\\n    accuracy = clf.score(X_test_scaled, y_test)\\n    print(f\\\"n_sequences={args.n_sequences} seq_len={args.seq_len} accuracy={accuracy:.3f}\\\")\\n\\n    # Acceptance check\\n    if accuracy >= 0.6:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} < 0.6\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST Autoencoder\",\n",
            "    \"description\": \"If MNIST available, train simple autoencoder, else use FakeData, reconstruct sample image, save original/recon as 'autoencode.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\n\\ndef load_mnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0:\\n            raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        return train, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n    import matplotlib\\n    matplotlib.use('Agg')\\n    import matplotlib.pyplot as plt\\n\\n    p = argparse.ArgumentParser(description=\\\"MNIST autoencoder (opt-in download) or FakeData fallback; saves autoencode.png.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"autoencode.png\\\", help=\\\"Output image path (default: autoencode.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, real = load_mnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n\\n    class Autoencoder(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.encoder = nn.Sequential(\\n                nn.Flatten(),\\n                nn.Linear(28*28, 128),\\n                nn.ReLU(),\\n                nn.Linear(128, 64),\\n                nn.ReLU(),\\n                nn.Linear(64, 32)\\n            )\\n            self.decoder = nn.Sequential(\\n                nn.Linear(32, 64),\\n                nn.ReLU(),\\n                nn.Linear(64, 128),\\n                nn.ReLU(),\\n                nn.Linear(128, 28*28),\\n                nn.Sigmoid()\\n            )\\n        def forward(self, x):\\n            z = self.encoder(x)\\n            recon = self.decoder(z)\\n            return recon.view(-1, 1, 28, 28)\\n\\n    model = Autoencoder()\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n    loss_fn = nn.MSELoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        total_loss = 0.0\\n        for xb, _ in train_loader:\\n            opt.zero_grad()\\n            recon = model(xb)\\n            loss = loss_fn(recon, xb)\\n            loss.backward()\\n            opt.step()\\n            total_loss += loss.item()\\n        avg_loss = total_loss / len(train_loader)\\n        print(f\\\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\\\")\\n\\n    model.eval()\\n    with torch.no_grad():\\n        sample_x, _ = next(iter(DataLoader(train_ds, batch_size=1, shuffle=False)))\\n        sample_recon = model(sample_x)\\n\\n    orig = sample_x[0, 0].cpu().numpy()\\n    recon = sample_recon[0, 0].cpu().numpy()\\n\\n    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\\n    axes[0].imshow(orig, cmap='gray')\\n    axes[0].set_title('Original')\\n    axes[0].axis('off')\\n    axes[1].imshow(recon, cmap='gray')\\n    axes[1].set_title('Reconstructed')\\n    axes[1].axis('off')\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    print(f\\\"dataset={'mnist' if real else 'fake'} output={args.output}\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST VAE Latent\",\n",
            "    \"description\": \"If FashionMNIST available, train basic VAE encoder, else use FakeData, visualize latent space, save as 'latent_vae.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_fashionmnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.FashionMNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0:\\n            raise RuntimeError(\\\"FashionMNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        return train, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n\\n    p = argparse.ArgumentParser(description=\\\"FashionMNIST VAE latent space visualization with opt-in download or FakeData fallback.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--latent-dim\\\", type=int, default=2, help=\\\"Latent dimension (default: 2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit FashionMNIST download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"latent_vae.png\\\", help=\\\"Output latent space plot filename (default: latent_vae.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, real = load_fashionmnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n\\n    class VAE(nn.Module):\\n        def __init__(self, latent_dim=2):\\n            super().__init__()\\n            self.latent_dim = latent_dim\\n            self.encoder = nn.Sequential(\\n                nn.Flatten(),\\n                nn.Linear(28*28, 256), nn.ReLU(),\\n                nn.Linear(256, 128), nn.ReLU()\\n            )\\n            self.fc_mu = nn.Linear(128, latent_dim)\\n            self.fc_logvar = nn.Linear(128, latent_dim)\\n            self.decoder = nn.Sequential(\\n                nn.Linear(latent_dim, 128), nn.ReLU(),\\n                nn.Linear(128, 256), nn.ReLU(),\\n                nn.Linear(256, 28*28), nn.Sigmoid()\\n            )\\n\\n        def encode(self, x):\\n            h = self.encoder(x)\\n            return self.fc_mu(h), self.fc_logvar(h)\\n\\n        def reparameterize(self, mu, logvar):\\n            std = torch.exp(0.5 * logvar)\\n            eps = torch.randn_like(std)\\n            return mu + eps * std\\n\\n        def decode(self, z):\\n            return self.decoder(z).view(-1, 1, 28, 28)\\n\\n        def forward(self, x):\\n            mu, logvar = self.encode(x)\\n            z = self.reparameterize(mu, logvar)\\n            recon = self.decode(z)\\n            return recon, mu, logvar\\n\\n    def vae_loss(recon, x, mu, logvar):\\n        bce = nn.functional.binary_cross_entropy(recon, x, reduction='sum')\\n        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\\n        return bce + kld\\n\\n    model = VAE(latent_dim=args.latent_dim)\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        total_loss = 0.0\\n        for xb, _ in train_loader:\\n            opt.zero_grad()\\n            recon, mu, logvar = model(xb)\\n            loss = vae_loss(recon, xb, mu, logvar)\\n            loss.backward()\\n            opt.step()\\n            total_loss += loss.item()\\n        avg_loss = total_loss / len(train_loader.dataset)\\n        print(f\\\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\\\")\\n\\n    model.eval()\\n    latents = []\\n    labels = []\\n    with torch.no_grad():\\n        for xb, yb in train_loader:\\n            mu, _ = model.encode(xb)\\n            latents.append(mu.cpu().numpy())\\n            labels.append(yb.cpu().numpy())\\n    latents = np.concatenate(latents, axis=0)\\n    labels = np.concatenate(labels, axis=0)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10', alpha=0.6, s=10)\\n    plt.colorbar(scatter, label='Class')\\n    plt.xlabel('Latent Dim 1')\\n    plt.ylabel('Latent Dim 2')\\n    plt.title(f\\\"VAE Latent Space ({'FashionMNIST' if real else 'FakeData'})\\\")\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    print(f\\\"Saved latent space plot to {args.output}\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"CIFAR10 Transfer Features\",\n",
            "    \"description\": \"If CIFAR10 available, extract features via pretrained CNN, else use FakeData, train LogisticRegression on features, report accuracy. Print TEST_PASS if accuracy ≥ 0.6 or fallback ≥ 0.4.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_cifar10_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n        ])\\n        train = datasets.CIFAR10(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.CIFAR10(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\\n        return train, test, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n        ])\\n        train = FakeData(size=max_train, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        test = FakeData(size=max_test, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef extract_features(model, dataloader, device):\\n    import torch\\n    model.eval()\\n    features_list = []\\n    labels_list = []\\n    with torch.no_grad():\\n        for xb, yb in dataloader:\\n            xb = xb.to(device)\\n            feat = model(xb)\\n            features_list.append(feat.cpu().numpy())\\n            labels_list.append(yb.numpy())\\n    X = np.vstack(features_list)\\n    y = np.concatenate(labels_list)\\n    return X, y\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    from torch.utils.data import DataLoader\\n    from sklearn.linear_model import LogisticRegression\\n\\n    p = argparse.ArgumentParser(description=\\\"CIFAR10 transfer features (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--max-train\\\", type=int, default=2000, help=\\\"Max training samples (default: 2000).\\\")\\n    p.add_argument(\\\"--max-test\\\", type=int, default=500, help=\\\"Max test samples (default: 500).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    train_ds, test_ds, real = load_cifar10_or_fakedata(\\n        max_train=args.max_train,\\n        max_test=args.max_test,\\n        seed=args.seed,\\n        allow_download=args.allow_download\\n    )\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=False)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    device = torch.device(\\\"cpu\\\")\\n\\n    class FeatureExtractor(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.conv = nn.Sequential(\\n                nn.Conv2d(3, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\\n                nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\\n                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\\n            )\\n        def forward(self, x):\\n            x = self.conv(x)\\n            return x.view(x.size(0), -1)\\n\\n    model = FeatureExtractor().to(device)\\n\\n    print(\\\"Extracting training features...\\\")\\n    X_train, y_train = extract_features(model, train_loader, device)\\n    print(\\\"Extracting test features...\\\")\\n    X_test, y_test = extract_features(model, test_loader, device)\\n\\n    if X_train.shape[0] == 0 or X_test.shape[0] == 0:\\n        print(\\\"TEST_FAIL: no samples extracted\\\")\\n        sys.exit(1)\\n\\n    clf = LogisticRegression(max_iter=500, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    print(f\\\"acc={acc:.3f} dataset={'cifar10' if real else 'fake'}\\\")\\n\\n    threshold = 0.6 if real else 0.4\\n    if acc >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"ImageFolder Augmentation\",\n",
            "    \"description\": \"If ImageFolder available, apply rotation/flips to 20 images, else generate FakeData, augment, save augmented grid as 'augment_grid.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nfrom pathlib import Path\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply rotation/flips to ImageFolder or FakeData; save augmented grid as augment_grid.png.\\\")\\n    p.add_argument(\\\"--imagefolder-path\\\", type=str, default=None, help=\\\"Path to ImageFolder root (default: None, use FakeData).\\\")\\n    p.add_argument(\\\"--num-images\\\", type=int, default=20, help=\\\"Number of images to augment (default: 20).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"augment_grid.png\\\", help=\\\"Output grid filename (default: augment_grid.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit dataset download if needed.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        from torchvision.utils import make_grid\\n        from PIL import Image\\n    except ImportError:\\n        print(\\\"TEST_FAIL: torchvision or PIL not available\\\")\\n        sys.exit(1)\\n\\n    torch.manual_seed(args.seed)\\n\\n    use_imagefolder = False\\n    if args.imagefolder_path and os.path.isdir(args.imagefolder_path):\\n        try:\\n            base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\\n            ds = datasets.ImageFolder(root=args.imagefolder_path, transform=base_tfm)\\n            if len(ds) > 0:\\n                use_imagefolder = True\\n                print(f\\\"Using ImageFolder from {args.imagefolder_path} with {len(ds)} images\\\")\\n        except Exception as e:\\n            print(f\\\"ImageFolder failed: {e}, falling back to FakeData\\\")\\n\\n    if not use_imagefolder:\\n        print(\\\"Using FakeData fallback\\\")\\n        base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\\n        ds = datasets.FakeData(size=args.num_images, image_size=(3, 64, 64), num_classes=10, transform=base_tfm)\\n\\n    num_samples = min(args.num_images, len(ds))\\n    indices = list(range(num_samples))\\n    random.shuffle(indices)\\n    indices = indices[:num_samples]\\n\\n    augmentations = [\\n        transforms.RandomRotation(degrees=30),\\n        transforms.RandomHorizontalFlip(p=1.0),\\n        transforms.RandomVerticalFlip(p=1.0),\\n        transforms.Compose([transforms.RandomRotation(degrees=15), transforms.RandomHorizontalFlip(p=0.5)]),\\n    ]\\n\\n    augmented_images = []\\n    for idx in indices:\\n        img_tensor, _ = ds[idx]\\n        img_pil = transforms.ToPILImage()(img_tensor)\\n        aug = random.choice(augmentations)\\n        aug_pil = aug(img_pil)\\n        aug_tensor = transforms.ToTensor()(aug_pil)\\n        augmented_images.append(aug_tensor)\\n\\n    if len(augmented_images) == 0:\\n        print(\\\"TEST_FAIL: no images to augment\\\")\\n        sys.exit(1)\\n\\n    grid_tensor = make_grid(augmented_images, nrow=5, padding=2, normalize=False)\\n    grid_pil = transforms.ToPILImage()(grid_tensor)\\n    grid_pil.save(args.output)\\n    print(f\\\"Saved augmented grid to {args.output} with {len(augmented_images)} images\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FakeData Style Transfer\",\n",
            "    \"description\": \"Generate FakeData content/style pairs, apply basic style transfer algorithm, save result as 'style_transfer.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom pathlib import Path\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"FakeData style transfer: generate content/style pairs, apply basic transfer, save result.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"style_transfer.png\\\", help=\\\"Output image path (default: style_transfer.png).\\\")\\n    p.add_argument(\\\"--size\\\", type=int, default=256, help=\\\"Image size (default: 256).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        import torch\\n        from torchvision.datasets import FakeData\\n        from torchvision import transforms\\n        from PIL import Image\\n    except ImportError as e:\\n        print(f\\\"TEST_FAIL: missing dependency {e}\\\")\\n        sys.exit(1)\\n\\n    # generate FakeData content and style images\\n    torch.manual_seed(args.seed)\\n    tfm = transforms.Compose([\\n        transforms.Resize((args.size, args.size)),\\n        transforms.ToTensor()\\n    ])\\n    \\n    fake_ds = FakeData(size=2, image_size=(3, args.size, args.size), transform=tfm)\\n    content_tensor, _ = fake_ds[0]\\n    style_tensor, _ = fake_ds[1]\\n\\n    # basic style transfer: blend content and style with simple weighted average\\n    # convert to numpy for manipulation\\n    content_np = content_tensor.permute(1, 2, 0).numpy()\\n    style_np = style_tensor.permute(1, 2, 0).numpy()\\n    \\n    # simple transfer: 70% content + 30% style\\n    transferred = 0.7 * content_np + 0.3 * style_np\\n    transferred = np.clip(transferred, 0, 1)\\n    \\n    # convert back to PIL and save\\n    transferred_uint8 = (transferred * 255).astype(np.uint8)\\n    result_img = Image.fromarray(transferred_uint8, mode='RGB')\\n    result_img.save(args.output)\\n    \\n    # acceptance check\\n    if Path(args.output).exists():\\n        print(f\\\"Saved style transfer result to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"KMeans CIFAR Segmentation\",\n",
            "    \"description\": \"If CIFAR10 available, segment image via KMeans clustering, else use FakeData, save segmented image as 'segmented.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nfrom PIL import Image\\n\\ndef load_cifar_or_fakedata(seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        ds = datasets.CIFAR10(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(ds) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        img_tensor, _ = ds[0]\\n        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\\n        return img_np, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        ds = FakeData(size=1, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        img_tensor, _ = ds[0]\\n        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\\n        return img_np, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"KMeans CIFAR10 segmentation (opt-in download) or FakeData fallback; save segmented.png; TEST_PASS if file exists.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=4, help=\\\"Number of clusters for KMeans (default: 4).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"segmented.png\\\", help=\\\"Output segmented image path (default: segmented.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    img, real = load_cifar_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    if img is None or img.size == 0:\\n        print(\\\"TEST_FAIL: image not available\\\")\\n        sys.exit(1)\\n\\n    h, w, c = img.shape\\n    pixels = img.reshape(-1, c).astype(np.float32)\\n\\n    from sklearn.cluster import KMeans\\n    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\\n    labels = kmeans.fit_predict(pixels)\\n    centers = kmeans.cluster_centers_.astype(np.uint8)\\n\\n    segmented = centers[labels].reshape(h, w, c)\\n\\n    pil_img = Image.fromarray(segmented, mode=\\\"RGB\\\")\\n    pil_img.save(args.output)\\n\\n    print(f\\\"dataset={'cifar10' if real else 'fake'} k={args.k} saved={args.output}\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Outlier Detection\",\n",
            "    \"description\": \"Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as 'outliers.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.datasets import load_iris\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as outliers.png.\\\")\\n    p.add_argument(\\\"--contamination\\\", type=float, default=0.1, help=\\\"Expected proportion of outliers (default: 0.1).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"outliers.png\\\", help=\\\"Output plot filename (default: outliers.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_iris()\\n        X = data.data\\n        feature_names = data.feature_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    iso = IsolationForest(contamination=args.contamination, random_state=args.seed)\\n    iso.fit(X)\\n    predictions = iso.predict(X)\\n\\n    normal_mask = predictions == 1\\n    outlier_mask = predictions == -1\\n\\n    normal_points = X[normal_mask]\\n    outlier_points = X[outlier_mask]\\n\\n    num_outliers = outlier_points.shape[0]\\n    num_normal = normal_points.shape[0]\\n\\n    print(f\\\"Total samples: {len(X)}\\\")\\n    print(f\\\"Normal points: {num_normal}\\\")\\n    print(f\\\"Outliers detected: {num_outliers}\\\")\\n\\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\\n    ax.scatter(normal_points[:, 0], normal_points[:, 1], c='blue', label='Normal', alpha=0.6, edgecolors='k')\\n    ax.scatter(outlier_points[:, 0], outlier_points[:, 1], c='red', label='Outlier', alpha=0.8, edgecolors='k', s=100)\\n    ax.set_xlabel(feature_names[0])\\n    ax.set_ylabel(feature_names[1])\\n    ax.set_title(\\\"Iris Outlier Detection (IsolationForest)\\\")\\n    ax.legend()\\n    ax.grid(True, alpha=0.3)\\n\\n    try:\\n        plt.savefig(args.output, dpi=100, bbox_inches='tight')\\n        print(f\\\"Plot saved to {args.output}\\\")\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    if os.path.getsize(args.output) == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Anomaly Score\",\n",
            "    \"description\": \"Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as 'anomaly_scores.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as anomaly_scores.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"anomaly_scores.png\\\", help=\\\"Output histogram filename (default: anomaly_scores.png).\\\")\\n    p.add_argument(\\\"--nu\\\", type=float, default=0.1, help=\\\"OneClassSVM nu parameter (default: 0.1).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X = data.data\\n        if X is None or len(X) == 0:\\n            raise ValueError(\\\"wine dataset is empty\\\")\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: could not load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    oc_svm = OneClassSVM(nu=args.nu, kernel='rbf', gamma='auto')\\n    oc_svm.fit(X_scaled)\\n\\n    scores = oc_svm.decision_function(X_scaled)\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.hist(scores, bins=30, edgecolor='black', alpha=0.7)\\n    plt.xlabel('Anomaly Score')\\n    plt.ylabel('Frequency')\\n    plt.title('Wine Dataset Anomaly Scores (OneClassSVM)')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Histogram saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SHAP Values\",\n",
            "    \"description\": \"Train RandomForest on breast_cancer, compute SHAP values for one sample, save waterfall plot as 'shap_waterfall.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nimport shap\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train RandomForest on breast_cancer, compute SHAP values, save waterfall plot.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"shap_waterfall.png\\\", help=\\\"Output waterfall plot filename (default: shap_waterfall.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=50, help=\\\"Number of trees in RandomForest (default: 50).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n        feature_names = data.feature_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed, max_depth=5)\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"RandomForest accuracy on test set: {acc:.3f}\\\")\\n\\n    if len(X_test) == 0:\\n        print(\\\"TEST_FAIL: test set is empty\\\")\\n        sys.exit(1)\\n\\n    explainer = shap.TreeExplainer(clf)\\n    shap_values = explainer(X_test)\\n\\n    sample_idx = 0\\n    shap.plots.waterfall(shap_values[sample_idx, :, 1], show=False)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100, bbox_inches='tight')\\n    plt.close()\\n    print(f\\\"Saved SHAP waterfall plot to {args.output}\\\")\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file does not exist\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Grad-CAM Heatmap\",\n",
            "    \"description\": \"Train simple CNN on digits, compute Grad-CAM heatmap for one image, overlay on input, save as 'gradcam.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train CNN on digits, compute Grad-CAM heatmap, overlay on input, save as gradcam.png.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"gradcam.png\\\", help=\\\"Output heatmap filename (default: gradcam.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n    from torchvision import datasets, transforms\\n    import cv2\\n\\n    def load_mnist_or_fakedata(seed=42, allow_download=False):\\n        try:\\n            torch.manual_seed(seed)\\n            tfm = transforms.ToTensor()\\n            train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n            test = datasets.MNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n            if len(train) == 0 or len(test) == 0:\\n                raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n            train = torch.utils.data.Subset(train, list(range(min(len(train), 2000))))\\n            test = torch.utils.data.Subset(test, list(range(min(len(test), 500))))\\n            return train, test, True\\n        except Exception:\\n            torch.manual_seed(seed)\\n            tfm = transforms.ToTensor()\\n            from torchvision.datasets import FakeData\\n            train = FakeData(size=2000, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n            test = FakeData(size=500, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n            return train, test, False\\n\\n    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    class SimpleCNN(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\\n            self.relu1 = nn.ReLU()\\n            self.pool1 = nn.MaxPool2d(2)\\n            self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\\n            self.relu2 = nn.ReLU()\\n            self.pool2 = nn.MaxPool2d(2)\\n            self.flatten = nn.Flatten()\\n            self.fc1 = nn.Linear(32 * 7 * 7, 64)\\n            self.relu3 = nn.ReLU()\\n            self.fc2 = nn.Linear(64, 10)\\n            self.gradients = None\\n            self.activations = None\\n\\n        def forward(self, x):\\n            x = self.conv1(x)\\n            x = self.relu1(x)\\n            x = self.pool1(x)\\n            x = self.conv2(x)\\n            x = self.relu2(x)\\n            x = self.pool2(x)\\n            if x.requires_grad:\\n                x.register_hook(self.save_gradient)\\n            self.activations = x\\n            x = self.flatten(x)\\n            x = self.fc1(x)\\n            x = self.relu3(x)\\n            x = self.fc2(x)\\n            return x\\n\\n        def save_gradient(self, grad):\\n            self.gradients = grad\\n\\n    model = SimpleCNN()\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n    loss_fn = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            opt.zero_grad()\\n            logits = model(xb)\\n            loss = loss_fn(logits, yb)\\n            loss.backward()\\n            opt.step()\\n\\n    model.eval()\\n    correct = total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            pred = model(xb).argmax(1)\\n            correct += (pred == yb).sum().item()\\n            total += yb.numel()\\n    acc = correct / max(total, 1)\\n    print(f\\\"acc={acc:.3f} dataset={'mnist' if real else 'fake'}\\\")\\n\\n    test_iter = iter(test_loader)\\n    xb, yb = next(test_iter)\\n    img = xb[0:1]\\n    label = yb[0].item()\\n\\n    model.zero_grad()\\n    img.requires_grad = True\\n    output = model(img)\\n    pred_class = output.argmax(1).item()\\n    score = output[0, pred_class]\\n    score.backward()\\n\\n    gradients = model.gradients\\n    activations = model.activations\\n\\n    if gradients is None or activations is None:\\n        print(\\\"TEST_FAIL: gradients or activations not captured\\\")\\n        sys.exit(1)\\n\\n    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\\n    for i in range(activations.shape[1]):\\n        activations[:, i, :, :] *= pooled_gradients[i]\\n\\n    heatmap = torch.mean(activations, dim=1).squeeze()\\n    heatmap = torch.clamp(heatmap, min=0)\\n    heatmap /= (torch.max(heatmap) + 1e-8)\\n    heatmap_np = heatmap.detach().cpu().numpy()\\n\\n    heatmap_resized = cv2.resize(heatmap_np, (28, 28))\\n    heatmap_uint8 = np.uint8(255 * heatmap_resized)\\n    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\\n\\n    img_np = img.squeeze().detach().cpu().numpy()\\n    img_uint8 = np.uint8(255 * img_np)\\n    img_color = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2BGR)\\n\\n    overlay = cv2.addWeighted(img_color, 0.5, heatmap_color, 0.5, 0)\\n    cv2.imwrite(args.output, overlay)\\n\\n    if os.path.exists(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# your existing data\n",
        "items = result[\"items_with_code\"]\n",
        "\n",
        "# the 6 fixed versions\n",
        "fixed_items = [\n",
        "    {\n",
        "        \"title\": \"Classification Synthetic Data\",\n",
        "        \"description\": \"Generate make_classification data (n=500, n_features=4), train an SGDClassifier inside a StandardScaler pipeline, report accuracy. Print TEST_PASS if accuracy ≥ 0.85.\",\n",
        "        \"code\": \"\"\"import argparse\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy.\")\n",
        "    p.add_argument(\"--n-samples\", type=int, default=500, help=\"Number of samples (default: 500).\")\n",
        "    p.add_argument(\"--n-features\", type=int, default=4, help=\"Number of features (default: 4).\")\n",
        "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    try:\n",
        "        import torch\n",
        "        torch.manual_seed(args.seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    X, y = make_classification(\n",
        "        n_samples=args.n_samples,\n",
        "        n_features=args.n_features,\n",
        "        n_informative=max(2, args.n_features // 2),\n",
        "        n_redundant=0,\n",
        "        n_clusters_per_class=1,\n",
        "        random_state=args.seed\n",
        "    )\n",
        "\n",
        "    if X is None or y is None or len(X) == 0:\n",
        "        print(\"TEST_FAIL: dataset generation failed\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
        "    )\n",
        "\n",
        "    clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"sgd\", SGDClassifier(max_iter=1000, tol=1e-3, random_state=args.seed))\n",
        "    ])\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"accuracy={acc:.3f}\")\n",
        "\n",
        "    if acc >= 0.85:\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: accuracy below 0.85\")\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Digits Confusion Heatmap\",\n",
        "        \"description\": \"Train SVC on sklearn digits, compute confusion matrix, plot it with matplotlib (no seaborn), save as digits_heatmap.png. Print TEST_PASS if file exists.\",\n",
        "        \"code\": \"\"\"import argparse\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"Train SVC on digits dataset, plot confusion matrix heatmap, save as digits_heatmap.png.\")\n",
        "    p.add_argument(\"--output\", type=str, default=\"digits_heatmap.png\", help=\"Output heatmap filename (default: digits_heatmap.png).\")\n",
        "    p.add_argument(\"--test-size\", type=float, default=0.3, help=\"Test set fraction (default: 0.3).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    try:\n",
        "        import torch\n",
        "        torch.manual_seed(args.seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        data = load_digits()\n",
        "        X, y = data.data, data.target\n",
        "    except Exception as e:\n",
        "        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if X is None or y is None or len(X) == 0:\n",
        "        print(\"TEST_FAIL: digits dataset is empty\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    Xtr, Xte, ytr, yte = train_test_split(\n",
        "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
        "    )\n",
        "\n",
        "    clf = SVC(kernel='rbf', gamma='scale', random_state=args.seed)\n",
        "    clf.fit(Xtr, ytr)\n",
        "    ypred = clf.predict(Xte)\n",
        "\n",
        "    cm = confusion_matrix(yte, ypred)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    im = ax.imshow(cm, cmap='Blues')\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Actual')\n",
        "    ax.set_title('Digits Confusion Matrix')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(args.output, dpi=100)\n",
        "    plt.close()\n",
        "\n",
        "    if not os.path.isfile(args.output):\n",
        "        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Confusion matrix heatmap saved to {args.output}\")\n",
        "    print(\"TEST_PASS\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"ImageFolder Histogram\",\n",
        "        \"description\": \"Try to load an ImageFolder and compute average RGB histogram, else fall back to FakeData. Save plot to histogram.png. Print TEST_PASS if file exists and is non-empty.\",\n",
        "        \"code\": \"\"\"import argparse\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_imagefolder_or_fakedata(root_dir, num_fake=50, seed=42, allow_download=False):\n",
        "    try:\n",
        "        import torch\n",
        "        from torchvision import datasets, transforms\n",
        "        torch.manual_seed(seed)\n",
        "        if os.path.isdir(root_dir):\n",
        "            tfm = transforms.ToTensor()\n",
        "            ds = datasets.ImageFolder(root=root_dir, transform=tfm)\n",
        "            if len(ds) > 0:\n",
        "                return ds, True\n",
        "        raise RuntimeError(\"ImageFolder not available or empty\")\n",
        "    except Exception:\n",
        "        import torch\n",
        "        from torchvision import transforms\n",
        "        from torchvision.datasets import FakeData\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        ds = FakeData(size=num_fake, image_size=(3, 64, 64), num_classes=2, transform=tfm)\n",
        "        return ds, False\n",
        "\n",
        "def compute_rgb_histogram(dataset, num_samples=None):\n",
        "    if num_samples is None:\n",
        "        num_samples = len(dataset)\n",
        "    else:\n",
        "        num_samples = min(num_samples, len(dataset))\n",
        "\n",
        "    r_sum = np.zeros(256, dtype=np.float64)\n",
        "    g_sum = np.zeros(256, dtype=np.float64)\n",
        "    b_sum = np.zeros(256, dtype=np.float64)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        img_tensor, _ = dataset[i]\n",
        "        img_np = (img_tensor.numpy() * 255).astype(np.uint8)\n",
        "        r_hist, _ = np.histogram(img_np[0].flatten(), bins=256, range=(0, 256))\n",
        "        g_hist, _ = np.histogram(img_np[1].flatten(), bins=256, range=(0, 256))\n",
        "        b_hist, _ = np.histogram(img_np[2].flatten(), bins=256, range=(0, 256))\n",
        "        r_sum += r_hist\n",
        "        g_sum += g_hist\n",
        "        b_sum += b_hist\n",
        "\n",
        "    r_avg = r_sum / num_samples\n",
        "    g_avg = g_sum / num_samples\n",
        "    b_avg = b_sum / num_samples\n",
        "    return r_avg, g_avg, b_avg\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"Compute average RGB histogram from ImageFolder or FakeData and save as histogram.png.\")\n",
        "    p.add_argument(\"--root-dir\", type=str, default=\"./imagefolder_data\", help=\"Path to ImageFolder root directory (default: ./imagefolder_data).\")\n",
        "    p.add_argument(\"--num-fake\", type=int, default=50, help=\"Number of FakeData images if ImageFolder unavailable (default: 50).\")\n",
        "    p.add_argument(\"--output\", type=str, default=\"histogram.png\", help=\"Output histogram file path (default: histogram.png).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit dataset download if needed.\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    try:\n",
        "        import torch\n",
        "        torch.manual_seed(args.seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    dataset, is_real = load_imagefolder_or_fakedata(\n",
        "        root_dir=args.root_dir,\n",
        "        num_fake=args.num_fake,\n",
        "        seed=args.seed,\n",
        "        allow_download=args.allow_download\n",
        "    )\n",
        "\n",
        "    if dataset is None or len(dataset) == 0:\n",
        "        print(\"TEST_FAIL: dataset not available or empty\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Using {'ImageFolder' if is_real else 'FakeData'} with {len(dataset)} images\")\n",
        "\n",
        "    r_hist, g_hist, b_hist = compute_rgb_histogram(dataset)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bins = np.arange(256)\n",
        "    ax.plot(bins, r_hist, color='red', alpha=0.7, label='Red')\n",
        "    ax.plot(bins, g_hist, color='green', alpha=0.7, label='Green')\n",
        "    ax.plot(bins, b_hist, color='blue', alpha=0.7, label='Blue')\n",
        "    ax.set_xlabel('Pixel Value')\n",
        "    ax.set_ylabel('Average Frequency')\n",
        "    ax.set_title('Average RGB Histogram')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(args.output, dpi=100)\n",
        "    plt.close()\n",
        "\n",
        "    if os.path.isfile(args.output):\n",
        "        file_size = os.path.getsize(args.output)\n",
        "        if file_size > 0:\n",
        "            print(f\"Histogram saved to {args.output} ({file_size} bytes)\")\n",
        "            print(\"TEST_PASS\")\n",
        "        else:\n",
        "            print(\"TEST_FAIL: output file is empty\")\n",
        "            sys.exit(1)\n",
        "    else:\n",
        "        print(\"TEST_FAIL: output file does not exist\")\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"KMeans on CIFAR Colors\",\n",
        "        \"description\": \"Load CIFAR10 pixels (or FakeData fallback), run KMeans(k=5) on RGB pixels, and print inertia. Print TEST_PASS if inertia < 2.0e5. Uses fewer images to reduce runtime.\",\n",
        "        \"code\": \"\"\"import argparse\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def load_cifar_or_fakedata(max_samples=500, seed=42, allow_download=False):\n",
        "    try:\n",
        "        import torch\n",
        "        from torchvision import datasets, transforms\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        train = datasets.CIFAR10(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
        "        if len(train) == 0:\n",
        "            raise RuntimeError(\"CIFAR10 cache missing and download disabled\")\n",
        "        subset = torch.utils.data.Subset(train, list(range(min(len(train), max_samples))))\n",
        "        loader = torch.utils.data.DataLoader(subset, batch_size=max_samples, shuffle=False)\n",
        "        for xb, _ in loader:\n",
        "            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\n",
        "            return pixels, True\n",
        "    except Exception:\n",
        "        import torch\n",
        "        from torchvision import transforms\n",
        "        from torchvision.datasets import FakeData\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        fake = FakeData(size=max_samples, image_size=(3, 32, 32), num_classes=10, transform=tfm)\n",
        "        loader = torch.utils.data.DataLoader(fake, batch_size=max_samples, shuffle=False)\n",
        "        for xb, _ in loader:\n",
        "            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\n",
        "            return pixels, False\n",
        "    return None, False\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"KMeans on CIFAR10 RGB pixels (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n",
        "    p.add_argument(\"--k\", type=int, default=5, help=\"Number of clusters (default: 5).\")\n",
        "    p.add_argument(\"--max-samples\", type=int, default=500, help=\"Max images to load (default: 500).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit CIFAR10 download if not cached.\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    try:\n",
        "        import torch\n",
        "        torch.manual_seed(args.seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    pixels, real = load_cifar_or_fakedata(max_samples=args.max_samples, seed=args.seed, allow_download=args.allow_download)\n",
        "    if pixels is None or len(pixels) == 0:\n",
        "        print(\"TEST_FAIL: dataset not available\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    from sklearn.cluster import KMeans\n",
        "    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\n",
        "    kmeans.fit(pixels)\n",
        "    inertia = kmeans.inertia_\n",
        "\n",
        "    print(f\"dataset={'cifar10' if real else 'fake'} k={args.k} inertia={inertia:.2e}\")\n",
        "\n",
        "    if inertia < 2.0e5:\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: inertia >= 2.0e5\")\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"MNIST Autoencoder\",\n",
        "        \"description\": \"Load MNIST (or FakeData fallback), train a small autoencoder for a few epochs, reconstruct one image, and save original/reconstructed pair to autoencode.png. Print TEST_PASS if file exists.\",\n",
        "        \"code\": \"\"\"import argparse\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def load_mnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\n",
        "    try:\n",
        "        import torch\n",
        "        from torchvision import datasets, transforms\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
        "        if len(train) == 0:\n",
        "            raise RuntimeError(\"MNIST cache missing and download disabled\")\n",
        "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
        "        return train, True\n",
        "    except Exception:\n",
        "        import torch\n",
        "        from torchvision import transforms\n",
        "        from torchvision.datasets import FakeData\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
        "        return train, False\n",
        "\n",
        "def main():\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import DataLoader\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg')\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    p = argparse.ArgumentParser(description=\"MNIST autoencoder (opt-in download) or FakeData fallback; saves autoencode.png.\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=3, help=\"Training epochs (default: 3).\")\n",
        "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n",
        "    p.add_argument(\"--output\", type=str, default=\"autoencode.png\", help=\"Output image path (default: autoencode.png).\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    train_ds, real = load_mnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
        "\n",
        "    class Autoencoder(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.encoder = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(28*28, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 32)\n",
        "            )\n",
        "            self.decoder = nn.Sequential(\n",
        "                nn.Linear(32, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, 28*28),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            z = self.encoder(x)\n",
        "            recon = self.decoder(z)\n",
        "            return recon.view(-1, 1, 28, 28)\n",
        "\n",
        "    model = Autoencoder()\n",
        "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0.0\n",
        "        for xb, _ in train_loader:\n",
        "            opt.zero_grad()\n",
        "            recon = model(xb)\n",
        "            loss = loss_fn(recon, xb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_x, _ = next(iter(DataLoader(train_ds, batch_size=1, shuffle=False)))\n",
        "        sample_recon = model(sample_x)\n",
        "\n",
        "    orig = sample_x[0, 0].cpu().numpy()\n",
        "    recon = sample_recon[0, 0].cpu().numpy()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
        "    axes[0].imshow(orig, cmap='gray')\n",
        "    axes[0].set_title('Original')\n",
        "    axes[0].axis('off')\n",
        "    axes[1].imshow(recon, cmap='gray')\n",
        "    axes[1].set_title('Reconstructed')\n",
        "    axes[1].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(args.output)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"dataset={'mnist' if real else 'fake'} output={args.output}\")\n",
        "\n",
        "    if os.path.isfile(args.output):\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: output file not created\")\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"FashionMNIST VAE Latent\",\n",
        "        \"description\": \"Train a small VAE on FashionMNIST (or FakeData fallback), collect 2D latent codes, and save scatter plot to latent_vae.png. Print TEST_PASS if file exists.\",\n",
        "        \"code\": \"\"\"import argparse\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_fashionmnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\n",
        "    try:\n",
        "        import torch\n",
        "        from torchvision import datasets, transforms\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        train = datasets.FashionMNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
        "        if len(train) == 0:\n",
        "            raise RuntimeError(\"FashionMNIST cache missing and download disabled\")\n",
        "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
        "        return train, True\n",
        "    except Exception:\n",
        "        import torch\n",
        "        from torchvision import transforms\n",
        "        from torchvision.datasets import FakeData\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
        "        return train, False\n",
        "\n",
        "def main():\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    p = argparse.ArgumentParser(description=\"FashionMNIST VAE latent space visualization with opt-in download or FakeData fallback.\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=2, help=\"Training epochs (default: 2).\")\n",
        "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
        "    p.add_argument(\"--latent-dim\", type=int, default=2, help=\"Latent dimension (default: 2).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit FashionMNIST download if not cached.\")\n",
        "    p.add_argument(\"--output\", type=str, default=\"latent_vae.png\", help=\"Output latent space plot filename (default: latent_vae.png).\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    train_ds, real = load_fashionmnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
        "\n",
        "    class VAE(nn.Module):\n",
        "        def __init__(self, latent_dim=2):\n",
        "            super().__init__()\n",
        "            self.latent_dim = latent_dim\n",
        "            self.encoder = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(28*28, 256), nn.ReLU(),\n",
        "                nn.Linear(256, 128), nn.ReLU()\n",
        "            )\n",
        "            self.fc_mu = nn.Linear(128, latent_dim)\n",
        "            self.fc_logvar = nn.Linear(128, latent_dim)\n",
        "            self.decoder = nn.Sequential(\n",
        "                nn.Linear(latent_dim, 128), nn.ReLU(),\n",
        "                nn.Linear(128, 256), nn.ReLU(),\n",
        "                nn.Linear(256, 28*28), nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "        def encode(self, x):\n",
        "            h = self.encoder(x)\n",
        "            return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "        def reparameterize(self, mu, logvar):\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mu + eps * std\n",
        "\n",
        "        def decode(self, z):\n",
        "            return self.decoder(z).view(-1, 1, 28, 28)\n",
        "\n",
        "        def forward(self, x):\n",
        "            mu, logvar = self.encode(x)\n",
        "            z = self.reparameterize(mu, logvar)\n",
        "            recon = self.decode(z)\n",
        "            return recon, mu, logvar\n",
        "\n",
        "    def vae_loss(recon, x, mu, logvar):\n",
        "        bce = nn.functional.binary_cross_entropy(recon, x, reduction='sum')\n",
        "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return bce + kld\n",
        "\n",
        "    model = VAE(latent_dim=args.latent_dim)\n",
        "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0.0\n",
        "        for xb, _ in train_loader:\n",
        "            opt.zero_grad()\n",
        "            recon, mu, logvar = model(xb)\n",
        "            loss = vae_loss(recon, xb, mu, logvar)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader.dataset)\n",
        "        print(f\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    latents = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in train_loader:\n",
        "            mu, _ = model.encode(xb)\n",
        "            latents.append(mu.cpu().numpy())\n",
        "            labels.append(yb.cpu().numpy())\n",
        "    latents = np.concatenate(latents, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10', alpha=0.6, s=10)\n",
        "    plt.colorbar(scatter, label='Class')\n",
        "    plt.xlabel('Latent Dim 1')\n",
        "    plt.ylabel('Latent Dim 2')\n",
        "    plt.title(f\"VAE Latent Space ({'FashionMNIST' if real else 'FakeData'})\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(args.output, dpi=100)\n",
        "    plt.close()\n",
        "    print(f\"Saved latent space plot to {args.output}\")\n",
        "\n",
        "    if os.path.isfile(args.output):\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: output file not created\")\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# titles we want to replace\n",
        "titles_to_fix = {fi[\"title\"] for fi in fixed_items}\n",
        "\n",
        "# replace in-place\n",
        "title_to_fixed = {fi[\"title\"]: fi for fi in fixed_items}\n",
        "for i, item in enumerate(items):\n",
        "    t = item.get(\"title\")\n",
        "    if t in titles_to_fix:\n",
        "        items[i] = title_to_fixed[t]\n",
        "\n",
        "# if some are missing (not present in original), append them\n",
        "existing_titles = {it[\"title\"] for it in items}\n",
        "for fi in fixed_items:\n",
        "    if fi[\"title\"] not in existing_titles:\n",
        "        items.append(fi)\n",
        "\n",
        "# optional: pretty print\n",
        "print(json.dumps(items, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcbboHst9Cg6",
        "outputId": "bd26df9d-3e19-45c3-d9e9-0d2654592f40"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Iris KNN Classifier\",\n",
            "    \"description\": \"Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.9.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Iris KNN classifier with k=3; prints TEST_PASS if accuracy >= 0.9.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=3, help=\\\"Number of neighbors for KNN (default: 3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # load iris dataset (no download required)\\n    try:\\n        data = load_iris()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # split into train/test\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train k-NN classifier\\n    knn = KNeighborsClassifier(n_neighbors=args.k)\\n    knn.fit(X_train, y_train)\\n\\n    # evaluate on test set\\n    accuracy = knn.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {accuracy:.3f}\\\")\\n\\n    # acceptance check\\n    if accuracy >= 0.9:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} < 0.9\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Logistic Regression\",\n",
            "    \"description\": \"Use sklearn wine dataset, scale features, train logistic regression, and report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Wine dataset logistic regression with feature scaling; TEST_PASS if accuracy >= 0.92.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\\n    clf.fit(X_train_scaled, y_train)\\n\\n    accuracy = clf.score(X_test_scaled, y_test)\\n    print(f\\\"Test accuracy: {accuracy:.4f}\\\")\\n\\n    if accuracy >= 0.92:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.4f} < 0.92\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SVM\",\n",
            "    \"description\": \"Train a linear SVM on sklearn breast_cancer dataset, report test accuracy. Print TEST_PASS if accuracy ≥ 0.93.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train linear SVM on breast_cancer dataset; report test accuracy.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--max-iter\\\", type=int, default=1000, help=\\\"Max iterations for LinearSVC (default: 1000).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"svm\\\", LinearSVC(max_iter=args.max_iter, random_state=args.seed))\\n    ])\\n\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    print(f\\\"test_accuracy={acc:.4f}\\\")\\n\\n    if acc >= 0.93:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.93\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Linear Classifier\",\n",
            "    \"description\": \"Classify sklearn digits using LogisticRegression, report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Classify sklearn digits using LogisticRegression; TEST_PASS if accuracy >= 0.92.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--max-iter\\\", type=int, default=1000, help=\\\"Max iterations for LogisticRegression (default: 1000).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"lr\\\", LogisticRegression(max_iter=args.max_iter, random_state=args.seed))\\n    ])\\n\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    print(f\\\"Test accuracy: {acc:.4f}\\\")\\n\\n    if acc >= 0.92:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.92\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Ridge Regression\",\n",
            "    \"description\": \"Fit Ridge regression on sklearn diabetes dataset, compute R² score. Print TEST_PASS if R² ≥ 0.4.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.metrics import r2_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Ridge regression on sklearn diabetes dataset; TEST_PASS if R² ≥ 0.4.\\\")\\n    p.add_argument(\\\"--alpha\\\", type=float, default=1.0, help=\\\"Ridge regularization strength (default: 1.0).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: diabetes dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    model = Ridge(alpha=args.alpha, random_state=args.seed)\\n    model.fit(X_train, y_train)\\n\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"R² score: {r2:.4f}\\\")\\n\\n    if r2 >= 0.4:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R² score {r2:.4f} below threshold 0.4\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons Decision Tree\",\n",
            "    \"description\": \"Generate make_moons data (n=300), train decision tree, plot decision boundary, save as 'moons_tree.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_moons data, train decision tree, plot decision boundary, save as moons_tree.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.2, help=\\\"Noise level for make_moons (default: 0.2).\\\")\\n    p.add_argument(\\\"--max-depth\\\", type=int, default=5, help=\\\"Max depth of decision tree (default: 5).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"moons_tree.png\\\", help=\\\"Output filename for decision boundary plot (default: moons_tree.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Generate make_moons data\\n    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # Train decision tree\\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\\n    clf.fit(X, y)\\n\\n    # Create decision boundary plot\\n    h = 0.02\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    \\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap=plt.cm.RdYlBu)\\n    plt.xlabel(\\\"Feature 1\\\")\\n    plt.ylabel(\\\"Feature 2\\\")\\n    plt.title(\\\"Decision Tree Boundary on make_moons\\\")\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    # Acceptance check\\n    import os\\n    if not os.path.exists(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n    \\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    acc = clf.score(X, y)\\n    print(f\\\"Training accuracy: {acc:.3f}\\\")\\n    print(f\\\"Decision boundary saved to {args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles Random Forest\",\n",
            "    \"description\": \"Generate make_circles data (n=300), train random forest, report accuracy. Print TEST_PASS if accuracy ≥ 0.88.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_circles data (n=300), train random forest, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=100, help=\\\"Number of trees in random forest (default: 100).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_circles(n_samples=args.n_samples, noise=0.1, factor=0.5, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\\n    \\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\\n    clf.fit(Xtr, ytr)\\n    \\n    acc = clf.score(Xte, yte)\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n    \\n    if acc >= 0.88:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold 0.88\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Blobs KMeans Clustering\",\n",
            "    \"description\": \"Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score. Print TEST_PASS if silhouette ≥ 0.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import silhouette_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of samples (default: 200).\\\")\\n    p.add_argument(\\\"--centers\\\", type=int, default=4, help=\\\"Number of cluster centers (default: 4).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples <= 0:\\n        print(\\\"TEST_FAIL: n_samples must be positive\\\")\\n        sys.exit(1)\\n    if args.centers <= 0:\\n        print(\\\"TEST_FAIL: centers must be positive\\\")\\n        sys.exit(1)\\n\\n    X, y_true = make_blobs(n_samples=args.n_samples, centers=args.centers, random_state=args.seed)\\n    \\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    kmeans = KMeans(n_clusters=args.centers, random_state=args.seed, n_init=10)\\n    y_pred = kmeans.fit_predict(X)\\n\\n    sil = silhouette_score(X, y_pred)\\n    print(f\\\"n_samples={args.n_samples} centers={args.centers} silhouette={sil:.3f}\\\")\\n\\n    if sil >= 0.5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: silhouette score {sil:.3f} below threshold 0.5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Classification Synthetic Data\",\n",
            "    \"description\": \"Generate make_classification data (n=500, n_features=4), train an SGDClassifier inside a StandardScaler pipeline, report accuracy. Print TEST_PASS if accuracy ≥ 0.85.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=500, help=\\\"Number of samples (default: 500).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=4, help=\\\"Number of features (default: 4).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_classification(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_informative=max(2, args.n_features // 2),\\n        n_redundant=0,\\n        n_clusters_per_class=1,\\n        random_state=args.seed\\n    )\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"sgd\\\", SGDClassifier(max_iter=1000, tol=1e-3, random_state=args.seed))\\n    ])\\n    clf.fit(X_train, y_train)\\n\\n    y_pred = clf.predict(X_test)\\n    acc = accuracy_score(y_test, y_pred)\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.85:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.85\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Regression Synthetic Data\",\n",
            "    \"description\": \"Generate make_regression data (n=300), fit LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate synthetic regression data (n=300), fit LinearRegression, compute R2; TEST_PASS if R2 >= 0.3.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=10, help=\\\"Number of features (default: 10).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=10.0, help=\\\"Standard deviation of Gaussian noise (default: 10.0).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Generate synthetic regression data\\n    X, y = make_regression(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        noise=args.noise,\\n        random_state=args.seed\\n    )\\n\\n    # Validate dataset\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    # Fit LinearRegression\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    # Predict and compute R²\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"n_samples={args.n_samples} n_features={args.n_features} noise={args.noise}\\\")\\n    print(f\\\"R2={r2:.4f}\\\")\\n\\n    # Acceptance check\\n    if r2 >= 0.3:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R2={r2:.4f} below threshold 0.3\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"PCA on Digits\",\n",
            "    \"description\": \"Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as 'digits_pca.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.decomposition import PCA\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as digits_pca.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"digits_pca.png\\\", help=\\\"Output PNG file path (default: digits_pca.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X = data.data\\n        y = data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    pca = PCA(n_components=2, random_state=args.seed)\\n    X_pca = pca.fit_transform(X)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\\n    plt.colorbar(scatter, label='Digit')\\n    plt.xlabel('First Principal Component')\\n    plt.ylabel('Second Principal Component')\\n    plt.title('PCA on Digits Dataset')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Saved PCA scatter plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Pairplot Visualization\",\n",
            "    \"description\": \"Plot pairwise feature scatter plots for iris dataset using matplotlib, save as 'iris_pairplot.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_iris_or_synthetic(seed=42):\\n    try:\\n        from sklearn.datasets import load_iris\\n        data = load_iris()\\n        X = data.data\\n        y = data.target\\n        feature_names = data.feature_names\\n        used = \\\"iris\\\"\\n    except Exception:\\n        rng = np.random.default_rng(seed)\\n        n = 150\\n        c = rng.integers(0, 3, size=n)\\n        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\\n        y = c\\n        feature_names = [\\\"sepal_length\\\", \\\"sepal_width\\\", \\\"petal_length\\\", \\\"petal_width\\\"]\\n        used = \\\"synthetic\\\"\\n    return X, y, feature_names, used\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Plot pairwise feature scatter plots for iris dataset; save as iris_pairplot.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"iris_pairplot.png\\\", help=\\\"Output filename (default: iris_pairplot.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y, feature_names, used = load_iris_or_synthetic(args.seed)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    n_features = X.shape[1]\\n    fig, axes = plt.subplots(n_features, n_features, figsize=(12, 12))\\n    \\n    colors = ['red', 'green', 'blue']\\n    for i in range(n_features):\\n        for j in range(n_features):\\n            ax = axes[i, j]\\n            if i == j:\\n                for cls in np.unique(y):\\n                    ax.hist(X[y == cls, i], bins=15, alpha=0.6, color=colors[cls % len(colors)], label=f\\\"Class {cls}\\\")\\n                ax.set_ylabel(\\\"Frequency\\\")\\n            else:\\n                for cls in np.unique(y):\\n                    ax.scatter(X[y == cls, j], X[y == cls, i], alpha=0.6, s=10, color=colors[cls % len(colors)], label=f\\\"Class {cls}\\\")\\n            \\n            if i == n_features - 1:\\n                ax.set_xlabel(feature_names[j])\\n            else:\\n                ax.set_xticklabels([])\\n            \\n            if j == 0:\\n                ax.set_ylabel(feature_names[i])\\n            else:\\n                ax.set_yticklabels([])\\n            \\n            if i == 0 and j == n_features - 1:\\n                ax.legend(loc='upper right', fontsize=8)\\n\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    print(f\\\"dataset={used} saved={args.output}\\\")\\n\\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file missing or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Feature Importance\",\n",
            "    \"description\": \"Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart as 'wine_importance.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"wine_importance.png\\\", help=\\\"Output filename for feature importance bar chart (default: wine_importance.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=100, help=\\\"Number of trees in RandomForest (default: 100).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X, y = data.data, data.target\\n        feature_names = data.feature_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: could not load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    importances = clf.feature_importances_\\n    if importances is None or len(importances) == 0:\\n        print(\\\"TEST_FAIL: feature importances are empty\\\")\\n        sys.exit(1)\\n\\n    indices = np.argsort(importances)[::-1]\\n\\n    plt.figure(figsize=(10, 6))\\n    plt.bar(range(len(importances)), importances[indices], align='center')\\n    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')\\n    plt.xlabel('Feature')\\n    plt.ylabel('Importance')\\n    plt.title('Wine Dataset Feature Importances')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    if os.path.getsize(args.output) == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"accuracy={acc:.3f} output={args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Confusion Matrix\",\n",
            "    \"description\": \"Train LogisticRegression on breast_cancer dataset, plot confusion matrix, save as 'bc_confusion.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train LogisticRegression on breast_cancer, plot confusion matrix, save as bc_confusion.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"bc_confusion.png\\\", help=\\\"Output confusion matrix image path (default: bc_confusion.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = LogisticRegression(max_iter=5000, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n    y_pred = clf.predict(X_test)\\n\\n    cm = confusion_matrix(y_test, y_pred)\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\\n    \\n    fig, ax = plt.subplots(figsize=(8, 6))\\n    disp.plot(ax=ax, cmap='Blues')\\n    plt.title('Breast Cancer Confusion Matrix')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    if os.path.getsize(args.output) == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"Confusion matrix saved to {args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Confusion Heatmap\",\n",
            "    \"description\": \"Train SVC on sklearn digits, compute confusion matrix, plot it with matplotlib (no seaborn), save as digits_heatmap.png. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nfrom sklearn.metrics import confusion_matrix\\nimport os\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train SVC on digits dataset, plot confusion matrix heatmap, save as digits_heatmap.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"digits_heatmap.png\\\", help=\\\"Output heatmap filename (default: digits_heatmap.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.3, help=\\\"Test set fraction (default: 0.3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = SVC(kernel='rbf', gamma='scale', random_state=args.seed)\\n    clf.fit(Xtr, ytr)\\n    ypred = clf.predict(Xte)\\n\\n    cm = confusion_matrix(yte, ypred)\\n\\n    fig, ax = plt.subplots(figsize=(8, 6))\\n    im = ax.imshow(cm, cmap='Blues')\\n    ax.set_xlabel('Predicted')\\n    ax.set_ylabel('Actual')\\n    ax.set_title('Digits Confusion Matrix')\\n    plt.colorbar(im, ax=ax)\\n    for i in range(cm.shape[0]):\\n        for j in range(cm.shape[1]):\\n            ax.text(j, i, cm[i, j], ha='center', va='center', color='black')\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"Confusion matrix heatmap saved to {args.output}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Residual Plot\",\n",
            "    \"description\": \"Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as 'residuals_plot.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error\\nimport os\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as residuals_plot.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"residuals_plot.png\\\", help=\\\"Output filename for residual plot (default: residuals_plot.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: diabetes dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    y_pred = model.predict(X_test)\\n    residuals = y_test - y_pred\\n    mse = mean_squared_error(y_test, y_pred)\\n\\n    print(f\\\"Test MSE: {mse:.2f}\\\")\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.scatter(y_pred, residuals, alpha=0.6, edgecolors='k')\\n    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\\n    plt.xlabel('Predicted Values')\\n    plt.ylabel('Residuals')\\n    plt.title('Residual Plot: Diabetes Dataset')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n\\n    try:\\n        plt.savefig(args.output, dpi=100)\\n        print(f\\\"Residual plot saved to {args.output}\\\")\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Audio Classification\",\n",
            "    \"description\": \"Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_audio(n_samples=250, n_features=20, seed=42):\\n    rng = np.random.default_rng(seed)\\n    X = []\\n    y = []\\n    for i in range(n_samples):\\n        if i < n_samples // 2:\\n            # high pitch: higher frequency components\\n            freq_base = rng.uniform(800, 1200)\\n            label = 1\\n        else:\\n            # low pitch: lower frequency components\\n            freq_base = rng.uniform(100, 400)\\n            label = 0\\n        # simulate audio features (e.g., spectral coefficients)\\n        features = rng.normal(freq_base, 50, size=n_features)\\n        X.append(features)\\n        y.append(label)\\n    return np.array(X), np.array(y)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=250, help=\\\"Number of synthetic audio samples (default: 250).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=20, help=\\\"Number of features per sample (default: 20).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # generate synthetic audio dataset\\n    X, y = generate_synthetic_audio(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train MLP\\n    clf = MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=300, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    # predict and evaluate\\n    y_pred = clf.predict(X_test)\\n    acc = accuracy_score(y_test, y_pred)\\n\\n    print(f\\\"dataset=synthetic_audio n_samples={args.n_samples} accuracy={acc:.3f}\\\")\\n\\n    # acceptance: accuracy >= 0.7\\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Time Series Forecast\",\n",
            "    \"description\": \"Generate 200 synthetic time series samples, predict next value using lag features, compute R². Print TEST_PASS if R² ≥ 0.25.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\n\\ndef generate_synthetic_time_series(n_samples=200, seed=42):\\n    \\\"\\\"\\\"Generate synthetic time series with trend, seasonality, and noise.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    t = np.arange(n_samples)\\n    trend = 0.05 * t\\n    seasonality = 10 * np.sin(2 * np.pi * t / 20)\\n    noise = rng.normal(0, 2, size=n_samples)\\n    series = trend + seasonality + noise\\n    return series\\n\\ndef create_lag_features(series, n_lags=3):\\n    \\\"\\\"\\\"Create lag features for time series prediction.\\\"\\\"\\\"\\n    X = []\\n    y = []\\n    for i in range(n_lags, len(series)):\\n        X.append(series[i-n_lags:i])\\n        y.append(series[i])\\n    return np.array(X), np.array(y)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 200 synthetic time series samples, predict next value using lag features, compute R².\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of time series samples (default: 200).\\\")\\n    p.add_argument(\\\"--n-lags\\\", type=int, default=3, help=\\\"Number of lag features (default: 3).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples < 50:\\n        print(\\\"TEST_FAIL: n_samples must be at least 50\\\")\\n        sys.exit(1)\\n    if args.n_lags < 1:\\n        print(\\\"TEST_FAIL: n_lags must be at least 1\\\")\\n        sys.exit(1)\\n    if not (0 < args.test_size < 1):\\n        print(\\\"TEST_FAIL: test_size must be between 0 and 1\\\")\\n        sys.exit(1)\\n\\n    series = generate_synthetic_time_series(n_samples=args.n_samples, seed=args.seed)\\n    X, y = create_lag_features(series, n_lags=args.n_lags)\\n\\n    if len(X) == 0 or len(y) == 0:\\n        print(\\\"TEST_FAIL: insufficient data after creating lag features\\\")\\n        sys.exit(1)\\n\\n    split_idx = int(len(X) * (1 - args.test_size))\\n    if split_idx < 1 or split_idx >= len(X):\\n        print(\\\"TEST_FAIL: invalid train/test split\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test = X[:split_idx], X[split_idx:]\\n    y_train, y_test = y[:split_idx], y[split_idx:]\\n\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"n_samples={args.n_samples} n_lags={args.n_lags} r2={r2:.4f}\\\")\\n\\n    if r2 >= 0.25:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R² {r2:.4f} below threshold 0.25\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Tabular Regression\",\n",
            "    \"description\": \"Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score\\n\\ndef generate_synthetic_tabular(n_samples=300, n_features=5, seed=42):\\n    rng = np.random.default_rng(seed)\\n    X = rng.normal(0, 1, size=(n_samples, n_features))\\n    true_coef = rng.uniform(-2, 2, size=n_features)\\n    y = X @ true_coef + rng.normal(0, 0.5, size=n_samples)\\n    return X, y\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R².\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of synthetic samples (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=5, help=\\\"Number of features (default: 5).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = generate_synthetic_tabular(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n    \\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\\n    \\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n    \\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n    \\n    print(f\\\"n_samples={args.n_samples} n_features={args.n_features} R²={r2:.3f}\\\")\\n    \\n    if r2 >= 0.3:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R²={r2:.3f} below threshold 0.3\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Text Sentiment\",\n",
            "    \"description\": \"Create 200 short synthetic sentences labeled positive or negative, vectorize with CountVectorizer, train a LogisticRegression, and print accuracy; print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n\\ndef generate_synthetic_sentences(n=200, seed=42):\\n    rng = np.random.default_rng(seed)\\n    positive_templates = [\\n        \\\"I love this product\\\",\\n        \\\"This is amazing\\\",\\n        \\\"Great experience\\\",\\n        \\\"Wonderful service\\\",\\n        \\\"Highly recommend\\\",\\n        \\\"Excellent quality\\\",\\n        \\\"Very satisfied\\\",\\n        \\\"Best purchase ever\\\",\\n        \\\"Fantastic results\\\",\\n        \\\"Really enjoyed it\\\"\\n    ]\\n    negative_templates = [\\n        \\\"I hate this product\\\",\\n        \\\"This is terrible\\\",\\n        \\\"Bad experience\\\",\\n        \\\"Poor service\\\",\\n        \\\"Do not recommend\\\",\\n        \\\"Awful quality\\\",\\n        \\\"Very disappointed\\\",\\n        \\\"Worst purchase ever\\\",\\n        \\\"Horrible results\\\",\\n        \\\"Really disliked it\\\"\\n    ]\\n    sentences = []\\n    labels = []\\n    for i in range(n):\\n        if i % 2 == 0:\\n            template = positive_templates[rng.integers(0, len(positive_templates))]\\n            label = 1\\n        else:\\n            template = negative_templates[rng.integers(0, len(negative_templates))]\\n            label = 0\\n        sentences.append(template)\\n        labels.append(label)\\n    return sentences, np.array(labels)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 200 synthetic sentiment sentences, vectorize with CountVectorizer, train LogisticRegression, print TEST_PASS if accuracy >= 0.7.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of synthetic sentences (default: 200).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    sentences, labels = generate_synthetic_sentences(n=args.n_samples, seed=args.seed)\\n    if len(sentences) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: no sentences generated\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        sentences, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\\n    )\\n\\n    vectorizer = CountVectorizer()\\n    X_train_vec = vectorizer.fit_transform(X_train)\\n    X_test_vec = vectorizer.transform(X_test)\\n\\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\\n    clf.fit(X_train_vec, y_train)\\n\\n    acc = clf.score(X_test_vec, y_test)\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST CNN Classifier\",\n",
            "    \"description\": \"If MNIST available, train simple CNN (2 conv layers), else generate FakeData (32x32 grayscale), train CNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.85 or fallback ≥ 0.6.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_mnist_or_fakedata(seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([transforms.ToTensor()])\\n        train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.MNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n        return train, test, True\\n    except Exception:\\n        import torch\\n        from torchvision.datasets import FakeData\\n        from torchvision import transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([transforms.ToTensor()])\\n        train = FakeData(size=1000, image_size=(1, 32, 32), num_classes=10, transform=tfm)\\n        test = FakeData(size=200, image_size=(1, 32, 32), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n\\n    p = argparse.ArgumentParser(description=\\\"MNIST CNN classifier with FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=2, help=\\\"Training epochs (default: 2).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    class SimpleCNN(nn.Module):\\n        def __init__(self, input_size=28):\\n            super().__init__()\\n            self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\\n            self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\\n            self.pool = nn.MaxPool2d(2, 2)\\n            self.relu = nn.ReLU()\\n            conv_out_size = (input_size // 4) * (input_size // 4) * 32\\n            self.fc1 = nn.Linear(conv_out_size, 64)\\n            self.fc2 = nn.Linear(64, 10)\\n\\n        def forward(self, x):\\n            x = self.pool(self.relu(self.conv1(x)))\\n            x = self.pool(self.relu(self.conv2(x)))\\n            x = x.view(x.size(0), -1)\\n            x = self.relu(self.fc1(x))\\n            x = self.fc2(x)\\n            return x\\n\\n    input_size = 28 if real else 32\\n    model = SimpleCNN(input_size=input_size)\\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\\n    criterion = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            optimizer.zero_grad()\\n            outputs = model(xb)\\n            loss = criterion(outputs, yb)\\n            loss.backward()\\n            optimizer.step()\\n\\n    model.eval()\\n    correct = 0\\n    total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            outputs = model(xb)\\n            _, predicted = torch.max(outputs.data, 1)\\n            total += yb.size(0)\\n            correct += (predicted == yb).sum().item()\\n\\n    accuracy = correct / max(total, 1)\\n    dataset_name = \\\"MNIST\\\" if real else \\\"FakeData\\\"\\n    print(f\\\"dataset={dataset_name} accuracy={accuracy:.3f}\\\")\\n\\n    threshold = 0.85 if real else 0.6\\n    if accuracy >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} below threshold {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST KNN\",\n",
            "    \"description\": \"If FashionMNIST available, flatten images, train KNN (k=5), else use FakeData, train KNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.7 or fallback ≥ 0.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_fashionmnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.FashionMNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.FashionMNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"FashionMNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\\n        return train, test, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        test = FakeData(size=max_test, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"FashionMNIST KNN (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=5, help=\\\"Number of neighbors for KNN (default: 5).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit FashionMNIST download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    train_ds, test_ds, real = load_fashionmnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n\\n    X_train = []\\n    y_train = []\\n    for img, label in train_ds:\\n        flat = img.numpy().flatten()\\n        X_train.append(flat)\\n        y_train.append(label)\\n    X_train = np.array(X_train)\\n    y_train = np.array(y_train)\\n\\n    X_test = []\\n    y_test = []\\n    for img, label in test_ds:\\n        flat = img.numpy().flatten()\\n        X_test.append(flat)\\n        y_test.append(label)\\n    X_test = np.array(X_test)\\n    y_test = np.array(y_test)\\n\\n    if len(X_train) == 0 or len(X_test) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    from sklearn.neighbors import KNeighborsClassifier\\n    knn = KNeighborsClassifier(n_neighbors=args.k)\\n    knn.fit(X_train, y_train)\\n    acc = knn.score(X_test, y_test)\\n\\n    print(f\\\"dataset={'fashionmnist' if real else 'fake'} k={args.k} accuracy={acc:.3f}\\\")\\n\\n    threshold = 0.7 if real else 0.5\\n    if acc >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"CIFAR10 Class Balance\",\n",
            "    \"description\": \"If CIFAR10 available, count class frequencies, else generate FakeData with 10 classes, count, save bar chart as 'class_balance.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_cifar10_or_fakedata(seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets\\n        torch.manual_seed(seed)\\n        ds = datasets.CIFAR10(root=\\\"./data\\\", train=True, download=bool(allow_download))\\n        if len(ds) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        labels = [y for _, y in ds]\\n        return labels, True\\n    except Exception:\\n        import torch\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        ds = FakeData(size=1000, image_size=(3, 32, 32), num_classes=10)\\n        labels = [y for _, y in ds]\\n        return labels, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Count CIFAR10 class frequencies or FakeData fallback; save bar chart; seeds in main.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"class_balance.png\\\", help=\\\"Output bar chart filename (default: class_balance.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    labels, real = load_cifar10_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    if labels is None or len(labels) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    counts = np.bincount(labels, minlength=10)\\n    print(f\\\"dataset={'cifar10' if real else 'fake'} class_counts={counts.tolist()}\\\")\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.bar(range(10), counts, color='steelblue')\\n    plt.xlabel(\\\"Class\\\")\\n    plt.ylabel(\\\"Frequency\\\")\\n    plt.title(\\\"Class Balance\\\")\\n    plt.xticks(range(10))\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"ImageFolder Histogram\",\n",
            "    \"description\": \"Try to load an ImageFolder and compute average RGB histogram, else fall back to FakeData. Save plot to histogram.png. Print TEST_PASS if file exists and is non-empty.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_imagefolder_or_fakedata(root_dir, num_fake=50, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        if os.path.isdir(root_dir):\\n            tfm = transforms.ToTensor()\\n            ds = datasets.ImageFolder(root=root_dir, transform=tfm)\\n            if len(ds) > 0:\\n                return ds, True\\n        raise RuntimeError(\\\"ImageFolder not available or empty\\\")\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        ds = FakeData(size=num_fake, image_size=(3, 64, 64), num_classes=2, transform=tfm)\\n        return ds, False\\n\\ndef compute_rgb_histogram(dataset, num_samples=None):\\n    if num_samples is None:\\n        num_samples = len(dataset)\\n    else:\\n        num_samples = min(num_samples, len(dataset))\\n\\n    r_sum = np.zeros(256, dtype=np.float64)\\n    g_sum = np.zeros(256, dtype=np.float64)\\n    b_sum = np.zeros(256, dtype=np.float64)\\n\\n    for i in range(num_samples):\\n        img_tensor, _ = dataset[i]\\n        img_np = (img_tensor.numpy() * 255).astype(np.uint8)\\n        r_hist, _ = np.histogram(img_np[0].flatten(), bins=256, range=(0, 256))\\n        g_hist, _ = np.histogram(img_np[1].flatten(), bins=256, range=(0, 256))\\n        b_hist, _ = np.histogram(img_np[2].flatten(), bins=256, range=(0, 256))\\n        r_sum += r_hist\\n        g_sum += g_hist\\n        b_sum += b_hist\\n\\n    r_avg = r_sum / num_samples\\n    g_avg = g_sum / num_samples\\n    b_avg = b_sum / num_samples\\n    return r_avg, g_avg, b_avg\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Compute average RGB histogram from ImageFolder or FakeData and save as histogram.png.\\\")\\n    p.add_argument(\\\"--root-dir\\\", type=str, default=\\\"./imagefolder_data\\\", help=\\\"Path to ImageFolder root directory (default: ./imagefolder_data).\\\")\\n    p.add_argument(\\\"--num-fake\\\", type=int, default=50, help=\\\"Number of FakeData images if ImageFolder unavailable (default: 50).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"histogram.png\\\", help=\\\"Output histogram file path (default: histogram.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit dataset download if needed.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    dataset, is_real = load_imagefolder_or_fakedata(\\n        root_dir=args.root_dir,\\n        num_fake=args.num_fake,\\n        seed=args.seed,\\n        allow_download=args.allow_download\\n    )\\n\\n    if dataset is None or len(dataset) == 0:\\n        print(\\\"TEST_FAIL: dataset not available or empty\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"Using {'ImageFolder' if is_real else 'FakeData'} with {len(dataset)} images\\\")\\n\\n    r_hist, g_hist, b_hist = compute_rgb_histogram(dataset)\\n\\n    fig, ax = plt.subplots(figsize=(10, 6))\\n    bins = np.arange(256)\\n    ax.plot(bins, r_hist, color='red', alpha=0.7, label='Red')\\n    ax.plot(bins, g_hist, color='green', alpha=0.7, label='Green')\\n    ax.plot(bins, b_hist, color='blue', alpha=0.7, label='Blue')\\n    ax.set_xlabel('Pixel Value')\\n    ax.set_ylabel('Average Frequency')\\n    ax.set_title('Average RGB Histogram')\\n    ax.legend()\\n    ax.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    if os.path.isfile(args.output):\\n        file_size = os.path.getsize(args.output)\\n        if file_size > 0:\\n            print(f\\\"Histogram saved to {args.output} ({file_size} bytes)\\\")\\n            print(\\\"TEST_PASS\\\")\\n        else:\\n            print(\\\"TEST_FAIL: output file is empty\\\")\\n            sys.exit(1)\\n    else:\\n        print(\\\"TEST_FAIL: output file does not exist\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FakeData GAN Discriminator\",\n",
            "    \"description\": \"Generate FakeData images, train simple CNN discriminator, report accuracy distinguishing real/fake. Print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train CNN discriminator on FakeData (real vs fake labels); print TEST_PASS if accuracy >= 0.7.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=64, help=\\\"Batch size (default: 64).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-real\\\", type=int, default=500, help=\\\"Number of real samples (default: 500).\\\")\\n    p.add_argument(\\\"--n-fake\\\", type=int, default=500, help=\\\"Number of fake samples (default: 500).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader, TensorDataset\\n    from torchvision.datasets import FakeData\\n    from torchvision import transforms\\n\\n    tfm = transforms.ToTensor()\\n    real_ds = FakeData(size=args.n_real, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n    fake_ds = FakeData(size=args.n_fake, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n\\n    real_images = []\\n    fake_images = []\\n    for i in range(args.n_real):\\n        img, _ = real_ds[i]\\n        real_images.append(img)\\n    for i in range(args.n_fake):\\n        img, _ = fake_ds[i]\\n        fake_images.append(img)\\n\\n    real_images = torch.stack(real_images)\\n    fake_images = torch.stack(fake_images)\\n    real_labels = torch.ones(args.n_real, dtype=torch.long)\\n    fake_labels = torch.zeros(args.n_fake, dtype=torch.long)\\n\\n    X = torch.cat([real_images, fake_images], dim=0)\\n    y = torch.cat([real_labels, fake_labels], dim=0)\\n\\n    perm = torch.randperm(len(X))\\n    X = X[perm]\\n    y = y[perm]\\n\\n    split = int(0.8 * len(X))\\n    X_train, X_test = X[:split], X[split:]\\n    y_train, y_test = y[:split], y[split:]\\n\\n    train_ds = TensorDataset(X_train, y_train)\\n    test_ds = TensorDataset(X_test, y_test)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    class Discriminator(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.net = nn.Sequential(\\n                nn.Conv2d(1, 16, 3, 1, 1), nn.ReLU(),\\n                nn.MaxPool2d(2),\\n                nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(),\\n                nn.MaxPool2d(2),\\n                nn.Flatten(),\\n                nn.Linear(32 * 7 * 7, 64), nn.ReLU(),\\n                nn.Linear(64, 2)\\n            )\\n        def forward(self, x):\\n            return self.net(x)\\n\\n    model = Discriminator()\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n    loss_fn = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            opt.zero_grad()\\n            logits = model(xb)\\n            loss = loss_fn(logits, yb)\\n            loss.backward()\\n            opt.step()\\n\\n    model.eval()\\n    correct = 0\\n    total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            pred = model(xb).argmax(1)\\n            correct += (pred == yb).sum().item()\\n            total += yb.numel()\\n\\n    acc = correct / max(total, 1)\\n    print(f\\\"discriminator_accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} < 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"KMeans on CIFAR Colors\",\n",
            "    \"description\": \"Load CIFAR10 pixels (or FakeData fallback), run KMeans(k=5) on RGB pixels, and print inertia. Print TEST_PASS if inertia < 2.0e5. Uses fewer images to reduce runtime.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_cifar_or_fakedata(max_samples=500, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.CIFAR10(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        subset = torch.utils.data.Subset(train, list(range(min(len(train), max_samples))))\\n        loader = torch.utils.data.DataLoader(subset, batch_size=max_samples, shuffle=False)\\n        for xb, _ in loader:\\n            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\\n            return pixels, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        fake = FakeData(size=max_samples, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        loader = torch.utils.data.DataLoader(fake, batch_size=max_samples, shuffle=False)\\n        for xb, _ in loader:\\n            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\\n            return pixels, False\\n    return None, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"KMeans on CIFAR10 RGB pixels (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=5, help=\\\"Number of clusters (default: 5).\\\")\\n    p.add_argument(\\\"--max-samples\\\", type=int, default=500, help=\\\"Max images to load (default: 500).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    pixels, real = load_cifar_or_fakedata(max_samples=args.max_samples, seed=args.seed, allow_download=args.allow_download)\\n    if pixels is None or len(pixels) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    from sklearn.cluster import KMeans\\n    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\\n    kmeans.fit(pixels)\\n    inertia = kmeans.inertia_\\n\\n    print(f\\\"dataset={'cifar10' if real else 'fake'} k={args.k} inertia={inertia:.2e}\\\")\\n\\n    if inertia < 2.0e5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: inertia >= 2.0e5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Decision Boundary\",\n",
            "    \"description\": \"Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as 'iris_boundary.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as iris_boundary.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"iris_boundary.png\\\", help=\\\"Output image path (default: iris_boundary.png).\\\")\\n    p.add_argument(\\\"--max-depth\\\", type=int, default=3, help=\\\"Max depth of decision tree (default: 3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_iris()\\n        X = data.data[:, :2]\\n        y = data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: could not load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\\n    clf.fit(X, y)\\n\\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\n    h = 0.02\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolor='k', cmap='viridis')\\n    plt.xlabel(data.feature_names[0])\\n    plt.ylabel(data.feature_names[1])\\n    plt.title(\\\"Iris Decision Boundary (first two features)\\\")\\n    plt.colorbar(scatter)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Decision boundary saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine PCA Scatter\",\n",
            "    \"description\": \"Apply PCA to wine dataset (2 components), scatter plot colored by target, save as 'wine_pca.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply PCA to wine dataset (2 components), scatter plot colored by target, save as wine_pca.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"wine_pca.png\\\", help=\\\"Output filename (default: wine_pca.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X = data.data\\n        y = data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    pca = PCA(n_components=2, random_state=args.seed)\\n    X_pca = pca.fit_transform(X_scaled)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, edgecolors='k')\\n    plt.colorbar(scatter, label='Wine Class')\\n    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\\n    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\\n    plt.title('Wine Dataset PCA (2 Components)')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(f\\\"Saved PCA scatter plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist or is empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer ROC Curve\",\n",
            "    \"description\": \"Train LogisticRegression on breast_cancer, compute ROC curve, save as 'roc_curve.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import roc_curve, auc\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train LogisticRegression on breast_cancer, compute ROC curve, save as roc_curve.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"roc_curve.png\\\", help=\\\"Output ROC curve image path (default: roc_curve.png).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.3, help=\\\"Test set fraction (default: 0.3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = LogisticRegression(max_iter=5000, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    y_scores = clf.predict_proba(X_test)[:, 1]\\n\\n    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\\n    roc_auc = auc(fpr, tpr)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\\n    plt.xlim([0.0, 1.0])\\n    plt.ylim([0.0, 1.05])\\n    plt.xlabel('False Positive Rate')\\n    plt.ylabel('True Positive Rate')\\n    plt.title('Receiver Operating Characteristic - Breast Cancer')\\n    plt.legend(loc='lower right')\\n    plt.grid(alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"ROC curve saved to {args.output}, AUC={roc_auc:.3f}\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits t-SNE Plot\",\n",
            "    \"description\": \"Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as 'tsne_digits.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.manifold import TSNE\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as tsne_digits.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"tsne_digits.png\\\", help=\\\"Output PNG file path (default: tsne_digits.png).\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=500, help=\\\"Number of samples to use (default: 500).\\\")\\n    p.add_argument(\\\"--perplexity\\\", type=float, default=30.0, help=\\\"t-SNE perplexity (default: 30.0).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    n = min(args.n_samples, len(X))\\n    indices = np.random.choice(len(X), size=n, replace=False)\\n    X_sub = X[indices]\\n    y_sub = y[indices]\\n\\n    try:\\n        tsne = TSNE(n_components=2, perplexity=args.perplexity, random_state=args.seed)\\n        X_embedded = tsne.fit_transform(X_sub)\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: t-SNE failed: {e}\\\")\\n        sys.exit(1)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_sub, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\\n    plt.colorbar(scatter, label='Digit')\\n    plt.title('t-SNE of Digits Dataset')\\n    plt.xlabel('t-SNE Component 1')\\n    plt.ylabel('t-SNE Component 2')\\n    plt.tight_layout()\\n\\n    try:\\n        plt.savefig(args.output, dpi=100)\\n        plt.close()\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Saved t-SNE plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file does not exist\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Cross Validation\",\n",
            "    \"description\": \"Run 5-fold CV on diabetes dataset with Ridge regression, compute mean R². Print TEST_PASS if mean R² ≥ 0.35.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.model_selection import cross_val_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Run 5-fold CV on diabetes dataset with Ridge regression; print TEST_PASS if mean R² ≥ 0.35.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--alpha\\\", type=float, default=1.0, help=\\\"Ridge regularization strength (default: 1.0).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: diabetes dataset is empty\\\")\\n        sys.exit(1)\\n\\n    model = Ridge(alpha=args.alpha, random_state=args.seed)\\n    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\\n    mean_r2 = scores.mean()\\n\\n    print(f\\\"5-fold CV R² scores: {scores}\\\")\\n    print(f\\\"Mean R²: {mean_r2:.4f}\\\")\\n\\n    if mean_r2 >= 0.35:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: mean R² {mean_r2:.4f} < 0.35\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons SVM Margin\",\n",
            "    \"description\": \"Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as 'svm_margin.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.svm import SVC\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as svm_margin.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of samples (default: 200).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.2, help=\\\"Noise level (default: 0.2).\\\")\\n    p.add_argument(\\\"--C\\\", type=float, default=1.0, help=\\\"SVM regularization parameter (default: 1.0).\\\")\\n    p.add_argument(\\\"--gamma\\\", type=float, default=2.0, help=\\\"RBF kernel gamma (default: 2.0).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"svm_margin.png\\\", help=\\\"Output image path (default: svm_margin.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n    \\n    clf = SVC(kernel='rbf', C=args.C, gamma=args.gamma)\\n    clf.fit(X_scaled, y)\\n    \\n    x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\\n    y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\\n    h = 0.02\\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\\n    \\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\\n    Z = Z.reshape(xx.shape)\\n    \\n    plt.figure(figsize=(10, 8))\\n    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 50), cmap='RdBu', alpha=0.6)\\n    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\\n    plt.contour(xx, yy, Z, levels=[-1, 1], linewidths=1, colors='black', linestyles='dashed')\\n    \\n    plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c='red', edgecolors='k', marker='o', s=50, label='Class 0')\\n    plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c='blue', edgecolors='k', marker='s', s=50, label='Class 1')\\n    \\n    support_vectors = clf.support_vectors_\\n    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=200, linewidth=1, facecolors='none', edgecolors='green', label='Support Vectors')\\n    \\n    plt.xlabel('Feature 1')\\n    plt.ylabel('Feature 2')\\n    plt.title('SVM Decision Boundary and Margins (RBF Kernel)')\\n    plt.legend()\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    \\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(f\\\"Saved decision boundary plot to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles Neural Net\",\n",
            "    \"description\": \"Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.87.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy >= 0.87.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=1000, help=\\\"Number of samples to generate (default: 1000).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.1, help=\\\"Noise level for make_circles (default: 0.1).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_circles(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    clf = MLPClassifier(\\n        hidden_layer_sizes=(100, 50),\\n        max_iter=500,\\n        random_state=args.seed,\\n        early_stopping=True,\\n        validation_fraction=0.1\\n    )\\n    clf.fit(X_train_scaled, y_train)\\n\\n    accuracy = clf.score(X_test_scaled, y_test)\\n    print(f\\\"accuracy={accuracy:.3f}\\\")\\n\\n    if accuracy >= 0.87:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.87\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Blobs Elbow Method\",\n",
            "    \"description\": \"Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as 'elbow_plot.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as elbow_plot.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=2, help=\\\"Number of features (default: 2).\\\")\\n    p.add_argument(\\\"--centers\\\", type=int, default=4, help=\\\"Number of centers for make_blobs (default: 4).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"elbow_plot.png\\\", help=\\\"Output filename for elbow plot (default: elbow_plot.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_blobs(n_samples=args.n_samples, n_features=args.n_features, centers=args.centers, random_state=args.seed)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    inertias = []\\n    k_range = range(1, 9)\\n    for k in k_range:\\n        km = KMeans(n_clusters=k, random_state=args.seed, n_init=10)\\n        km.fit(X)\\n        inertias.append(km.inertia_)\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.plot(list(k_range), inertias, marker='o')\\n    plt.xlabel('Number of clusters (k)')\\n    plt.ylabel('Inertia')\\n    plt.title('Elbow Method for Optimal k')\\n    plt.grid(True)\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Elbow plot saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Classification Learning Curve\",\n",
            "    \"description\": \"Generate make_classification data, train LogisticRegression, plot learning curve, save as 'learning_curve.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import learning_curve\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate make_classification data, train LogisticRegression, plot learning curve, save as learning_curve.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=500, help=\\\"Number of samples (default: 500).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=20, help=\\\"Number of features (default: 20).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"learning_curve.png\\\", help=\\\"Output file path (default: learning_curve.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_classification(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_informative=max(2, args.n_features // 2),\\n        n_redundant=max(0, args.n_features // 4),\\n        random_state=args.seed\\n    )\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\\n\\n    train_sizes = np.linspace(0.1, 1.0, 10)\\n    train_sizes_abs, train_scores, val_scores = learning_curve(\\n        clf, X, y,\\n        train_sizes=train_sizes,\\n        cv=5,\\n        scoring='accuracy',\\n        random_state=args.seed,\\n        n_jobs=1\\n    )\\n\\n    train_mean = np.mean(train_scores, axis=1)\\n    train_std = np.std(train_scores, axis=1)\\n    val_mean = np.mean(val_scores, axis=1)\\n    val_std = np.std(val_scores, axis=1)\\n\\n    plt.figure(figsize=(8, 6))\\n    plt.plot(train_sizes_abs, train_mean, 'o-', label='Training score', linewidth=2)\\n    plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2)\\n    plt.plot(train_sizes_abs, val_mean, 'o-', label='Validation score', linewidth=2)\\n    plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2)\\n    plt.xlabel('Training Set Size')\\n    plt.ylabel('Accuracy')\\n    plt.title('Learning Curve (LogisticRegression)')\\n    plt.legend(loc='best')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(f\\\"Learning curve saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Regression Residual Analysis\",\n",
            "    \"description\": \"Generate make_regression data, train LinearRegression, plot residual histogram, save as 'resid_hist.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nimport os\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate regression data, train LinearRegression, plot residual histogram, save as resid_hist.png.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples (default: 300).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=5, help=\\\"Number of features (default: 5).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=10.0, help=\\\"Noise level (default: 10.0).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"resid_hist.png\\\", help=\\\"Output histogram filename (default: resid_hist.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_regression(n_samples=args.n_samples, n_features=args.n_features, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\\n    \\n    model = LinearRegression()\\n    model.fit(Xtr, ytr)\\n    \\n    y_pred = model.predict(Xte)\\n    residuals = yte - y_pred\\n    \\n    plt.figure(figsize=(8, 6))\\n    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\\n    plt.xlabel('Residuals')\\n    plt.ylabel('Frequency')\\n    plt.title('Residual Histogram')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    \\n    if not os.path.exists(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n    \\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n    \\n    print(f\\\"Residual histogram saved to {args.output} (size: {file_size} bytes)\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic NLP Spam Filter\",\n",
            "    \"description\": \"Generate 250 synthetic email texts labeled spam/ham, vectorize with TF-IDF, train Naive Bayes, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_emails(n=250, seed=42):\\n    rng = np.random.default_rng(seed)\\n    spam_words = [\\\"win\\\", \\\"free\\\", \\\"prize\\\", \\\"click\\\", \\\"offer\\\", \\\"buy\\\", \\\"discount\\\", \\\"limited\\\", \\\"urgent\\\", \\\"cash\\\", \\\"bonus\\\", \\\"guarantee\\\", \\\"credit\\\", \\\"loan\\\", \\\"money\\\"]\\n    ham_words = [\\\"meeting\\\", \\\"schedule\\\", \\\"report\\\", \\\"project\\\", \\\"team\\\", \\\"update\\\", \\\"review\\\", \\\"discuss\\\", \\\"attached\\\", \\\"please\\\", \\\"thanks\\\", \\\"regards\\\", \\\"confirm\\\", \\\"deadline\\\", \\\"agenda\\\"]\\n    \\n    texts = []\\n    labels = []\\n    \\n    for i in range(n):\\n        is_spam = rng.random() < 0.5\\n        if is_spam:\\n            num_words = rng.integers(5, 15)\\n            words = rng.choice(spam_words, size=num_words, replace=True)\\n            text = \\\" \\\".join(words)\\n            labels.append(1)\\n        else:\\n            num_words = rng.integers(5, 15)\\n            words = rng.choice(ham_words, size=num_words, replace=True)\\n            text = \\\" \\\".join(words)\\n            labels.append(0)\\n        texts.append(text)\\n    \\n    return texts, labels\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 250 synthetic spam/ham emails, train Naive Bayes with TF-IDF, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=250, help=\\\"Number of synthetic emails to generate (default: 250).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    texts, labels = generate_synthetic_emails(n=args.n_samples, seed=args.seed)\\n    \\n    if len(texts) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: no data generated\\\")\\n        sys.exit(1)\\n    \\n    X_train, X_test, y_train, y_test = train_test_split(\\n        texts, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\\n    )\\n    \\n    vectorizer = TfidfVectorizer(max_features=100)\\n    X_train_vec = vectorizer.fit_transform(X_train)\\n    X_test_vec = vectorizer.transform(X_test)\\n    \\n    clf = MultinomialNB()\\n    clf.fit(X_train_vec, y_train)\\n    \\n    y_pred = clf.predict(X_test_vec)\\n    acc = accuracy_score(y_test, y_pred)\\n    \\n    print(f\\\"n_samples={args.n_samples} accuracy={acc:.3f}\\\")\\n    \\n    if acc >= 0.7:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.7\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Recommender System\",\n",
            "    \"description\": \"Generate 200 synthetic user-item ratings, apply matrix factorization (NMF), compute RMSE. Print TEST_PASS if RMSE ≤ 1.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.decomposition import NMF\\nfrom sklearn.metrics import mean_squared_error\\n\\ndef generate_synthetic_ratings(n_users=50, n_items=40, n_ratings=200, seed=42):\\n    \\\"\\\"\\\"Generate synthetic user-item ratings matrix.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    \\n    # Create sparse ratings: user_id, item_id, rating\\n    user_ids = rng.integers(0, n_users, size=n_ratings)\\n    item_ids = rng.integers(0, n_items, size=n_ratings)\\n    ratings = rng.uniform(1.0, 5.0, size=n_ratings)\\n    \\n    # Build dense matrix (users x items)\\n    R = np.zeros((n_users, n_items))\\n    for u, i, r in zip(user_ids, item_ids, ratings):\\n        R[u, i] = r\\n    \\n    # Mask: which entries are observed\\n    mask = (R > 0)\\n    \\n    return R, mask, user_ids, item_ids, ratings\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Synthetic recommender with NMF; RMSE ≤ 1.5 for TEST_PASS.\\\")\\n    p.add_argument(\\\"--n-users\\\", type=int, default=50, help=\\\"Number of users (default: 50).\\\")\\n    p.add_argument(\\\"--n-items\\\", type=int, default=40, help=\\\"Number of items (default: 40).\\\")\\n    p.add_argument(\\\"--n-ratings\\\", type=int, default=200, help=\\\"Number of ratings (default: 200).\\\")\\n    p.add_argument(\\\"--n-components\\\", type=int, default=10, help=\\\"NMF latent factors (default: 10).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n    \\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n    \\n    # Generate synthetic ratings\\n    R, mask, user_ids, item_ids, ratings = generate_synthetic_ratings(\\n        n_users=args.n_users,\\n        n_items=args.n_items,\\n        n_ratings=args.n_ratings,\\n        seed=args.seed\\n    )\\n    \\n    if R is None or mask is None or np.sum(mask) == 0:\\n        print(\\\"TEST_FAIL: no ratings generated\\\")\\n        sys.exit(1)\\n    \\n    print(f\\\"Generated {args.n_ratings} ratings for {args.n_users} users and {args.n_items} items\\\")\\n    \\n    # Apply NMF (Non-negative Matrix Factorization)\\n    # NMF requires non-negative values; our ratings are already ≥ 1.0\\n    model = NMF(n_components=args.n_components, init='random', random_state=args.seed, max_iter=500)\\n    \\n    try:\\n        W = model.fit_transform(R)  # user factors\\n        H = model.components_       # item factors\\n        R_pred = W @ H              # reconstructed matrix\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: NMF failed - {e}\\\")\\n        sys.exit(1)\\n    \\n    # Compute RMSE on observed ratings only\\n    observed_true = R[mask]\\n    observed_pred = R_pred[mask]\\n    \\n    if len(observed_true) == 0:\\n        print(\\\"TEST_FAIL: no observed ratings to evaluate\\\")\\n        sys.exit(1)\\n    \\n    rmse = np.sqrt(mean_squared_error(observed_true, observed_pred))\\n    print(f\\\"RMSE on observed ratings: {rmse:.4f}\\\")\\n    \\n    # Acceptance: RMSE ≤ 1.5\\n    if rmse <= 1.5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: RMSE {rmse:.4f} > 1.5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Speech Emotion\",\n",
            "    \"description\": \"Generate 200 synthetic speech-like feature vectors labeled emotion, train RandomForest, report accuracy. Print TEST_PASS if accuracy ≥ 0.65.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_speech_emotion(n_samples=200, n_features=13, n_classes=4, seed=42):\\n    \\\"\\\"\\\"Generate synthetic speech-like feature vectors with emotion labels.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    \\n    # Emotion classes: 0=neutral, 1=happy, 2=sad, 3=angry\\n    y = rng.integers(0, n_classes, size=n_samples)\\n    \\n    # Generate MFCC-like features (13 coefficients typical for speech)\\n    X = np.zeros((n_samples, n_features))\\n    \\n    for i in range(n_samples):\\n        emotion = y[i]\\n        # Each emotion has characteristic feature patterns\\n        if emotion == 0:  # neutral\\n            X[i] = rng.normal(0.0, 1.0, size=n_features)\\n        elif emotion == 1:  # happy (higher pitch, more energy)\\n            X[i] = rng.normal(1.5, 1.2, size=n_features)\\n        elif emotion == 2:  # sad (lower pitch, less energy)\\n            X[i] = rng.normal(-1.5, 0.8, size=n_features)\\n        elif emotion == 3:  # angry (high energy, variable pitch)\\n            X[i] = rng.normal(0.5, 2.0, size=n_features)\\n    \\n    return X, y\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 200 synthetic speech emotion vectors, train RandomForest, report accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of synthetic samples (default: 200).\\\")\\n    p.add_argument(\\\"--n-features\\\", type=int, default=13, help=\\\"Number of features per sample (default: 13).\\\")\\n    p.add_argument(\\\"--n-classes\\\", type=int, default=4, help=\\\"Number of emotion classes (default: 4).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.25, help=\\\"Test set fraction (default: 0.25).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=100, help=\\\"Number of trees in RandomForest (default: 100).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n    \\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n    \\n    # Generate synthetic speech emotion dataset\\n    X, y = generate_synthetic_speech_emotion(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_classes=args.n_classes,\\n        seed=args.seed\\n    )\\n    \\n    # Validate dataset\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n    \\n    if len(X) != args.n_samples:\\n        print(f\\\"TEST_FAIL: expected {args.n_samples} samples, got {len(X)}\\\")\\n        sys.exit(1)\\n    \\n    # Split dataset\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n    \\n    # Train RandomForest\\n    clf = RandomForestClassifier(\\n        n_estimators=args.n_estimators,\\n        random_state=args.seed,\\n        n_jobs=-1\\n    )\\n    clf.fit(X_train, y_train)\\n    \\n    # Predict and evaluate\\n    y_pred = clf.predict(X_test)\\n    accuracy = accuracy_score(y_test, y_pred)\\n    \\n    print(f\\\"samples={args.n_samples} features={args.n_features} classes={args.n_classes}\\\")\\n    print(f\\\"train_size={len(X_train)} test_size={len(X_test)}\\\")\\n    print(f\\\"accuracy={accuracy:.3f}\\\")\\n    \\n    # Acceptance check\\n    if accuracy >= 0.65:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.65\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Financial Forecast\",\n",
            "    \"description\": \"Generate 300 synthetic stock-like sequences, predict direction (up/down), train LogisticRegression, report accuracy. Print TEST_PASS if accuracy ≥ 0.6.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef generate_synthetic_stock_sequences(n_sequences=300, seq_len=20, seed=42):\\n    \\\"\\\"\\\"Generate synthetic stock-like sequences with up/down labels.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    sequences = []\\n    labels = []\\n    \\n    for _ in range(n_sequences):\\n        # Generate a random walk with drift\\n        drift = rng.uniform(-0.02, 0.02)\\n        volatility = rng.uniform(0.01, 0.05)\\n        returns = rng.normal(drift, volatility, seq_len)\\n        prices = 100 * np.exp(np.cumsum(returns))\\n        \\n        # Label: 1 if final price > initial price, else 0\\n        label = 1 if prices[-1] > prices[0] else 0\\n        \\n        sequences.append(prices)\\n        labels.append(label)\\n    \\n    return np.array(sequences), np.array(labels)\\n\\ndef extract_features(sequences):\\n    \\\"\\\"\\\"Extract simple features from price sequences.\\\"\\\"\\\"\\n    features = []\\n    for seq in sequences:\\n        # Compute basic statistics\\n        mean_price = np.mean(seq)\\n        std_price = np.std(seq)\\n        min_price = np.min(seq)\\n        max_price = np.max(seq)\\n        price_range = max_price - min_price\\n        \\n        # Compute returns\\n        returns = np.diff(seq) / seq[:-1]\\n        mean_return = np.mean(returns)\\n        std_return = np.std(returns)\\n        \\n        # Trend: slope of linear fit\\n        x = np.arange(len(seq))\\n        trend = np.polyfit(x, seq, 1)[0]\\n        \\n        # Final vs initial price ratio\\n        price_ratio = seq[-1] / seq[0]\\n        \\n        features.append([\\n            mean_price, std_price, min_price, max_price, price_range,\\n            mean_return, std_return, trend, price_ratio\\n        ])\\n    \\n    return np.array(features)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 300 synthetic stock sequences, predict direction with LogisticRegression.\\\")\\n    p.add_argument(\\\"--n-sequences\\\", type=int, default=300, help=\\\"Number of synthetic sequences (default: 300).\\\")\\n    p.add_argument(\\\"--seq-len\\\", type=int, default=20, help=\\\"Length of each sequence (default: 20).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Validate inputs\\n    if args.n_sequences <= 0:\\n        print(\\\"TEST_FAIL: n_sequences must be positive\\\")\\n        sys.exit(1)\\n    if args.seq_len <= 1:\\n        print(\\\"TEST_FAIL: seq_len must be > 1\\\")\\n        sys.exit(1)\\n    if not (0 < args.test_size < 1):\\n        print(\\\"TEST_FAIL: test_size must be in (0, 1)\\\")\\n        sys.exit(1)\\n\\n    # Generate synthetic data\\n    sequences, labels = generate_synthetic_stock_sequences(\\n        n_sequences=args.n_sequences,\\n        seq_len=args.seq_len,\\n        seed=args.seed\\n    )\\n    \\n    if len(sequences) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: failed to generate synthetic data\\\")\\n        sys.exit(1)\\n\\n    # Extract features\\n    X = extract_features(sequences)\\n    y = labels\\n\\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # Scale features\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    # Train LogisticRegression\\n    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\\n    clf.fit(X_train_scaled, y_train)\\n\\n    # Evaluate\\n    accuracy = clf.score(X_test_scaled, y_test)\\n    print(f\\\"n_sequences={args.n_sequences} seq_len={args.seq_len} accuracy={accuracy:.3f}\\\")\\n\\n    # Acceptance check\\n    if accuracy >= 0.6:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} < 0.6\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST Autoencoder\",\n",
            "    \"description\": \"Load MNIST (or FakeData fallback), train a small autoencoder for a few epochs, reconstruct one image, and save original/reconstructed pair to autoencode.png. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\n\\ndef load_mnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0:\\n            raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        return train, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n    import matplotlib\\n    matplotlib.use('Agg')\\n    import matplotlib.pyplot as plt\\n\\n    p = argparse.ArgumentParser(description=\\\"MNIST autoencoder (opt-in download) or FakeData fallback; saves autoencode.png.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"autoencode.png\\\", help=\\\"Output image path (default: autoencode.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, real = load_mnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n\\n    class Autoencoder(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.encoder = nn.Sequential(\\n                nn.Flatten(),\\n                nn.Linear(28*28, 128),\\n                nn.ReLU(),\\n                nn.Linear(128, 64),\\n                nn.ReLU(),\\n                nn.Linear(64, 32)\\n            )\\n            self.decoder = nn.Sequential(\\n                nn.Linear(32, 64),\\n                nn.ReLU(),\\n                nn.Linear(64, 128),\\n                nn.ReLU(),\\n                nn.Linear(128, 28*28),\\n                nn.Sigmoid()\\n            )\\n        def forward(self, x):\\n            z = self.encoder(x)\\n            recon = self.decoder(z)\\n            return recon.view(-1, 1, 28, 28)\\n\\n    model = Autoencoder()\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n    loss_fn = nn.MSELoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        total_loss = 0.0\\n        for xb, _ in train_loader:\\n            opt.zero_grad()\\n            recon = model(xb)\\n            loss = loss_fn(recon, xb)\\n            loss.backward()\\n            opt.step()\\n            total_loss += loss.item()\\n        avg_loss = total_loss / len(train_loader)\\n        print(f\\\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\\\")\\n\\n    model.eval()\\n    with torch.no_grad():\\n        sample_x, _ = next(iter(DataLoader(train_ds, batch_size=1, shuffle=False)))\\n        sample_recon = model(sample_x)\\n\\n    orig = sample_x[0, 0].cpu().numpy()\\n    recon = sample_recon[0, 0].cpu().numpy()\\n\\n    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\\n    axes[0].imshow(orig, cmap='gray')\\n    axes[0].set_title('Original')\\n    axes[0].axis('off')\\n    axes[1].imshow(recon, cmap='gray')\\n    axes[1].set_title('Reconstructed')\\n    axes[1].axis('off')\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    print(f\\\"dataset={'mnist' if real else 'fake'} output={args.output}\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST VAE Latent\",\n",
            "    \"description\": \"Train a small VAE on FashionMNIST (or FakeData fallback), collect 2D latent codes, and save scatter plot to latent_vae.png. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\n\\ndef load_fashionmnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.FashionMNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0:\\n            raise RuntimeError(\\\"FashionMNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        return train, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n\\n    p = argparse.ArgumentParser(description=\\\"FashionMNIST VAE latent space visualization with opt-in download or FakeData fallback.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=2, help=\\\"Training epochs (default: 2).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--latent-dim\\\", type=int, default=2, help=\\\"Latent dimension (default: 2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit FashionMNIST download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"latent_vae.png\\\", help=\\\"Output latent space plot filename (default: latent_vae.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, real = load_fashionmnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n\\n    class VAE(nn.Module):\\n        def __init__(self, latent_dim=2):\\n            super().__init__()\\n            self.latent_dim = latent_dim\\n            self.encoder = nn.Sequential(\\n                nn.Flatten(),\\n                nn.Linear(28*28, 256), nn.ReLU(),\\n                nn.Linear(256, 128), nn.ReLU()\\n            )\\n            self.fc_mu = nn.Linear(128, latent_dim)\\n            self.fc_logvar = nn.Linear(128, latent_dim)\\n            self.decoder = nn.Sequential(\\n                nn.Linear(latent_dim, 128), nn.ReLU(),\\n                nn.Linear(128, 256), nn.ReLU(),\\n                nn.Linear(256, 28*28), nn.Sigmoid()\\n            )\\n\\n        def encode(self, x):\\n            h = self.encoder(x)\\n            return self.fc_mu(h), self.fc_logvar(h)\\n\\n        def reparameterize(self, mu, logvar):\\n            std = torch.exp(0.5 * logvar)\\n            eps = torch.randn_like(std)\\n            return mu + eps * std\\n\\n        def decode(self, z):\\n            return self.decoder(z).view(-1, 1, 28, 28)\\n\\n        def forward(self, x):\\n            mu, logvar = self.encode(x)\\n            z = self.reparameterize(mu, logvar)\\n            recon = self.decode(z)\\n            return recon, mu, logvar\\n\\n    def vae_loss(recon, x, mu, logvar):\\n        bce = nn.functional.binary_cross_entropy(recon, x, reduction='sum')\\n        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\\n        return bce + kld\\n\\n    model = VAE(latent_dim=args.latent_dim)\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        total_loss = 0.0\\n        for xb, _ in train_loader:\\n            opt.zero_grad()\\n            recon, mu, logvar = model(xb)\\n            loss = vae_loss(recon, xb, mu, logvar)\\n            loss.backward()\\n            opt.step()\\n            total_loss += loss.item()\\n        avg_loss = total_loss / len(train_loader.dataset)\\n        print(f\\\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\\\")\\n\\n    model.eval()\\n    latents = []\\n    labels = []\\n    with torch.no_grad():\\n        for xb, yb in train_loader:\\n            mu, _ = model.encode(xb)\\n            latents.append(mu.cpu().numpy())\\n            labels.append(yb.cpu().numpy())\\n    latents = np.concatenate(latents, axis=0)\\n    labels = np.concatenate(labels, axis=0)\\n\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10', alpha=0.6, s=10)\\n    plt.colorbar(scatter, label='Class')\\n    plt.xlabel('Latent Dim 1')\\n    plt.ylabel('Latent Dim 2')\\n    plt.title(f\\\"VAE Latent Space ({'FashionMNIST' if real else 'FakeData'})\\\")\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n    print(f\\\"Saved latent space plot to {args.output}\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"CIFAR10 Transfer Features\",\n",
            "    \"description\": \"If CIFAR10 available, extract features via pretrained CNN, else use FakeData, train LogisticRegression on features, report accuracy. Print TEST_PASS if accuracy ≥ 0.6 or fallback ≥ 0.4.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_cifar10_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n        ])\\n        train = datasets.CIFAR10(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.CIFAR10(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\\n        return train, test, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n        ])\\n        train = FakeData(size=max_train, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        test = FakeData(size=max_test, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef extract_features(model, dataloader, device):\\n    import torch\\n    model.eval()\\n    features_list = []\\n    labels_list = []\\n    with torch.no_grad():\\n        for xb, yb in dataloader:\\n            xb = xb.to(device)\\n            feat = model(xb)\\n            features_list.append(feat.cpu().numpy())\\n            labels_list.append(yb.numpy())\\n    X = np.vstack(features_list)\\n    y = np.concatenate(labels_list)\\n    return X, y\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    from torch.utils.data import DataLoader\\n    from sklearn.linear_model import LogisticRegression\\n\\n    p = argparse.ArgumentParser(description=\\\"CIFAR10 transfer features (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\\\")\\n    p.add_argument(\\\"--max-train\\\", type=int, default=2000, help=\\\"Max training samples (default: 2000).\\\")\\n    p.add_argument(\\\"--max-test\\\", type=int, default=500, help=\\\"Max test samples (default: 500).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    train_ds, test_ds, real = load_cifar10_or_fakedata(\\n        max_train=args.max_train,\\n        max_test=args.max_test,\\n        seed=args.seed,\\n        allow_download=args.allow_download\\n    )\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=False)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    device = torch.device(\\\"cpu\\\")\\n\\n    class FeatureExtractor(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.conv = nn.Sequential(\\n                nn.Conv2d(3, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\\n                nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\\n                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\\n            )\\n        def forward(self, x):\\n            x = self.conv(x)\\n            return x.view(x.size(0), -1)\\n\\n    model = FeatureExtractor().to(device)\\n\\n    print(\\\"Extracting training features...\\\")\\n    X_train, y_train = extract_features(model, train_loader, device)\\n    print(\\\"Extracting test features...\\\")\\n    X_test, y_test = extract_features(model, test_loader, device)\\n\\n    if X_train.shape[0] == 0 or X_test.shape[0] == 0:\\n        print(\\\"TEST_FAIL: no samples extracted\\\")\\n        sys.exit(1)\\n\\n    clf = LogisticRegression(max_iter=500, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    print(f\\\"acc={acc:.3f} dataset={'cifar10' if real else 'fake'}\\\")\\n\\n    threshold = 0.6 if real else 0.4\\n    if acc >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"ImageFolder Augmentation\",\n",
            "    \"description\": \"If ImageFolder available, apply rotation/flips to 20 images, else generate FakeData, augment, save augmented grid as 'augment_grid.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nfrom pathlib import Path\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply rotation/flips to ImageFolder or FakeData; save augmented grid as augment_grid.png.\\\")\\n    p.add_argument(\\\"--imagefolder-path\\\", type=str, default=None, help=\\\"Path to ImageFolder root (default: None, use FakeData).\\\")\\n    p.add_argument(\\\"--num-images\\\", type=int, default=20, help=\\\"Number of images to augment (default: 20).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"augment_grid.png\\\", help=\\\"Output grid filename (default: augment_grid.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit dataset download if needed.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        from torchvision.utils import make_grid\\n        from PIL import Image\\n    except ImportError:\\n        print(\\\"TEST_FAIL: torchvision or PIL not available\\\")\\n        sys.exit(1)\\n\\n    torch.manual_seed(args.seed)\\n\\n    use_imagefolder = False\\n    if args.imagefolder_path and os.path.isdir(args.imagefolder_path):\\n        try:\\n            base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\\n            ds = datasets.ImageFolder(root=args.imagefolder_path, transform=base_tfm)\\n            if len(ds) > 0:\\n                use_imagefolder = True\\n                print(f\\\"Using ImageFolder from {args.imagefolder_path} with {len(ds)} images\\\")\\n        except Exception as e:\\n            print(f\\\"ImageFolder failed: {e}, falling back to FakeData\\\")\\n\\n    if not use_imagefolder:\\n        print(\\\"Using FakeData fallback\\\")\\n        base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\\n        ds = datasets.FakeData(size=args.num_images, image_size=(3, 64, 64), num_classes=10, transform=base_tfm)\\n\\n    num_samples = min(args.num_images, len(ds))\\n    indices = list(range(num_samples))\\n    random.shuffle(indices)\\n    indices = indices[:num_samples]\\n\\n    augmentations = [\\n        transforms.RandomRotation(degrees=30),\\n        transforms.RandomHorizontalFlip(p=1.0),\\n        transforms.RandomVerticalFlip(p=1.0),\\n        transforms.Compose([transforms.RandomRotation(degrees=15), transforms.RandomHorizontalFlip(p=0.5)]),\\n    ]\\n\\n    augmented_images = []\\n    for idx in indices:\\n        img_tensor, _ = ds[idx]\\n        img_pil = transforms.ToPILImage()(img_tensor)\\n        aug = random.choice(augmentations)\\n        aug_pil = aug(img_pil)\\n        aug_tensor = transforms.ToTensor()(aug_pil)\\n        augmented_images.append(aug_tensor)\\n\\n    if len(augmented_images) == 0:\\n        print(\\\"TEST_FAIL: no images to augment\\\")\\n        sys.exit(1)\\n\\n    grid_tensor = make_grid(augmented_images, nrow=5, padding=2, normalize=False)\\n    grid_pil = transforms.ToPILImage()(grid_tensor)\\n    grid_pil.save(args.output)\\n    print(f\\\"Saved augmented grid to {args.output} with {len(augmented_images)} images\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FakeData Style Transfer\",\n",
            "    \"description\": \"Generate FakeData content/style pairs, apply basic style transfer algorithm, save result as 'style_transfer.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom pathlib import Path\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"FakeData style transfer: generate content/style pairs, apply basic transfer, save result.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"style_transfer.png\\\", help=\\\"Output image path (default: style_transfer.png).\\\")\\n    p.add_argument(\\\"--size\\\", type=int, default=256, help=\\\"Image size (default: 256).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        import torch\\n        from torchvision.datasets import FakeData\\n        from torchvision import transforms\\n        from PIL import Image\\n    except ImportError as e:\\n        print(f\\\"TEST_FAIL: missing dependency {e}\\\")\\n        sys.exit(1)\\n\\n    # generate FakeData content and style images\\n    torch.manual_seed(args.seed)\\n    tfm = transforms.Compose([\\n        transforms.Resize((args.size, args.size)),\\n        transforms.ToTensor()\\n    ])\\n    \\n    fake_ds = FakeData(size=2, image_size=(3, args.size, args.size), transform=tfm)\\n    content_tensor, _ = fake_ds[0]\\n    style_tensor, _ = fake_ds[1]\\n\\n    # basic style transfer: blend content and style with simple weighted average\\n    # convert to numpy for manipulation\\n    content_np = content_tensor.permute(1, 2, 0).numpy()\\n    style_np = style_tensor.permute(1, 2, 0).numpy()\\n    \\n    # simple transfer: 70% content + 30% style\\n    transferred = 0.7 * content_np + 0.3 * style_np\\n    transferred = np.clip(transferred, 0, 1)\\n    \\n    # convert back to PIL and save\\n    transferred_uint8 = (transferred * 255).astype(np.uint8)\\n    result_img = Image.fromarray(transferred_uint8, mode='RGB')\\n    result_img.save(args.output)\\n    \\n    # acceptance check\\n    if Path(args.output).exists():\\n        print(f\\\"Saved style transfer result to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"KMeans CIFAR Segmentation\",\n",
            "    \"description\": \"If CIFAR10 available, segment image via KMeans clustering, else use FakeData, save segmented image as 'segmented.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nfrom PIL import Image\\n\\ndef load_cifar_or_fakedata(seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        ds = datasets.CIFAR10(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(ds) == 0:\\n            raise RuntimeError(\\\"CIFAR10 cache missing and download disabled\\\")\\n        img_tensor, _ = ds[0]\\n        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\\n        return img_np, True\\n    except Exception:\\n        import torch\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        ds = FakeData(size=1, image_size=(3, 32, 32), num_classes=10, transform=tfm)\\n        img_tensor, _ = ds[0]\\n        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\\n        return img_np, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"KMeans CIFAR10 segmentation (opt-in download) or FakeData fallback; save segmented.png; TEST_PASS if file exists.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=4, help=\\\"Number of clusters for KMeans (default: 4).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit CIFAR10 download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"segmented.png\\\", help=\\\"Output segmented image path (default: segmented.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    img, real = load_cifar_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    if img is None or img.size == 0:\\n        print(\\\"TEST_FAIL: image not available\\\")\\n        sys.exit(1)\\n\\n    h, w, c = img.shape\\n    pixels = img.reshape(-1, c).astype(np.float32)\\n\\n    from sklearn.cluster import KMeans\\n    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\\n    labels = kmeans.fit_predict(pixels)\\n    centers = kmeans.cluster_centers_.astype(np.uint8)\\n\\n    segmented = centers[labels].reshape(h, w, c)\\n\\n    pil_img = Image.fromarray(segmented, mode=\\\"RGB\\\")\\n    pil_img.save(args.output)\\n\\n    print(f\\\"dataset={'cifar10' if real else 'fake'} k={args.k} saved={args.output}\\\")\\n\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Outlier Detection\",\n",
            "    \"description\": \"Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as 'outliers.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.datasets import load_iris\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as outliers.png.\\\")\\n    p.add_argument(\\\"--contamination\\\", type=float, default=0.1, help=\\\"Expected proportion of outliers (default: 0.1).\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"outliers.png\\\", help=\\\"Output plot filename (default: outliers.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_iris()\\n        X = data.data\\n        feature_names = data.feature_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    iso = IsolationForest(contamination=args.contamination, random_state=args.seed)\\n    iso.fit(X)\\n    predictions = iso.predict(X)\\n\\n    normal_mask = predictions == 1\\n    outlier_mask = predictions == -1\\n\\n    normal_points = X[normal_mask]\\n    outlier_points = X[outlier_mask]\\n\\n    num_outliers = outlier_points.shape[0]\\n    num_normal = normal_points.shape[0]\\n\\n    print(f\\\"Total samples: {len(X)}\\\")\\n    print(f\\\"Normal points: {num_normal}\\\")\\n    print(f\\\"Outliers detected: {num_outliers}\\\")\\n\\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\\n    ax.scatter(normal_points[:, 0], normal_points[:, 1], c='blue', label='Normal', alpha=0.6, edgecolors='k')\\n    ax.scatter(outlier_points[:, 0], outlier_points[:, 1], c='red', label='Outlier', alpha=0.8, edgecolors='k', s=100)\\n    ax.set_xlabel(feature_names[0])\\n    ax.set_ylabel(feature_names[1])\\n    ax.set_title(\\\"Iris Outlier Detection (IsolationForest)\\\")\\n    ax.legend()\\n    ax.grid(True, alpha=0.3)\\n\\n    try:\\n        plt.savefig(args.output, dpi=100, bbox_inches='tight')\\n        print(f\\\"Plot saved to {args.output}\\\")\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    if os.path.getsize(args.output) == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Anomaly Score\",\n",
            "    \"description\": \"Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as 'anomaly_scores.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as anomaly_scores.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"anomaly_scores.png\\\", help=\\\"Output histogram filename (default: anomaly_scores.png).\\\")\\n    p.add_argument(\\\"--nu\\\", type=float, default=0.1, help=\\\"OneClassSVM nu parameter (default: 0.1).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_wine()\\n        X = data.data\\n        if X is None or len(X) == 0:\\n            raise ValueError(\\\"wine dataset is empty\\\")\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: could not load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    oc_svm = OneClassSVM(nu=args.nu, kernel='rbf', gamma='auto')\\n    oc_svm.fit(X_scaled)\\n\\n    scores = oc_svm.decision_function(X_scaled)\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.hist(scores, bins=30, edgecolor='black', alpha=0.7)\\n    plt.xlabel('Anomaly Score')\\n    plt.ylabel('Frequency')\\n    plt.title('Wine Dataset Anomaly Scores (OneClassSVM)')\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100)\\n    plt.close()\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(f\\\"Histogram saved to {args.output}\\\")\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: output file {args.output} not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SHAP Values\",\n",
            "    \"description\": \"Train RandomForest on breast_cancer, compute SHAP values for one sample, save waterfall plot as 'shap_waterfall.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nimport shap\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train RandomForest on breast_cancer, compute SHAP values, save waterfall plot.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"shap_waterfall.png\\\", help=\\\"Output waterfall plot filename (default: shap_waterfall.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=50, help=\\\"Number of trees in RandomForest (default: 50).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n        feature_names = data.feature_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed, max_depth=5)\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"RandomForest accuracy on test set: {acc:.3f}\\\")\\n\\n    if len(X_test) == 0:\\n        print(\\\"TEST_FAIL: test set is empty\\\")\\n        sys.exit(1)\\n\\n    explainer = shap.TreeExplainer(clf)\\n    shap_values = explainer(X_test)\\n\\n    sample_idx = 0\\n    shap.plots.waterfall(shap_values[sample_idx, :, 1], show=False)\\n    plt.tight_layout()\\n    plt.savefig(args.output, dpi=100, bbox_inches='tight')\\n    plt.close()\\n    print(f\\\"Saved SHAP waterfall plot to {args.output}\\\")\\n\\n    import os\\n    if os.path.isfile(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file does not exist\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Digits Grad-CAM Heatmap\",\n",
            "    \"description\": \"Train simple CNN on digits, compute Grad-CAM heatmap for one image, overlay on input, save as 'gradcam.png'. Print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train CNN on digits, compute Grad-CAM heatmap, overlay on input, save as gradcam.png.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=3, help=\\\"Training epochs (default: 3).\\\")\\n    p.add_argument(\\\"--batch\\\", type=int, default=128, help=\\\"Batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"gradcam.png\\\", help=\\\"Output heatmap filename (default: gradcam.png).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n    from torchvision import datasets, transforms\\n    import cv2\\n\\n    def load_mnist_or_fakedata(seed=42, allow_download=False):\\n        try:\\n            torch.manual_seed(seed)\\n            tfm = transforms.ToTensor()\\n            train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n            test = datasets.MNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n            if len(train) == 0 or len(test) == 0:\\n                raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n            train = torch.utils.data.Subset(train, list(range(min(len(train), 2000))))\\n            test = torch.utils.data.Subset(test, list(range(min(len(test), 500))))\\n            return train, test, True\\n        except Exception:\\n            torch.manual_seed(seed)\\n            tfm = transforms.ToTensor()\\n            from torchvision.datasets import FakeData\\n            train = FakeData(size=2000, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n            test = FakeData(size=500, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n            return train, test, False\\n\\n    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\\n\\n    class SimpleCNN(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\\n            self.relu1 = nn.ReLU()\\n            self.pool1 = nn.MaxPool2d(2)\\n            self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\\n            self.relu2 = nn.ReLU()\\n            self.pool2 = nn.MaxPool2d(2)\\n            self.flatten = nn.Flatten()\\n            self.fc1 = nn.Linear(32 * 7 * 7, 64)\\n            self.relu3 = nn.ReLU()\\n            self.fc2 = nn.Linear(64, 10)\\n            self.gradients = None\\n            self.activations = None\\n\\n        def forward(self, x):\\n            x = self.conv1(x)\\n            x = self.relu1(x)\\n            x = self.pool1(x)\\n            x = self.conv2(x)\\n            x = self.relu2(x)\\n            x = self.pool2(x)\\n            if x.requires_grad:\\n                x.register_hook(self.save_gradient)\\n            self.activations = x\\n            x = self.flatten(x)\\n            x = self.fc1(x)\\n            x = self.relu3(x)\\n            x = self.fc2(x)\\n            return x\\n\\n        def save_gradient(self, grad):\\n            self.gradients = grad\\n\\n    model = SimpleCNN()\\n    opt = optim.Adam(model.parameters(), lr=1e-3)\\n    loss_fn = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            opt.zero_grad()\\n            logits = model(xb)\\n            loss = loss_fn(logits, yb)\\n            loss.backward()\\n            opt.step()\\n\\n    model.eval()\\n    correct = total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            pred = model(xb).argmax(1)\\n            correct += (pred == yb).sum().item()\\n            total += yb.numel()\\n    acc = correct / max(total, 1)\\n    print(f\\\"acc={acc:.3f} dataset={'mnist' if real else 'fake'}\\\")\\n\\n    test_iter = iter(test_loader)\\n    xb, yb = next(test_iter)\\n    img = xb[0:1]\\n    label = yb[0].item()\\n\\n    model.zero_grad()\\n    img.requires_grad = True\\n    output = model(img)\\n    pred_class = output.argmax(1).item()\\n    score = output[0, pred_class]\\n    score.backward()\\n\\n    gradients = model.gradients\\n    activations = model.activations\\n\\n    if gradients is None or activations is None:\\n        print(\\\"TEST_FAIL: gradients or activations not captured\\\")\\n        sys.exit(1)\\n\\n    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\\n    for i in range(activations.shape[1]):\\n        activations[:, i, :, :] *= pooled_gradients[i]\\n\\n    heatmap = torch.mean(activations, dim=1).squeeze()\\n    heatmap = torch.clamp(heatmap, min=0)\\n    heatmap /= (torch.max(heatmap) + 1e-8)\\n    heatmap_np = heatmap.detach().cpu().numpy()\\n\\n    heatmap_resized = cv2.resize(heatmap_np, (28, 28))\\n    heatmap_uint8 = np.uint8(255 * heatmap_resized)\\n    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\\n\\n    img_np = img.squeeze().detach().cpu().numpy()\\n    img_uint8 = np.uint8(255 * img_np)\\n    img_color = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2BGR)\\n\\n    overlay = cv2.addWeighted(img_color, 0.5, heatmap_color, 0.5, 0)\\n    cv2.imwrite(args.output, overlay)\\n\\n    if os.path.exists(args.output):\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file not created\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "items = result[\"items_with_code\"]\n",
        "\n",
        "for item in items:\n",
        "    if item[\"title\"] == \"Classification Synthetic Data\":\n",
        "        print(item)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpA7EnHE9UXi",
        "outputId": "e9c0901b-27a2-4a9f-8164-1e61b9737823"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Classification Synthetic Data', 'description': 'Generate make_classification data (n=500, n_features=4), train an SGDClassifier inside a StandardScaler pipeline, report accuracy. Print TEST_PASS if accuracy ≥ 0.85.', 'code': 'import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy.\")\\n    p.add_argument(\"--n-samples\", type=int, default=500, help=\"Number of samples (default: 500).\")\\n    p.add_argument(\"--n-features\", type=int, default=4, help=\"Number of features (default: 4).\")\\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y = make_classification(\\n        n_samples=args.n_samples,\\n        n_features=args.n_features,\\n        n_informative=max(2, args.n_features // 2),\\n        n_redundant=0,\\n        n_clusters_per_class=1,\\n        random_state=args.seed\\n    )\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\"TEST_FAIL: dataset generation failed\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    clf = Pipeline([\\n        (\"scaler\", StandardScaler()),\\n        (\"sgd\", SGDClassifier(max_iter=1000, tol=1e-3, random_state=args.seed))\\n    ])\\n    clf.fit(X_train, y_train)\\n\\n    y_pred = clf.predict(X_test)\\n    acc = accuracy_score(y_test, y_pred)\\n    print(f\"accuracy={acc:.3f}\")\\n\\n    if acc >= 0.85:\\n        print(\"TEST_PASS\")\\n    else:\\n        print(\"TEST_FAIL: accuracy below 0.85\")\\n        sys.exit(1)\\n\\nif __name__ == \"__main__\":\\n    main()\\n# END_OF_SCRIPT'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "claude_sonnet = result[\"items_with_code\"]"
      ],
      "metadata": {
        "id": "1phO9DX9_L2c"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 49th project -> index 48  (Breast Cancer SHAP Values)\n",
        "# 50th project -> index 49  (Digits Grad-CAM Heatmap)\n",
        "to_drop = {48, 49}\n",
        "\n",
        "claude_results = [item for i, item in enumerate(claude_sonnet) if i not in to_drop]\n",
        "\n",
        "with open(\"claude_sonnet_clean.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"items_with_code\": claude_results}, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Kept {len(claude_results)} projects, dropped {len(to_drop)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMof0fb8BzGi",
        "outputId": "5edf4f2f-a5ff-4553-d191-d1145c0cf519"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept 48 projects, dropped 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Now we have a dataset with 48 samples. The dataset is in JSONL format with title, description and code. This is only the base dataset and still we need to improve and generate more samples with more diversity in both topics and styles."
      ],
      "metadata": {
        "id": "Pnro2mqpboNm"
      }
    }
  ]
}