{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlve1UQSNZKhkZnRTCuPmS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/H4miiiid/MentorApp/blob/main/context_engineering_models(10_samples).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CmgoMnluG1Y",
        "outputId": "fd4ee3aa-cb84-4243-a79a-33da327c72c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is the test file to generate 10 samples (description + code) in JSON format with different models."
      ],
      "metadata": {
        "id": "eWREQ6kyUoWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Generate project descriptions and titles with three different models"
      ],
      "metadata": {
        "id": "HfK-afzwXPZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup the GPT-5 model and OpenRouter API key"
      ],
      "metadata": {
        "id": "ElC6Hg0L9q8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install requests jsonschema"
      ],
      "metadata": {
        "id": "S5uFmQWFVKMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, re, textwrap, datetime\n",
        "import requests\n",
        "from jsonschema import Draft7Validator\n",
        "\n",
        "OPENROUTER_API_KEY = \"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\"\n",
        "MODELS = [\n",
        "    #\"openai/gpt-4.1-mini\",\n",
        "    \"anthropic/claude-sonnet-4.5\",\n",
        "    \"qwen/qwen3-coder\",\n",
        "]"
      ],
      "metadata": {
        "id": "K0l84vbFVFOK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Set the variables"
      ],
      "metadata": {
        "id": "OO1bcsPjWvXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a meticulous AI project designer.\n",
        "Your job is to produce concise, implementable AI mini-project ideas that can be turned into runnable Python scripts.\n",
        "\n",
        "Dataset policy (VERY IMPORTANT):\n",
        "- You must prefer ONLY these real datasets when proposing projects:\n",
        "  - sklearn: iris, digits, wine, breast_cancer, diabetes\n",
        "  - sklearn generators: make_classification, make_regression, make_blobs, make_moons, make_circles\n",
        "  - torchvision: MNIST, FashionMNIST, CIFAR10, FakeData, ImageFolder\n",
        "- Do NOT propose or mention 20 Newsgroups, fetch_20newsgroups, or any \"newsgroups\"/\"news group\" variant.\n",
        "- If the project is NOT naturally covered by the whitelist (e.g. NLP, audio, recommendation, time-series), you MUST say in the description:\n",
        "  - “generate 200–300 synthetic … samples” (text / audio-like / tabular / time-series)\n",
        "  - Keep it clearly offline and small.\n",
        "- You may mention standard AI datasets (MNIST, FashionMNIST, CIFAR10) even if they need a download, BUT you must phrase the description so the script can fall back to a small synthetic dataset if the download is not available.\n",
        "\n",
        "Metrics & acceptance:\n",
        "- Every project idea must propose an acceptance/check that is realistic for the dataset + model.\n",
        "- If the code will FALL BACK to synthetic/FakeData, the acceptance must also FALL BACK to an easier threshold.\n",
        "- Use these safe ranges:\n",
        "  • iris (classification): accuracy ≥ 0.90\n",
        "  • wine (classification): accuracy ≥ 0.90–0.92\n",
        "  • breast_cancer (classification): accuracy ≥ 0.90–0.94\n",
        "  • digits + simple model (logreg / linear SVM): accuracy ≥ 0.90–0.93 (not 0.98)\n",
        "  • diabetes (regression): R² ≥ 0.35–0.45\n",
        "  • classic synthetic classifiers (make_moons, make_circles, make_blobs): accuracy ≥ 0.85–0.90, or silhouette ≥ 0.5 for blobs\n",
        "  • PCA / plotting / KMeans on images: acceptance = “file exists and non-empty” or “score in easy range”\n",
        "\n",
        "- If the task says “use synthetic / generate N samples”: set accuracy to 0.60–0.75 or R² to 0.25–0.35.\n",
        "- Do NOT demand SOTA or long training (no 0.99+, no 1e-4 MAE) for mini-projects.\n",
        "- If the dataset may not be available offline (e.g. Fashion-MNIST, MNIST, Reuters), explicitly tell the code generator:\n",
        "  “If real dataset not available → generate synthetic data → use lower threshold.”\n",
        "\n",
        "Rules:\n",
        "- Output must be valid JSON ONLY (no extra text).\n",
        "- Each item has exactly two keys: \"title\" and \"description\".\n",
        "- Titles are short and specific (≤ 6 words).\n",
        "- Descriptions are 1–2 sentences, concrete, and implementable offline in 20–60 minutes.\n",
        "- Prefer single-file, single-metric projects with tiny data and fast runtime.\n",
        "- Avoid duplicate or near-duplicate ideas.\n",
        "- Prefer standard Python libs or widely used ML libs (numpy, pandas, scikit-learn, PyTorch, TensorFlow, OpenCV).\n",
        "- No external downloads; use built-in toy datasets (e.g., sklearn iris/digits) or tiny synthetic data.\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "5vOUmWY-V7b6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEW_SHOTS = \"\"\"\n",
        "{\"title\":\"Iris KNN Classifier\",\n",
        "\"description\":\"Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.9.\"}\n",
        "{\"title\":\"Synthetic Text Sentiment\",\n",
        "\"description\":\"Create 200 short synthetic sentences labeled positive or negative, vectorize with CountVectorizer, train a LogisticRegression, and print accuracy; print TEST_PASS if accuracy ≥ 0.7.\"}\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "jkZyHY39Wi0b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. A function for describing the task"
      ],
      "metadata": {
        "id": "3heZju0CXrbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def build_task(n=10):\n",
        "    return textwrap.dedent(f\"\"\"\n",
        "    Task: Generate {n} distinct AI mini-project ideas.\n",
        "\n",
        "    Constraints:\n",
        "    - Return a JSON array of length {n}.\n",
        "    - Each item: object with exactly \"title\" (string) and \"description\" (string).\n",
        "    - No comments, no prose outside JSON.\n",
        "\n",
        "    Scope & Simplicity:\n",
        "    - Each project is doable offline in 20–60 minutes on CPU.\n",
        "    - Single-file mindset: one clear goal, one primary metric or artifact.\n",
        "    - Keep dependencies minimal (numpy/pandas/sklearn/torch/tf/opencv only).\n",
        "    - Mention one artifact or metric (png, accuracy, inertia, silhouette, TEST_PASS).\n",
        "\n",
        "    Dataset whitelist (must follow):\n",
        "    - sklearn: iris, digits, wine, breast_cancer, diabetes\n",
        "    - sklearn generators: make_classification, make_regression, make_blobs, make_moons, make_circles\n",
        "    - torchvision: MNIST, FashionMNIST, CIFAR10, FakeData, ImageFolder\n",
        "    - Do NOT use 20 Newsgroups or fetch_20newsgroups.\n",
        "\n",
        "    For NLP / audio / task-specific topics:\n",
        "    - Explicitly say: “generate 200–300 synthetic <domain> samples” so the code agent knows to build data in-code.\n",
        "\n",
        "    Description style:\n",
        "    - Titles ≤ 6 words, specific.\n",
        "    - Descriptions are 1–2 sentences with concrete I/O hints (flags, paths, outputs).\n",
        "    - Include at least one quick validation (e.g., accuracy threshold, file existence, non-empty output).\n",
        "\n",
        "    Diversity (within the allowed areas):\n",
        "    - Avoid repeating the same idea or trivial variants.\n",
        "\n",
        "    Follow the style of these examples without repeating them:\n",
        "    {FEW_SHOTS}\n",
        "\n",
        "    Now produce the JSON array of {n} items.\n",
        "    \"\"\").strip()\n"
      ],
      "metadata": {
        "id": "6-5DGgCOXZjl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. OpenRouter call helper"
      ],
      "metadata": {
        "id": "Fp4QApEQXwmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to make a safe Python variable name from a slug\n",
        "def varname_from_slug(slug: str) -> str:\n",
        "    name = slug.lower().replace(\"/\", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\")\n",
        "    return f\"{name}_result\""
      ],
      "metadata": {
        "id": "NzPyoCu1N0-S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic OpenRouter caller taking model_id\n",
        "def call_openrouter_model(model_id, messages, temperature=0.3, top_p=0.9, max_tokens=6000):\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
        "        \"X-Title\": f\"Multi-Model Project Generator\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": float(temperature),\n",
        "        \"top_p\": float(top_p),\n",
        "        \"max_tokens\": int(max_tokens),\n",
        "    }\n",
        "\n",
        "    # force SiliconFlow for Qwen models\n",
        "    if model_id.startswith(\"qwen/\"):\n",
        "      payload[\"provider\"] = {\n",
        "          \"only\": [\"atlas-cloud/fp8\"],\n",
        "          \"allow_fallbacks\": False\n",
        "      }\n",
        "\n",
        "    t0 = time.time()\n",
        "    r = requests.post(url, headers=headers, json=payload, timeout=120)\n",
        "    latency = time.time() - t0\n",
        "    r.raise_for_status()\n",
        "    content = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content, latency"
      ],
      "metadata": {
        "id": "rWSuWp7WMSRz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validator for title/description array\n",
        "from jsonschema import Draft7Validator\n",
        "def validate_items(arr, N):\n",
        "    ITEM_SCHEMA = {\n",
        "        \"type\":\"object\",\n",
        "        \"required\":[\"title\",\"description\"],\n",
        "        \"properties\":{\n",
        "            \"title\":{\"type\":\"string\",\"minLength\":3, \"maxLength\":100},\n",
        "            \"description\":{\"type\":\"string\",\"minLength\":20, \"maxLength\":600}\n",
        "        },\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "    ARRAY_SCHEMA = {\"type\":\"array\",\"items\":ITEM_SCHEMA, \"minItems\":N, \"maxItems\":N}\n",
        "    errs = [e.message for e in Draft7Validator(ARRAY_SCHEMA).iter_errors(arr)]\n",
        "    titles = [ (x.get(\"title\") or \"\").strip().lower() for x in arr ]\n",
        "    if len(set(titles)) != len(titles):\n",
        "        errs.append(\"Duplicate titles detected.\")\n",
        "    return errs"
      ],
      "metadata": {
        "id": "Vf2XvtJbOCqB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust extractor (keeps your previous logic)\n",
        "import re, json, time, requests\n",
        "def extract_json_array(text: str):\n",
        "    m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = m.group(1).strip()\n",
        "    start = text.find('[')\n",
        "    if start == -1:\n",
        "        return None\n",
        "    depth = 0\n",
        "    for i in range(start, len(text)):\n",
        "        ch = text[i]\n",
        "        if ch == '[': depth += 1\n",
        "        elif ch == ']':\n",
        "            depth -= 1\n",
        "            if depth == 0:\n",
        "                return text[start:i+1]\n",
        "    return None"
      ],
      "metadata": {
        "id": "FPYRGxi3OGxS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single shared message set\n",
        "N = 10\n",
        "task = build_task(N) + \"\\n\\nReturn a raw JSON array only — no prose, no code fences, no markdown.\"\n",
        "messages = [\n",
        "    {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
        "    {\"role\":\"user\",\"content\": task}\n",
        "]\n",
        "\n",
        "# Loop models and create a separate variable per model with the results\n",
        "for model_id in MODELS:\n",
        "    print(f\"\\n===== {model_id} =====\")\n",
        "    varname = varname_from_slug(model_id)\n",
        "    try:\n",
        "        raw, secs = call_openrouter_model(model_id, messages, temperature=0.2)\n",
        "        json_str = extract_json_array(raw)\n",
        "        if not json_str:\n",
        "            globals()[varname] = {\n",
        "                \"raw\": raw, \"json_str\": None, \"items\": None,\n",
        "                \"errors\": [\"No JSON array found\"], \"latency\": secs\n",
        "            }\n",
        "            print(f\"❌ No JSON array found | {secs:.1f}s\")\n",
        "            continue\n",
        "\n",
        "        items = json.loads(json_str)\n",
        "        errors = validate_items(items, N)\n",
        "\n",
        "        globals()[varname] = {\n",
        "            \"raw\": raw, \"json_str\": json_str, \"items\": items,\n",
        "            \"errors\": errors, \"latency\": secs\n",
        "        }\n",
        "\n",
        "        if errors:\n",
        "            print(f\"⚠️ Parsed but validation errors ({len(errors)}) | {secs:.1f}s\")\n",
        "            for e in errors[:5]:\n",
        "                print(\" -\", e)\n",
        "        else:\n",
        "            print(f\"✅ Valid JSON ({len(items)} items) | {secs:.1f}s\")\n",
        "            print(json.dumps(items[:2], indent=2, ensure_ascii=False))\n",
        "    except Exception as e:\n",
        "        globals()[varname] = {\n",
        "            \"raw\": None, \"json_str\": None, \"items\": None,\n",
        "            \"errors\": [str(e)], \"latency\": None\n",
        "        }\n",
        "        print(\"❌ Exception:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoWEsUznP-wi",
        "outputId": "a4466938-29d2-4395-ac5d-1bca6ca5b5fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== anthropic/claude-sonnet-4.5 =====\n",
            "✅ Valid JSON (10 items) | 11.0s\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Digits SVM Classifier\",\n",
            "    \"description\": \"Load sklearn's digits dataset, train a linear SVM, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.95.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Random Forest\",\n",
            "    \"description\": \"Load sklearn's wine dataset, train a RandomForestClassifier with 50 trees, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "===== qwen/qwen3-coder =====\n",
            "✅ Valid JSON (10 items) | 42.3s\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Digits PCA Visualization\",\n",
            "    \"description\": \"Load sklearn's digits dataset, apply PCA to reduce to 2D, and save a scatter plot as 'digits_pca.png'. Print TEST_PASS if the file exists and is non-empty.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Cluster Silhouette\",\n",
            "    \"description\": \"Use sklearn's wine dataset to perform KMeans clustering (k=3), compute the average silhouette score, and print it. Print TEST_PASS if the score is ≥ 0.4.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model_id in MODELS:\n",
        "    print(\" -\", varname_from_slug(model_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEr6Y84rbtaM",
        "outputId": "7d46b514-0233-47fe-cd80-7083d60379fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - anthropic_claude_sonnet_4_5_result\n",
            " - qwen_qwen3_coder_result\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All the resuts for three models that I used are below:"
      ],
      "metadata": {
        "id": "4lC6XKaeW9ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(anthropic_claude_sonnet_4_5_result['items'], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXqi-EmWRcja",
        "outputId": "49b50034-1b31-4560-8a63-b0678e08bf27"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Digits SVM Classifier\",\n",
            "    \"description\": \"Load sklearn's digits dataset, train a linear SVM, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.95.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Random Forest\",\n",
            "    \"description\": \"Load sklearn's wine dataset, train a RandomForestClassifier with 50 trees, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"K-Means on Blobs\",\n",
            "    \"description\": \"Generate 300 samples with make_blobs (3 clusters), run KMeans (k=3), and print silhouette score. Print TEST_PASS if silhouette ≥ 0.5.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Linear Regression\",\n",
            "    \"description\": \"Load sklearn's diabetes dataset, train a LinearRegression model, and print test R² score. Print TEST_PASS if R² ≥ 0.4.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST Logistic Regression\",\n",
            "    \"description\": \"Load MNIST (or generate 1000 synthetic 28×28 grayscale images if unavailable), flatten pixels, train LogisticRegression, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.85.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Decision Tree\",\n",
            "    \"description\": \"Load sklearn's breast_cancer dataset, train a DecisionTreeClassifier (max_depth=5), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons Neural Network\",\n",
            "    \"description\": \"Generate 400 samples with make_moons, train a 2-layer PyTorch MLP (10 hidden units), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.85.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles SVC Classifier\",\n",
            "    \"description\": \"Generate 300 samples with make_circles, train an SVC with RBF kernel, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Time-Series Forecasting\",\n",
            "    \"description\": \"Generate 250 synthetic time-series samples (sine wave + noise), train a simple linear model to predict next value, and print mean absolute error. Print TEST_PASS if MAE ≤ 0.3.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"PCA Visualization on Iris\",\n",
            "    \"description\": \"Load sklearn's iris dataset, apply PCA to 2 components, save a scatter plot as pca_iris.png, and print TEST_PASS if file exists.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(qwen_qwen3_coder_result['items'], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv0RTeu2Ur29",
        "outputId": "5f0424bd-c6e2-4e80-d503-4f840bb65036"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Digits PCA Visualization\",\n",
            "    \"description\": \"Load sklearn's digits dataset, apply PCA to reduce to 2D, and save a scatter plot as 'digits_pca.png'. Print TEST_PASS if the file exists and is non-empty.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Cluster Silhouette\",\n",
            "    \"description\": \"Use sklearn's wine dataset to perform KMeans clustering (k=3), compute the average silhouette score, and print it. Print TEST_PASS if the score is ≥ 0.4.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SVM\",\n",
            "    \"description\": \"Train an SVM classifier on sklearn's breast_cancer dataset, split 80/20, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Linear Regression\",\n",
            "    \"description\": \"Fit a linear regression model on sklearn's diabetes dataset, print the R² score on test split. Print TEST_PASS if R² ≥ 0.4.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST CNN Classifier\",\n",
            "    \"description\": \"Train a small CNN on MNIST (or FakeData if unavailable), print test accuracy after 5 epochs. Print TEST_PASS if accuracy ≥ 0.9.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Moons DBSCAN\",\n",
            "    \"description\": \"Generate sklearn's make_moons dataset (n=300), apply DBSCAN clustering, and print the number of clusters found. Print TEST_PASS if clusters ≥ 2.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Audio Classifier\",\n",
            "    \"description\": \"Generate 250 synthetic 1D audio-like samples with 2 classes, extract MFCC-like features, train a LogisticRegression, and print accuracy. Print TEST_PASS if accuracy ≥ 0.75.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST KMeans\",\n",
            "    \"description\": \"Apply KMeans (k=10) to FashionMNIST grayscale images (or FakeData), compute inertia, and print it. Print TEST_PASS if inertia ≤ 50000.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Decision Tree\",\n",
            "    \"description\": \"Train a DecisionTreeClassifier on sklearn's iris dataset, print test accuracy and tree depth. Print TEST_PASS if accuracy ≥ 0.9 and depth ≤ 5.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Blobs GMM\",\n",
            "    \"description\": \"Generate sklearn's make_blobs dataset (n=200, centers=4), fit a GaussianMixture model, and print AIC score. Print TEST_PASS if AIC ≤ 1000.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generating the codes for each projects that have been generated earlier with the models"
      ],
      "metadata": {
        "id": "HXJrkTU-XJ3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- External memory (compact, curated) --------\n",
        "memory = {\n",
        "  \"style_guide\": [\n",
        "    \"Single-file script with `if __name__ == '__main__':` entrypoint.\",\n",
        "    \"Use argparse with clear --help and sensible defaults.\",\n",
        "    \"Prefer standard library datasets (sklearn, torchvision, keras).\",\n",
        "    \"Attempt auto-download/cache with short timeout; if unavailable, fallback to a tiny structured synthetic dataset.\",\n",
        "    \"Fix randomness: set seeds for random, numpy; torch if used; run on CPU by default.\",\n",
        "    \"Validate inputs (paths, columns, image loads) and fail gracefully with one-line reason.\",\n",
        "    \"Keep runtime < 2 minutes (few epochs, small subsets).\",\n",
        "    \"Print `TEST_PASS` on success; otherwise `TEST_FAIL: <reason>`.\"\n",
        "  ],\n",
        "  \"lessons\": [\n",
        "    \"When standardizing features use sklearn.pipeline.Pipeline to avoid leakage.\",\n",
        "    \"For OpenCV Canny, expose --threshold1 and --threshold2; convert to grayscale before edges.\",\n",
        "    \"For CSV tasks, explicitly validate required columns; show a friendly error if missing.\",\n",
        "    \"For plotting, save figures to disk and plt.close() to avoid backend issues.\",\n",
        "    \"One file only. Return exactly ONE ```python block. No extra prose.\",\n",
        "    \"CLI + help. Use a single 'argparse.ArgumentParser()'. All help strings are single-line (no embedded newlines).\",\n",
        "    \"Seeds in 'main()'. Expose '--seed' and set seeds for random, numpy, and torch (if present) inside main().\",\n",
        "    \"Data access policy. Only use library datasets when --allow-download is passed. Otherwise do not download; use a robust fallback (sklearn tabular, torchvision FakeData, PIL shapes, etc.). If using 20NG, call with download_if_missing=False unless allowed.\",\n",
        "    \"Task–dataset match. Choose datasets that match the task (e.g., do not use 20 Newsgroups for spam/ham).\",\n",
        "    \"CV safety. For OpenCV: 1. convert to grayscale if needed. 2. ensure input to detectors is uint8 (cv2.convertScaleAbs if needed). 3. for Haar, check face_cascade.empty() == False or fail.\",\n",
        "    \"Acceptance contract. Implement explicit pass/fail checks (files exist, metrics ≥ thresholds, non-empty edge map, etc.). Print TEST_PASS only when all conditions hold; otherwise TEST_FAIL: <reason> and sys.exit(1).\",\n",
        "    \"No broken syntax. Never split identifiers across lines. Never break f-strings or string literals across lines.\",\n",
        "    \"End marker. Append '# END_OF_SCRIPT' as the last line of the file.\"\n",
        "  ],\n",
        "  \"snippets\": [\n",
        "    # seed block to embed in each script\n",
        "    \"import random, numpy as np\\nrandom.seed(42)\\nnp.random.seed(42)\\ntry:\\n    import torch\\n    torch.manual_seed(42)\\nexcept Exception:\\n    pass\"\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "9lD2Trr6Ux6B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT_CODEGEN = \"\"\"\n",
        "You are a meticulous senior Python engineer who writes production-quality, runnable scripts.\n",
        "Priorities: (1) correctness, (2) reproducibility, (3) clarity, (4) speed.\n",
        "\n",
        "Formatting & Output Contract:\n",
        "- Return ONE code block only: ```python ...```\n",
        "- The code must be a single file with `if __name__ == \"__main__\":` entrypoint.\n",
        "- Provide a clear CLI via argparse and `--help`. All help strings must be single-line (no embedded newlines).\n",
        "- Do not print explanations. Do not include markdown outside the single code block.\n",
        "- Append `# END_OF_SCRIPT` as the final line of the file.\n",
        "\n",
        "Dataset whitelist (MUST follow):\n",
        "- You may directly load/use ONLY these real datasets:\n",
        "  - sklearn: iris, digits, wine, breast_cancer, diabetes\n",
        "  - sklearn generators: make_classification, make_regression, make_blobs, make_moons, make_circles\n",
        "  - torchvision: MNIST, FashionMNIST, CIFAR10, FakeData, ImageFolder\n",
        "- Do NOT use 20 Newsgroups, fetch_20newsgroups, or any \"newsgroups\" variant.\n",
        "- If the project description says to “generate 200–300 synthetic … samples”, you MUST implement that synthetic dataset in code (e.g. build 200 labeled sentences, or 300 tabular rows, or 200 (x,y) pairs).\n",
        "- If the project mentions an allowed dataset that might require download (e.g. MNIST, FashionMNIST, CIFAR10), first TRY to load it, and if it fails or `--allow-download` was not passed, fall back to a synthetic dataset that matches the task.\n",
        "\n",
        "Behavioral Rules:\n",
        "- Expose `--seed` and set seeds **inside `main()`** for `random`, `numpy`, and `torch` (if available); run on CPU by default.\n",
        "- Validate inputs (paths, columns, image loads, flags) and fail gracefully with a concise message.\n",
        "- For OpenCV tasks: convert to grayscale when needed; ensure `uint8` input; for Haar cascades, ensure `face_cascade.empty() == False` or fail.\n",
        "- Keep the code minimal, readable, and fully runnable in a fresh Colab.\n",
        "- Never split identifiers across lines; never break string literals, f-strings, or comments across lines.\n",
        "  - Comments must be on one line each (e.g. `# custom text prediction`), not split into two lines.\n",
        "- Implement explicit acceptance checks tied to the task (files exist, metrics ≥ thresholds, non-empty edge map, etc.).\n",
        "- Print `TEST_PASS` only when all acceptance conditions hold; otherwise print `TEST_FAIL: <reason>` and `sys.exit(1)`.\n",
        "\n",
        "Self-Check Before Returning (silently revise if any item fails):\n",
        "- argparse help strings are single-line.\n",
        "- Seeds are applied in `main()` for random/numpy/torch.\n",
        "- No downloads are attempted because `--allow-download` was not passed.\n",
        "- Dataset matches the task semantics.\n",
        "- Dataset name is NOT `20newsgroups` / `fetch_20newsgroups` / “newsgroups”, unless the task is explicitly about newsgroups AND `--allow-download` was passed.\n",
        "- Acceptance checks implemented; `TEST_PASS`/`TEST_FAIL` present.\n",
        "- File ends with `# END_OF_SCRIPT`.\n",
        "- Code parses without SyntaxError and comments are not broken across lines.\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "dTqjD9yBhJ6O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEWSHOTS_CODE = \"\"\"\n",
        "Example A (tabular classification with sklearn iris -> fallback synthetic; seeds-in-main; single-line help; acceptance checks)\n",
        "```python\n",
        "import argparse, sys\n",
        "import random, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def load_iris_or_synthetic(seed=42):\n",
        "    try:\n",
        "        from sklearn.datasets import load_iris  # no download required\n",
        "        data = load_iris()\n",
        "        X, y, used = data.data, data.target, \"iris\"\n",
        "    except Exception:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        n = 210\n",
        "        c = rng.integers(0, 3, size=n)\n",
        "        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\n",
        "        y, used = c, \"synthetic\"\n",
        "    return X, y, used\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"Iris (no-download) or synthetic fallback; seeds set in main; explicit acceptance.\")\n",
        "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    # seeds in main\n",
        "    random.seed(args.seed); np.random.seed(args.seed)\n",
        "    try:\n",
        "        import torch; torch.manual_seed(args.seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    X, y, used = load_iris_or_synthetic(args.seed)\n",
        "    if X is None or y is None or len(X) == 0:\n",
        "        print(\"TEST_FAIL: dataset not available\"); sys.exit(1)\n",
        "\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\n",
        "    clf = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LogisticRegression(max_iter=300))])\n",
        "    clf.fit(Xtr, ytr)\n",
        "    acc = clf.score(Xte, yte)\n",
        "    print(f\"dataset={used} accuracy={acc:.3f}\")\n",
        "    # acceptance: stricter if iris, looser if synthetic\n",
        "    if acc >= (0.85 if used == \"iris\" else 0.70):\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: accuracy below threshold\"); sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\n",
        "```\n",
        "\n",
        "Example B (vision MNIST with opt-in download -> fallback FakeData; uint8 safety; seeds-in-main; acceptance checks)\n",
        "```python\n",
        "import argparse, sys, os\n",
        "import random, numpy as np\n",
        "\n",
        "def load_mnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\n",
        "    try:\n",
        "        import torch\n",
        "        from torchvision import datasets, transforms\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        # only download if explicitly allowed\n",
        "        train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
        "        test  = datasets.MNIST(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n",
        "        # if dataset objects are empty because cache missing and download disabled, trigger fallback\n",
        "        if len(train) == 0 or len(test) == 0:\n",
        "            raise RuntimeError(\"MNIST cache missing and download disabled\")\n",
        "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
        "        test  = torch.utils.data.Subset(test,  list(range(min(len(test),  max_test))))\n",
        "        return train, test, True\n",
        "    except Exception:\n",
        "        import torch\n",
        "        from torchvision import transforms\n",
        "        from torchvision.datasets import FakeData\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
        "        test  = FakeData(size=max_test,  image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
        "        return train, test, False\n",
        "\n",
        "def main():\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    p = argparse.ArgumentParser(description=\"MNIST (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=1, help=\"Training epochs (default: 1).\")\n",
        "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    # seeds in main\n",
        "    random.seed(args.seed); np.random.seed(args.seed); torch.manual_seed(args.seed)\n",
        "\n",
        "    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n",
        "    train = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
        "    test  = DataLoader(test_ds,  batch_size=args.batch, shuffle=False)\n",
        "\n",
        "    class TinyCNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Conv2d(1, 16, 3, 1), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(16, 32, 3, 1), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(32*5*5, 64), nn.ReLU(),\n",
        "                nn.Linear(64, 10)\n",
        "            )\n",
        "        def forward(self, x): return self.net(x)\n",
        "\n",
        "    model = TinyCNN()\n",
        "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for _ in range(args.epochs):\n",
        "        for xb, yb in train:\n",
        "            # ensure uint8 -> float32 is handled by ToTensor; just train\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "    # eval + acceptance\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test:\n",
        "            pred = model(xb).argmax(1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.numel()\n",
        "    acc = correct / max(total, 1)\n",
        "    print(f\"acc={acc:.3f} dataset={'mnist' if real else 'fake'}\")\n",
        "    # stricter if real, looser if fake\n",
        "    if acc >= (0.85 if real else 0.20):\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: accuracy below threshold\"); sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\n",
        "```\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "Zfk26-lPhjg-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, requests\n",
        "\n",
        "# --- helpers ---\n",
        "def extract_code_block(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Return the first code block content if present; otherwise return the whole text.\n",
        "    Prefers ```python ... ``` but accepts ``` ... ```.\n",
        "    \"\"\"\n",
        "    m = re.search(r\"```(?:python)?\\s*([\\s\\S]*?)\\s*```\", text, re.IGNORECASE)\n",
        "    return m.group(1) if m else text\n",
        "\n",
        "def print_long(s: str, width: int = 4000):\n",
        "    \"\"\"Print long strings without Colab truncation, in chunks.\"\"\"\n",
        "    for i in range(0, len(s), width):\n",
        "        print(s[i:i+width])"
      ],
      "metadata": {
        "id": "OqSL3SvAmQsI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Task builder (per project) --------\n",
        "def build_code_task(project, memory):\n",
        "    guide = \"\\n- \".join(memory[\"style_guide\"])\n",
        "    lessons = \"\\n- \".join(memory[\"lessons\"][-6:])\n",
        "    seed_block = memory[\"snippets\"][0]\n",
        "\n",
        "    return f\"\"\"\n",
        "    PROJECT TITLE:\n",
        "    {project['title']}\n",
        "\n",
        "    PROJECT DESCRIPTION:\n",
        "    {project['description']}\n",
        "\n",
        "    Follow this style guide:\n",
        "    - {guide}\n",
        "\n",
        "    Incorporate recent lessons:\n",
        "    - {lessons}\n",
        "\n",
        "    Hard guardrails (must follow):\n",
        "    - Return ONE code block only: ```python ...``` (no extra prose).\n",
        "    - Single file with `if __name__ == \"__main__\":` entrypoint.\n",
        "    - Use argparse; **all help strings are single-line** (no embedded newlines).\n",
        "    - Expose `--seed` and set seeds **inside `main()`** for random, numpy, and torch (if available).\n",
        "    - Use ONLY the whitelist datasets when a real dataset is required:\n",
        "      - sklearn: iris, digits, wine, breast_cancer, diabetes\n",
        "      - sklearn generators: make_classification, make_regression, make_blobs, make_moons, make_circles\n",
        "      - torchvision: MNIST, FashionMNIST, CIFAR10, FakeData, ImageFolder\n",
        "    - If the description asks for NLP, audio, or task-specific data that is NOT in the whitelist, generate 200–300 synthetic samples in code (labelled if classification).\n",
        "    - Do NOT use 20 Newsgroups or fetch_20newsgroups.\n",
        "    - Choose datasets that **match the task semantics** (e.g., do NOT use 20 Newsgroups for spam/ham).\n",
        "    - For OpenCV tasks: convert to grayscale when needed; ensure `uint8` input (use `cv2.convertScaleAbs` if necessary); for Haar cascades verify `face_cascade.empty()==False` or fail.\n",
        "    - Never split identifiers across lines; never break string literals or f-strings across lines.\n",
        "\n",
        "    Embed this seed block near the top of the script:\n",
        "    ```python\n",
        "    {seed_block}\"\"\"\n"
      ],
      "metadata": {
        "id": "GAf1N6exiOEE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- OpenRouter caller --> (code) --------\n",
        "import time\n",
        "\n",
        "def call_openrouter_model_code(model_id, messages, temperature=0.2, top_p=0.9, max_tokens=6000):\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
        "        \"X-Title\": \"Multi-Model Project Generator\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": float(temperature),\n",
        "        \"top_p\": float(top_p),\n",
        "        \"max_tokens\": int(max_tokens),\n",
        "    }\n",
        "\n",
        "    # force SiliconFlow for Qwen models\n",
        "    if model_id.startswith(\"qwen/\"):\n",
        "      payload[\"provider\"] = {\n",
        "          \"only\": [\"novita/fp8\"],\n",
        "          \"allow_fallbacks\": False\n",
        "      }\n",
        "\n",
        "    time.sleep(2.0)\n",
        "    t0 = time.time()\n",
        "    r = requests.post(url, headers=headers, json=payload, timeout=120)\n",
        "    latency = time.time() - t0\n",
        "    r.raise_for_status()\n",
        "    code = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return code, latency\n"
      ],
      "metadata": {
        "id": "o4q9n4p5itoy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------- Map: model slug -> per-model dict variable name --------\n",
        "\n",
        "MODEL_TO_RESULTVAR = {\n",
        "#\"openai/gpt-5\": \"openai_gpt_5_result\",\n",
        "\"anthropic/claude-sonnet-4.5\": \"anthropic_claude_sonnet_4_5_result\" ,\n",
        "\"qwen/qwen3-coder\": \"qwen_qwen3_coder_result\"\n",
        "}"
      ],
      "metadata": {
        "id": "jhRCCtsKjETU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Generate code for the first 4 projects per model --------\n",
        "\n",
        "print(\"Generating code for first 10 items of each model's projects...\\n\")\n",
        "per_model_generated_code = {}\n",
        "\n",
        "def code_items_varname(slug: str) -> str:\n",
        "    return slug.lower().replace(\"/\", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\") + \"_code_items\"\n",
        "\n",
        "\n",
        "for model_id, varname in MODEL_TO_RESULTVAR.items():\n",
        "    result = globals().get(varname)\n",
        "    if not result or not result.get(\"items\"):\n",
        "        print(f\"Skipping {model_id}: no items found in `{varname}`\")\n",
        "        continue\n",
        "\n",
        "    projects = result[\"items\"][:]\n",
        "    print(f\"\\n===== {model_id}: generating and attaching code for {len(projects)} projects =====\")\n",
        "\n",
        "    items_with_code = []\n",
        "    for idx, proj in enumerate(projects, 1):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT_CODEGEN},\n",
        "            {\"role\": \"user\", \"content\": \"Study these two short code examples and copy their structure (CLI, dataset policy, seeds, TEST_PASS contract).\"},\n",
        "            {\"role\": \"assistant\", \"content\": FEWSHOTS_CODE},\n",
        "            {\"role\": \"user\", \"content\": build_code_task(proj, memory)},\n",
        "        ]\n",
        "        raw, latency = call_openrouter_model_code(model_id, messages, temperature=0.2, max_tokens=7000)\n",
        "        code = extract_code_block(raw)\n",
        "\n",
        "        item = {\n",
        "            \"title\": proj[\"title\"],\n",
        "            \"description\": proj[\"description\"],\n",
        "            \"code\": code\n",
        "        }\n",
        "        items_with_code.append(item)\n",
        "\n",
        "        # show the full code (no truncation)\n",
        "        print(f\"\\n--- {model_id} • Project {idx}: {proj['title']} --- Latency: {latency:.2f}s ---\\n\")\n",
        "        print_long(code)  # full code printed\n",
        "\n",
        "    # put per-model list into a dedicated variable\n",
        "    var_codes = code_items_varname(model_id)  # e.g., openai_gpt_5_code_items\n",
        "    globals()[var_codes] = items_with_code\n",
        "\n",
        "    # also store inside the original result dict under 'items_with_code' for convenience\n",
        "    result[\"items_with_code\"] = items_with_code\n",
        "\n",
        "    # pretty JSON view of the per-model list\n",
        "    print(f\"\\n>>> {model_id} • JSON with title, description, code:\")\n",
        "    print_long(json.dumps(items_with_code, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6ZV3JUYkuZW",
        "outputId": "1a9e0d7c-7f0e-41a4-d362-ecfeca67bbef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating code for first 10 items of each model's projects...\n",
            "\n",
            "\n",
            "===== anthropic/claude-sonnet-4.5: generating and attaching code for 10 projects =====\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 1: Digits SVM Classifier --- Latency: 8.21s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_digits\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.svm import LinearSVC\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.pipeline import Pipeline\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train a linear SVM on sklearn digits dataset and print test accuracy.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # Load digits dataset (no download required)\n",
            "    try:\n",
            "        data = load_digits()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: digits dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Split data\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # Build pipeline with scaler and linear SVM\n",
            "    clf = Pipeline([\n",
            "        (\"scaler\", StandardScaler()),\n",
            "        (\"svm\", LinearSVC(max_iter=2000, random_state=args.seed))\n",
            "    ])\n",
            "\n",
            "    # Train\n",
            "    clf.fit(X_train, y_train)\n",
            "\n",
            "    # Evaluate\n",
            "    acc = clf.score(X_test, y_test)\n",
            "    print(f\"Test accuracy: {acc:.4f}\")\n",
            "\n",
            "    # Acceptance check\n",
            "    if acc >= 0.95:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.95\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 2: Wine Random Forest --- Latency: 8.12s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_wine\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train RandomForestClassifier on sklearn wine dataset; print TEST_PASS if accuracy >= 0.92.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--n-estimators\", type=int, default=50, help=\"Number of trees in forest (default: 50).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # load wine dataset (no download required)\n",
            "    try:\n",
            "        data = load_wine()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load wine dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: wine dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # split\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # train RandomForest\n",
            "    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "\n",
            "    # evaluate\n",
            "    acc = clf.score(X_test, y_test)\n",
            "    print(f\"Test accuracy: {acc:.4f}\")\n",
            "\n",
            "    # acceptance check\n",
            "    if acc >= 0.92:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.4f} < 0.92\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 3: K-Means on Blobs --- Latency: 7.45s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import make_blobs\n",
            "from sklearn.cluster import KMeans\n",
            "from sklearn.metrics import silhouette_score\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"K-Means clustering on synthetic blobs; print silhouette score and TEST_PASS if >= 0.5.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n",
            "    p.add_argument(\"--n-clusters\", type=int, default=3, help=\"Number of clusters (default: 3).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    if args.n_samples < 1:\n",
            "        print(\"TEST_FAIL: n_samples must be >= 1\")\n",
            "        sys.exit(1)\n",
            "    if args.n_clusters < 2:\n",
            "        print(\"TEST_FAIL: n_clusters must be >= 2\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X, y_true = make_blobs(n_samples=args.n_samples, centers=args.n_clusters, random_state=args.seed)\n",
            "    \n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    kmeans = KMeans(n_clusters=args.n_clusters, random_state=args.seed, n_init=10)\n",
            "    labels = kmeans.fit_predict(X)\n",
            "\n",
            "    sil = silhouette_score(X, labels)\n",
            "    print(f\"n_samples={args.n_samples} n_clusters={args.n_clusters} silhouette={sil:.3f}\")\n",
            "\n",
            "    if sil >= 0.5:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: silhouette score {sil:.3f} < 0.5\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 4: Diabetes Linear Regression --- Latency: 8.52s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_diabetes\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.metrics import r2_score\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train LinearRegression on sklearn diabetes dataset and print test R² score.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # load diabetes dataset (no download required)\n",
            "    try:\n",
            "        data = load_diabetes()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load diabetes dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # split\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed\n",
            "    )\n",
            "\n",
            "    # train\n",
            "    model = LinearRegression()\n",
            "    model.fit(X_train, y_train)\n",
            "\n",
            "    # predict and evaluate\n",
            "    y_pred = model.predict(X_test)\n",
            "    r2 = r2_score(y_test, y_pred)\n",
            "\n",
            "    print(f\"R² score: {r2:.4f}\")\n",
            "\n",
            "    # acceptance: R² >= 0.4\n",
            "    if r2 >= 0.4:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: R² score {r2:.4f} below threshold 0.4\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 5: MNIST Logistic Regression --- Latency: 13.59s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def load_mnist_or_synthetic(seed=42, allow_download=False):\n",
            "    \"\"\"Load MNIST if available/allowed, else generate 1000 synthetic 28x28 grayscale images.\"\"\"\n",
            "    try:\n",
            "        from torchvision import datasets, transforms\n",
            "        import torch\n",
            "        torch.manual_seed(seed)\n",
            "        transform = transforms.Compose([transforms.ToTensor()])\n",
            "        train_ds = datasets.MNIST(root='./data', train=True, download=bool(allow_download), transform=transform)\n",
            "        test_ds = datasets.MNIST(root='./data', train=False, download=bool(allow_download), transform=transform)\n",
            "        if len(train_ds) == 0 or len(test_ds) == 0:\n",
            "            raise RuntimeError(\"MNIST cache missing and download disabled\")\n",
            "        X_train = train_ds.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0\n",
            "        y_train = train_ds.targets.numpy()\n",
            "        X_test = test_ds.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0\n",
            "        y_test = test_ds.targets.numpy()\n",
            "        return X_train, y_train, X_test, y_test, True\n",
            "    except Exception:\n",
            "        rng = np.random.default_rng(seed)\n",
            "        n_train = 800\n",
            "        n_test = 200\n",
            "        X_train = rng.random((n_train, 28*28), dtype=np.float32)\n",
            "        y_train = rng.integers(0, 10, size=n_train)\n",
            "        X_test = rng.random((n_test, 28*28), dtype=np.float32)\n",
            "        y_test = rng.integers(0, 10, size=n_test)\n",
            "        return X_train, y_train, X_test, y_test, False\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"MNIST logistic regression with fallback to synthetic 28x28 images.\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X_train, y_train, X_test, y_test, is_real = load_mnist_or_synthetic(seed=args.seed, allow_download=args.allow_download)\n",
            "\n",
            "    if X_train is None or len(X_train) == 0 or X_test is None or len(X_test) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    from sklearn.linear_model import LogisticRegression\n",
            "    clf = LogisticRegression(max_iter=100, solver='lbfgs', random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "    acc = clf.score(X_test, y_test)\n",
            "\n",
            "    dataset_name = \"MNIST\" if is_real else \"synthetic\"\n",
            "    print(f\"dataset={dataset_name} test_accuracy={acc:.3f}\")\n",
            "\n",
            "    if acc >= 0.85:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.3f} < 0.85\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 6: Breast Cancer Decision Tree --- Latency: 7.28s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import load_breast_cancer\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train DecisionTreeClassifier on breast_cancer dataset; print TEST_PASS if accuracy >= 0.90.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--max-depth\", type=int, default=5, help=\"Max depth of decision tree (default: 5).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # load breast_cancer dataset\n",
            "    try:\n",
            "        data = load_breast_cancer()\n",
            "        X, y = data.data, data.target\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load breast_cancer dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # train/test split\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # train decision tree\n",
            "    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "\n",
            "    # evaluate\n",
            "    acc = clf.score(X_test, y_test)\n",
            "    print(f\"Test accuracy: {acc:.3f}\")\n",
            "\n",
            "    # acceptance check\n",
            "    if acc >= 0.90:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.3f} < 0.90\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 7: Moons Neural Network --- Latency: 13.71s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 400 samples with make_moons, train a 2-layer PyTorch MLP (10 hidden units), print test accuracy.\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--epochs\", type=int, default=100, help=\"Training epochs (default: 100).\")\n",
            "    p.add_argument(\"--lr\", type=float, default=0.01, help=\"Learning rate (default: 0.01).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # generate dataset\n",
            "    try:\n",
            "        from sklearn.datasets import make_moons\n",
            "        from sklearn.model_selection import train_test_split\n",
            "        import torch\n",
            "        import torch.nn as nn\n",
            "        import torch.optim as optim\n",
            "    except ImportError as e:\n",
            "        print(f\"TEST_FAIL: missing dependency {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X, y = make_moons(n_samples=400, noise=0.1, random_state=args.seed)\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # convert to torch tensors\n",
            "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
            "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
            "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
            "    y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
            "\n",
            "    # define 2-layer MLP with 10 hidden units\n",
            "    class MLP(nn.Module):\n",
            "        def __init__(self):\n",
            "            super().__init__()\n",
            "            self.fc1 = nn.Linear(2, 10)\n",
            "            self.fc2 = nn.Linear(10, 2)\n",
            "        \n",
            "        def forward(self, x):\n",
            "            x = torch.relu(self.fc1(x))\n",
            "            x = self.fc2(x)\n",
            "            return x\n",
            "\n",
            "    model = MLP()\n",
            "    criterion = nn.CrossEntropyLoss()\n",
            "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
            "\n",
            "    # train\n",
            "    model.train()\n",
            "    for epoch in range(args.epochs):\n",
            "        optimizer.zero_grad()\n",
            "        outputs = model(X_train_t)\n",
            "        loss = criterion(outputs, y_train_t)\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "\n",
            "    # evaluate\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        test_outputs = model(X_test_t)\n",
            "        _, predicted = torch.max(test_outputs, 1)\n",
            "        correct = (predicted == y_test_t).sum().item()\n",
            "        total = y_test_t.size(0)\n",
            "        accuracy = correct / max(total, 1)\n",
            "\n",
            "    print(f\"test_accuracy={accuracy:.3f}\")\n",
            "\n",
            "    # acceptance check\n",
            "    if accuracy >= 0.85:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {accuracy:.3f} < 0.85\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 8: Circles SVC Classifier --- Latency: 9.54s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.datasets import make_circles\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.svm import SVC\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.pipeline import Pipeline\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 300 samples with make_circles, train SVC with RBF kernel, print test accuracy.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--noise\", type=float, default=0.1, help=\"Noise level for make_circles (default: 0.1).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # Generate circles dataset\n",
            "    X, y = make_circles(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\n",
            "    \n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Split into train and test\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # Build pipeline with StandardScaler and SVC with RBF kernel\n",
            "    clf = Pipeline([\n",
            "        (\"scaler\", StandardScaler()),\n",
            "        (\"svc\", SVC(kernel=\"rbf\", random_state=args.seed))\n",
            "    ])\n",
            "\n",
            "    # Train\n",
            "    clf.fit(X_train, y_train)\n",
            "\n",
            "    # Evaluate\n",
            "    acc = clf.score(X_test, y_test)\n",
            "    print(f\"Test accuracy: {acc:.3f}\")\n",
            "\n",
            "    # Acceptance check\n",
            "    if acc >= 0.90:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.3f} below threshold 0.90\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 9: Synthetic Time-Series Forecasting --- Latency: 11.64s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "\n",
            "def generate_synthetic_timeseries(n_samples=250, seq_len=50, seed=42):\n",
            "    \"\"\"Generate synthetic time-series: sine wave + noise.\"\"\"\n",
            "    rng = np.random.default_rng(seed)\n",
            "    X = []\n",
            "    y = []\n",
            "    for i in range(n_samples):\n",
            "        # random frequency and phase\n",
            "        freq = rng.uniform(0.05, 0.15)\n",
            "        phase = rng.uniform(0, 2 * np.pi)\n",
            "        noise_level = rng.uniform(0.05, 0.15)\n",
            "        # generate sequence\n",
            "        t = np.arange(seq_len + 1)\n",
            "        series = np.sin(2 * np.pi * freq * t + phase) + rng.normal(0, noise_level, size=seq_len + 1)\n",
            "        # use first seq_len as features, last value as target\n",
            "        X.append(series[:seq_len])\n",
            "        y.append(series[seq_len])\n",
            "    return np.array(X), np.array(y)\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate 250 synthetic time-series samples (sine wave + noise), train linear model, print MAE.\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=250, help=\"Number of synthetic time-series samples (default: 250).\")\n",
            "    p.add_argument(\"--seq-len\", type=int, default=50, help=\"Length of each time-series sequence (default: 50).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # generate synthetic time-series\n",
            "    X, y = generate_synthetic_timeseries(n_samples=args.n_samples, seq_len=args.seq_len, seed=args.seed)\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset generation failed\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # split train/test\n",
            "    from sklearn.model_selection import train_test_split\n",
            "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\n",
            "\n",
            "    # train simple linear model\n",
            "    from sklearn.linear_model import LinearRegression\n",
            "    model = LinearRegression()\n",
            "    model.fit(X_train, y_train)\n",
            "\n",
            "    # predict and compute MAE\n",
            "    y_pred = model.predict(X_test)\n",
            "    from sklearn.metrics import mean_absolute_error\n",
            "    mae = mean_absolute_error(y_test, y_pred)\n",
            "\n",
            "    print(f\"n_samples={args.n_samples} seq_len={args.seq_len} MAE={mae:.4f}\")\n",
            "\n",
            "    # acceptance: MAE <= 0.3\n",
            "    if mae <= 0.3:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: MAE {mae:.4f} > 0.3\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 10: PCA Visualization on Iris --- Latency: 10.96s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "import matplotlib\n",
            "matplotlib.use('Agg')\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.decomposition import PCA\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Apply PCA to iris dataset and save scatter plot as pca_iris.png.\")\n",
            "    p.add_argument(\"--output\", type=str, default=\"pca_iris.png\", help=\"Output filename for PCA scatter plot (default: pca_iris.png).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # Load iris dataset (no download required)\n",
            "    try:\n",
            "        data = load_iris()\n",
            "        X = data.data\n",
            "        y = data.target\n",
            "        target_names = data.target_names\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to load iris dataset: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: iris dataset is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Apply PCA to 2 components\n",
            "    pca = PCA(n_components=2, random_state=args.seed)\n",
            "    X_pca = pca.fit_transform(X)\n",
            "\n",
            "    # Create scatter plot\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    colors = ['red', 'green', 'blue']\n",
            "    for i, color, name in zip(range(3), colors, target_names):\n",
            "        mask = y == i\n",
            "        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], color=color, label=name, alpha=0.7, edgecolors='k')\n",
            "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
            "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
            "    plt.title('PCA of Iris Dataset')\n",
            "    plt.legend()\n",
            "    plt.grid(True, alpha=0.3)\n",
            "    plt.tight_layout()\n",
            "\n",
            "    # Save plot\n",
            "    try:\n",
            "        plt.savefig(args.output, dpi=100)\n",
            "        plt.close()\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: failed to save plot: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Acceptance check: verify file exists\n",
            "    import os\n",
            "    if not os.path.isfile(args.output):\n",
            "        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    file_size = os.path.getsize(args.output)\n",
            "    if file_size == 0:\n",
            "        print(f\"TEST_FAIL: output file {args.output} is empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    print(f\"PCA scatter plot saved to {args.output} ({file_size} bytes)\")\n",
            "    print(\"TEST_PASS\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            ">>> anthropic/claude-sonnet-4.5 • JSON with title, description, code:\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Digits SVM Classifier\",\n",
            "    \"description\": \"Load sklearn's digits dataset, train a linear SVM, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.95.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train a linear SVM on sklearn digits dataset and print test accuracy.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Load digits dataset (no download required)\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # Build pipeline with scaler and linear SVM\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"svm\\\", LinearSVC(max_iter=2000, random_state=args.seed))\\n    ])\\n\\n    # Train\\n    clf.fit(X_train, y_train)\\n\\n    # Evaluate\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {acc:.4f}\\\")\\n\\n    # Acceptance check\\n    if acc >= 0.95:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.95\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Random Forest\",\n",
            "    \"description\": \"Load sklearn's wine dataset, train a RandomForestClassifier with 50 trees, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train RandomForestClassifier on sklearn wine dataset; print TEST_PASS if accuracy >= 0.92.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=50, help=\\\"Number of trees in forest (default: 50).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # load wine dataset (no download required)\\n    try:\\n        data = load_wine()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train RandomForest\\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    # evaluate\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {acc\n",
            ":.4f}\\\")\\n\\n    # acceptance check\\n    if acc >= 0.92:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} < 0.92\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"K-Means on Blobs\",\n",
            "    \"description\": \"Generate 300 samples with make_blobs (3 clusters), run KMeans (k=3), and print silhouette score. Print TEST_PASS if silhouette ≥ 0.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import silhouette_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"K-Means clustering on synthetic blobs; print silhouette score and TEST_PASS if >= 0.5.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--n-clusters\\\", type=int, default=3, help=\\\"Number of clusters (default: 3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples < 1:\\n        print(\\\"TEST_FAIL: n_samples must be >= 1\\\")\\n        sys.exit(1)\\n    if args.n_clusters < 2:\\n        print(\\\"TEST_FAIL: n_clusters must be >= 2\\\")\\n        sys.exit(1)\\n\\n    X, y_true = make_blobs(n_samples=args.n_samples, centers=args.n_clusters, random_state=args.seed)\\n    \\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    kmeans = KMeans(n_clusters=args.n_clusters, random_state=args.seed, n_init=10)\\n    labels = kmeans.fit_predict(X)\\n\\n    sil = silhouette_score(X, labels)\\n    print(f\\\"n_samples={args.n_samples} n_clusters={args.n_clusters} silhouette={sil:.3f}\\\")\\n\\n    if sil >= 0.5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: silhouette score {sil:.3f} < 0.5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Linear Regression\",\n",
            "    \"description\": \"Load sklearn's diabetes dataset, train a LinearRegression model, and print test R² score. Print TEST_PASS if R² ≥ 0.4.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train LinearRegression on sklearn diabetes dataset and print test R² score.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # load diabetes dataset (no download required)\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    # train\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    # predict and evaluate\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"R² score: {r2:.4f}\\\")\\n\\n    # acceptance: R² >= 0.4\\n    if r2 >= 0.4:\\n        print\n",
            "(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R² score {r2:.4f} below threshold 0.4\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST Logistic Regression\",\n",
            "    \"description\": \"Load MNIST (or generate 1000 synthetic 28×28 grayscale images if unavailable), flatten pixels, train LogisticRegression, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.85.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_mnist_or_synthetic(seed=42, allow_download=False):\\n    \\\"\\\"\\\"Load MNIST if available/allowed, else generate 1000 synthetic 28x28 grayscale images.\\\"\\\"\\\"\\n    try:\\n        from torchvision import datasets, transforms\\n        import torch\\n        torch.manual_seed(seed)\\n        transform = transforms.Compose([transforms.ToTensor()])\\n        train_ds = datasets.MNIST(root='./data', train=True, download=bool(allow_download), transform=transform)\\n        test_ds = datasets.MNIST(root='./data', train=False, download=bool(allow_download), transform=transform)\\n        if len(train_ds) == 0 or len(test_ds) == 0:\\n            raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n        X_train = train_ds.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0\\n        y_train = train_ds.targets.numpy()\\n        X_test = test_ds.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0\\n        y_test = test_ds.targets.numpy()\\n        return X_train, y_train, X_test, y_test, True\\n    except Exception:\\n        rng = np.random.default_rng(seed)\\n        n_train = 800\\n        n_test = 200\\n        X_train = rng.random((n_train, 28*28), dtype=np.float32)\\n        y_train = rng.integers(0, 10, size=n_train)\\n        X_test = rng.random((n_test, 28*28), dtype=np.float32)\\n        y_test = rng.integers(0, 10, size=n_test)\\n        return X_train, y_train, X_test, y_test, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"MNIST logistic regression with fallback to synthetic 28x28 images.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X_train, y_train, X_test, y_test, is_real = load_mnist_or_synthetic(seed=args.seed, allow_download=args.allow_download)\\n\\n    if X_train is None or len(X_train) == 0 or X_test is None or len(X_test) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    from sklearn.linear_model import LogisticRegression\\n    clf = LogisticRegression(max_iter=100, solver='lbfgs', random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    dataset_name = \\\"MNIST\\\" if is_real else \\\"synthetic\\\"\\n    print(f\\\"dataset={dataset_name} test_accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.85:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} < 0.85\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Decision Tree\",\n",
            "    \"description\": \"Load sklearn's breast_cancer dataset, train a DecisionTreeClassifier (max_depth=5), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train DecisionTreeClassifier on breast_cancer dataset; print TEST_PASS if accuracy >= 0.90.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2)\n",
            ".\\\")\\n    p.add_argument(\\\"--max-depth\\\", type=int, default=5, help=\\\"Max depth of decision tree (default: 5).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # load breast_cancer dataset\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # train/test split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train decision tree\\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    # evaluate\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {acc:.3f}\\\")\\n\\n    # acceptance check\\n    if acc >= 0.90:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} < 0.90\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons Neural Network\",\n",
            "    \"description\": \"Generate 400 samples with make_moons, train a 2-layer PyTorch MLP (10 hidden units), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.85.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 400 samples with make_moons, train a 2-layer PyTorch MLP (10 hidden units), print test accuracy.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=100, help=\\\"Training epochs (default: 100).\\\")\\n    p.add_argument(\\\"--lr\\\", type=float, default=0.01, help=\\\"Learning rate (default: 0.01).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # generate dataset\\n    try:\\n        from sklearn.datasets import make_moons\\n        from sklearn.model_selection import train_test_split\\n        import torch\\n        import torch.nn as nn\\n        import torch.optim as optim\\n    except ImportError as e:\\n        print(f\\\"TEST_FAIL: missing dependency {e}\\\")\\n        sys.exit(1)\\n\\n    X, y = make_moons(n_samples=400, noise=0.1, random_state=args.seed)\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # convert to torch tensors\\n    X_train_t = torch.tensor(X_train, dtype=torch.float32)\\n    y_train_t = torch.tensor(y_train, dtype=torch.long)\\n    X_test_t = torch.tensor(X_test, dtype=torch.float32)\\n    y_test_t = torch.tensor(y_test, dtype=torch.long)\\n\\n    # define 2-layer MLP with 10 hidden units\\n    class MLP(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.fc1 = nn.Linear(2, 10)\\n            self.fc2 = nn.Linear(10, 2)\\n        \\n        def forward(self, x):\\n            x = torch.relu(self.fc1(x))\\n            x = self.fc2(x)\\n            return x\\n\\n    model = MLP()\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = optim.SGD(model.parameters(), lr=args.lr)\\n\\n    # train\\n    model.train()\\n    for epoch in range(args.epochs):\\n        optimiz\n",
            "er.zero_grad()\\n        outputs = model(X_train_t)\\n        loss = criterion(outputs, y_train_t)\\n        loss.backward()\\n        optimizer.step()\\n\\n    # evaluate\\n    model.eval()\\n    with torch.no_grad():\\n        test_outputs = model(X_test_t)\\n        _, predicted = torch.max(test_outputs, 1)\\n        correct = (predicted == y_test_t).sum().item()\\n        total = y_test_t.size(0)\\n        accuracy = correct / max(total, 1)\\n\\n    print(f\\\"test_accuracy={accuracy:.3f}\\\")\\n\\n    # acceptance check\\n    if accuracy >= 0.85:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} < 0.85\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles SVC Classifier\",\n",
            "    \"description\": \"Generate 300 samples with make_circles, train an SVC with RBF kernel, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 300 samples with make_circles, train SVC with RBF kernel, print test accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.1, help=\\\"Noise level for make_circles (default: 0.1).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Generate circles dataset\\n    X, y = make_circles(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # Split into train and test\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # Build pipeline with StandardScaler and SVC with RBF kernel\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"svc\\\", SVC(kernel=\\\"rbf\\\", random_state=args.seed))\\n    ])\\n\\n    # Train\\n    clf.fit(X_train, y_train)\\n\\n    # Evaluate\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {acc:.3f}\\\")\\n\\n    # Acceptance check\\n    if acc >= 0.90:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold 0.90\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Time-Series Forecasting\",\n",
            "    \"description\": \"Generate 250 synthetic time-series samples (sine wave + noise), train a simple linear model to predict next value, and print mean absolute error. Print TEST_PASS if MAE ≤ 0.3.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef generate_synthetic_timeseries(n_samples=250, seq_len=50, seed=42):\\n    \\\"\\\"\\\"Generate synthetic time-series: sine wave + noise.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    X = []\\n    y = []\\n    for i in range(n_samples):\\n        # random frequency and phase\\n        freq = rng.uniform(0.05, 0.15)\\n        phase = rng.uniform(0, 2 * np.pi)\\n        noise_level = rng.uniform(0.05, 0.15)\\n        # generate sequence\\n        t = np.arange(seq_len + 1)\\n        series = np.sin(2 * np.pi * freq * t + phase) + rng.normal(0, noise_level, size=seq_len + 1)\\n        # use first seq_len as features, last value as ta\n",
            "rget\\n        X.append(series[:seq_len])\\n        y.append(series[seq_len])\\n    return np.array(X), np.array(y)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 250 synthetic time-series samples (sine wave + noise), train linear model, print MAE.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=250, help=\\\"Number of synthetic time-series samples (default: 250).\\\")\\n    p.add_argument(\\\"--seq-len\\\", type=int, default=50, help=\\\"Length of each time-series sequence (default: 50).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # generate synthetic time-series\\n    X, y = generate_synthetic_timeseries(n_samples=args.n_samples, seq_len=args.seq_len, seed=args.seed)\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # split train/test\\n    from sklearn.model_selection import train_test_split\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\\n\\n    # train simple linear model\\n    from sklearn.linear_model import LinearRegression\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    # predict and compute MAE\\n    y_pred = model.predict(X_test)\\n    from sklearn.metrics import mean_absolute_error\\n    mae = mean_absolute_error(y_test, y_pred)\\n\\n    print(f\\\"n_samples={args.n_samples} seq_len={args.seq_len} MAE={mae:.4f}\\\")\\n\\n    # acceptance: MAE <= 0.3\\n    if mae <= 0.3:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: MAE {mae:.4f} > 0.3\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"PCA Visualization on Iris\",\n",
            "    \"description\": \"Load sklearn's iris dataset, apply PCA to 2 components, save a scatter plot as pca_iris.png, and print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.decomposition import PCA\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply PCA to iris dataset and save scatter plot as pca_iris.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"pca_iris.png\\\", help=\\\"Output filename for PCA scatter plot (default: pca_iris.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Load iris dataset (no download required)\\n    try:\\n        data = load_iris()\\n        X = data.data\\n        y = data.target\\n        target_names = data.target_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # Apply PCA to 2 components\\n    pca = PCA(n_components=2, random_state=args.seed)\\n    X_pca = pca.fit_transform(X)\\n\\n    # Create scatter plot\\n    plt.figure(figsize=(8, 6))\\n    colors = ['red', 'green', 'blue']\\n    for i, color, name in zip(range(3), colors, target_names):\\n        mask = y == i\\n        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], color=color, label=name, alpha=0.7, edgecolors='k')\\n    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\\n    plt.ylabel(f'PC2 ({pca.explained_variance_\n",
            "ratio_[1]:.2%} variance)')\\n    plt.title('PCA of Iris Dataset')\\n    plt.legend()\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n\\n    # Save plot\\n    try:\\n        plt.savefig(args.output, dpi=100)\\n        plt.close()\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    # Acceptance check: verify file exists\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"PCA scatter plot saved to {args.output} ({file_size} bytes)\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  }\n",
            "]\n",
            "\n",
            "===== qwen/qwen3-coder: generating and attaching code for 10 projects =====\n",
            "\n",
            "--- qwen/qwen3-coder • Project 1: Digits PCA Visualization --- Latency: 7.91s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import os\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.decomposition import PCA\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Seed block\n",
            "random.seed(42)\n",
            "np.random.seed(42)\n",
            "try:\n",
            "    import torch\n",
            "    torch.manual_seed(42)\n",
            "except Exception:\n",
            "    pass\n",
            "\n",
            "def load_digits_or_synthetic(seed=42):\n",
            "    try:\n",
            "        from sklearn.datasets import load_digits\n",
            "        data = load_digits()\n",
            "        X, y = data.data, data.target\n",
            "        used = \"digits\"\n",
            "    except Exception:\n",
            "        # Fallback: synthetic 8x8-like grayscale data (64 features), 10 classes\n",
            "        rng = np.random.default_rng(seed)\n",
            "        n_samples = 300\n",
            "        n_features = 64\n",
            "        X = rng.integers(0, 16, size=(n_samples, n_features)).astype(np.float32)\n",
            "        y = rng.integers(0, 10, size=n_samples)\n",
            "        used = \"synthetic\"\n",
            "    return X, y, used\n",
            "\n",
            "def main():\n",
            "    parser = argparse.ArgumentParser(description=\"Apply PCA to sklearn digits (or synthetic) and save 2D scatter plot.\")\n",
            "    parser.add_argument(\"--output\", type=str, default=\"digits_pca.png\", help=\"Output plot filename (default: digits_pca.png).\")\n",
            "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    # Set seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y, dataset_type = load_digits_or_synthetic(seed=args.seed)\n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset could not be loaded\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # PCA to 2D\n",
            "    pca = PCA(n_components=2)\n",
            "    X_pca = pca.fit_transform(X)\n",
            "\n",
            "    # Plot\n",
            "    plt.figure(figsize=(8, 6))\n",
            "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"tab10\", s=5)\n",
            "    plt.colorbar(scatter, ticks=range(10))\n",
            "    plt.title(f\"PCA of {dataset_type.capitalize()} Dataset\")\n",
            "    plt.xlabel(\"PC1\")\n",
            "    plt.ylabel(\"PC2\")\n",
            "    plt.tight_layout()\n",
            "    plt.savefig(args.output)\n",
            "    plt.close()\n",
            "\n",
            "    # Acceptance check\n",
            "    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: output file missing or empty\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- qwen/qwen3-coder • Project 2: Wine Cluster Silhouette --- Latency: 6.10s ---\n",
            "\n",
            "import argparse, sys\n",
            "import random, numpy as np\n",
            "from sklearn.cluster import KMeans\n",
            "from sklearn.metrics import silhouette_score\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "def load_wine_or_synthetic(seed=42):\n",
            "    try:\n",
            "        from sklearn.datasets import load_wine\n",
            "        data = load_wine()\n",
            "        X, used = data.data, \"wine\"\n",
            "    except Exception:\n",
            "        rng = np.random.default_rng(seed)\n",
            "        n, d, k = 200, 13, 3\n",
            "        X = rng.normal(0, 1, size=(n, d))\n",
            "        y = rng.integers(0, k, size=n)\n",
            "        for i in range(k):\n",
            "            X[y == i] += rng.normal(0, 1, size=d) * 3\n",
            "        used = \"synthetic\"\n",
            "    return X, used\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Cluster wine dataset (or synthetic) with KMeans and compute silhouette score.\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, used = load_wine_or_synthetic(args.seed)\n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    scaler = StandardScaler()\n",
            "    X_scaled = scaler.fit_transform(X)\n",
            "\n",
            "    kmeans = KMeans(n_clusters=3, random_state=args.seed, n_init=10)\n",
            "    labels = kmeans.fit_predict(X_scaled)\n",
            "\n",
            "    score = silhouette_score(X_scaled, labels)\n",
            "    print(f\"dataset={used} silhouette={score:.3f}\")\n",
            "\n",
            "    if score >= 0.4:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: silhouette score below 0.4\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- qwen/qwen3-coder • Project 3: Breast Cancer SVM --- Latency: 9.83s ---\n",
            "\n",
            "import argparse, sys\n",
            "import random, numpy as np\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.svm import SVC\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "def load_breast_cancer_or_synthetic(seed=42):\n",
            "    try:\n",
            "        from sklearn.datasets import load_breast_cancer\n",
            "        data = load_breast_cancer()\n",
            "        X, y = data.data, data.target\n",
            "        used = \"breast_cancer\"\n",
            "    except Exception:\n",
            "        # Generate synthetic dataset matching breast cancer dims: 569x30 -> ~200x30\n",
            "        rng = np.random.default_rng(seed)\n",
            "        n_samples, n_features = 200, 30\n",
            "        X = rng.standard_normal((n_samples, n_features))\n",
            "        y = rng.integers(0, 2, size=n_samples)\n",
            "        used = \"synthetic\"\n",
            "    return X, y, used\n",
            "\n",
            "def main():\n",
            "    parser = argparse.ArgumentParser(description=\"Train SVM on sklearn breast_cancer or synthetic fallback; seeds in main; explicit acceptance.\")\n",
            "    parser.add_argument(\"--test-size\", type=float, default=0.2, help=\"Fraction of data to use for testing (default: 0.2).\")\n",
            "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed for reproducibility (default: 42).\")\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    # Set seeds inside main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except ImportError:\n",
            "        pass\n",
            "\n",
            "    # Load dataset\n",
            "    X, y, dataset_name = load_breast_cancer_or_synthetic(seed=args.seed)\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: unable to load dataset\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Split data\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # Scale features\n",
            "    scaler = StandardScaler()\n",
            "    X_train_scaled = scaler.fit_transform(X_train)\n",
            "    X_test_scaled = scaler.transform(X_test)\n",
            "\n",
            "    # Train SVM\n",
            "    svm = SVC(kernel='rbf', random_state=args.seed)\n",
            "    svm.fit(X_train_scaled, y_train)\n",
            "\n",
            "    # Predict and evaluate\n",
            "    y_pred = svm.predict(X_test_scaled)\n",
            "    acc = accuracy_score(y_test, y_pred)\n",
            "\n",
            "    print(f\"dataset={dataset_name} accuracy={acc:.4f}\")\n",
            "\n",
            "    # Acceptance threshold: stricter for real data\n",
            "    threshold = 0.92 if dataset_name == \"breast_cancer\" else 0.80\n",
            "    if acc >= threshold:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy below {threshold}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- qwen/qwen3-coder • Project 4: Diabetes Linear Regression --- Latency: 7.08s ---\n",
            "\n",
            "import argparse, sys\n",
            "import random, numpy as np\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.metrics import r2_score\n",
            "\n",
            "def load_diabetes_or_synthetic(seed=42):\n",
            "    try:\n",
            "        from sklearn.datasets import load_diabetes\n",
            "        data = load_diabetes()\n",
            "        X, y = data.data, data.target\n",
            "        used = \"diabetes\"\n",
            "    except Exception:\n",
            "        rng = np.random.default_rng(seed)\n",
            "        n = 200\n",
            "        X = rng.normal(0, 1, size=(n, 10))\n",
            "        y = X.sum(axis=1) + rng.normal(0, 0.1, size=n)\n",
            "        used = \"synthetic\"\n",
            "    return X, y, used\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Fit linear regression on sklearn diabetes (no-download) or synthetic fallback.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Fraction of data to use for testing (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed for reproducibility (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y, used = load_diabetes_or_synthetic(seed=args.seed)\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: failed to load dataset\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\n",
            "    model = LinearRegression()\n",
            "    model.fit(Xtr, ytr)\n",
            "    y_pred = model.predict(Xte)\n",
            "    r2 = r2_score(yte, y_pred)\n",
            "    print(f\"dataset={used} r2={r2:.3f}\")\n",
            "\n",
            "    if r2 >= 0.4:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: R2 score below 0.4\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- qwen/qwen3-coder • Project 5: MNIST CNN Classifier --- Latency: 11.42s ---\n",
            "\n",
            "import argparse, sys\n",
            "import random, numpy as np\n",
            "\n",
            "random.seed(42)\n",
            "np.random.seed(42)\n",
            "try:\n",
            "    import torch\n",
            "    torch.manual_seed(42)\n",
            "except Exception:\n",
            "    pass\n",
            "\n",
            "def load_mnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
            "        test = datasets.MNIST(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n",
            "        if len(train) == 0 or len(test) == 0:\n",
            "            raise RuntimeError(\"MNIST cache missing and download disabled\")\n",
            "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
            "        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\n",
            "        return train, test, True\n",
            "    except Exception:\n",
            "        from torchvision import transforms\n",
            "        from torchvision.datasets import FakeData\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "        test = FakeData(size=max_test, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "        return train, test, False\n",
            "\n",
            "def main():\n",
            "    import torch\n",
            "    import torch.nn as nn\n",
            "    import torch.optim as optim\n",
            "    from torch.utils.data import DataLoader\n",
            "\n",
            "    p = argparse.ArgumentParser(description=\"Train CNN on MNIST (or FakeData); print TEST_PASS if test acc ≥ 0.9.\")\n",
            "    p.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs (default: 5).\")\n",
            "    p.add_argument(\"--batch-size\", type=int, default=128, help=\"Training batch size (default: 128).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Allow downloading MNIST if not cached.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    torch.manual_seed(args.seed)\n",
            "\n",
            "    train_ds, test_ds, is_real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n",
            "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
            "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False)\n",
            "\n",
            "    class SimpleCNN(nn.Module):\n",
            "        def __init__(self):\n",
            "            super().__init__()\n",
            "            self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
            "            self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
            "            self.pool = nn.MaxPool2d(2)\n",
            "            self.fc1 = nn.Linear(32 * 5 * 5, 64)\n",
            "            self.fc2 = nn.Linear(64, 10)\n",
            "\n",
            "        def forward(self, x):\n",
            "            x = self.pool(torch.relu(self.conv1(x)))\n",
            "            x = self.pool(torch.relu(self.conv2(x)))\n",
            "            x = torch.flatten(x, 1)\n",
            "            x = torch.relu(self.fc1(x))\n",
            "            x = self.fc2(x)\n",
            "            return x\n",
            "\n",
            "    model = SimpleCNN()\n",
            "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
            "    criterion = nn.CrossEntropyLoss()\n",
            "\n",
            "    model.train()\n",
            "    for epoch in range(args.epochs):\n",
            "        for xb, yb in train_loader:\n",
            "            optimizer.zero_grad()\n",
            "            output = model(xb)\n",
            "            loss = criterion(output, yb)\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "    model.eval()\n",
            "    correct = total = 0\n",
            "    with torch.no_grad():\n",
            "        for xb, yb in test_loader:\n",
            "            output = model(xb)\n",
            "            pred = output.argmax(dim=1)\n",
            "            correct += (pred == yb).sum().item()\n",
            "            total += yb.size(0)\n",
            "\n",
            "    acc = correct / total if total > 0 else 0\n",
            "    print(f\"Test Accuracy: {acc:.4f} (dataset: {'MNIST' if is_real else 'FakeData'})\")\n",
            "\n",
            "    if acc >= 0.9:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: accuracy below 0.9 threshold\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- qwen/qwen3-coder • Project 6: Make Moons DBSCAN --- Latency: 12.62s ---\n",
            "\n",
            "import argparse, sys\n",
            "import random, numpy as np\n",
            "from sklearn.cluster import DBSCAN\n",
            "from sklearn.datasets import make_moons\n",
            "\n",
            "# Seed block\n",
            "random.seed(42)\n",
            "np.random.seed(42)\n",
            "try:\n",
            "    import torch\n",
            "    torch.manual_seed(42)\n",
            "except Exception:\n",
            "    pass\n",
            "\n",
            "def load_moons_or_synthetic(n_samples=300, seed=42):\n",
            "    \"\"\"Load make_moons data or fallback to synthetic if error.\"\"\"\n",
            "    try:\n",
            "        X, _ = make_moons(n_samples=n_samples, noise=0.1, random_state=seed)\n",
            "        return X, \"moons\"\n",
            "    except Exception:\n",
            "        # Synthetic fallback matching moon shape roughly\n",
            "        rng = np.random.default_rng(seed)\n",
            "        t = rng.uniform(0, 2*np.pi, n_samples//2)\n",
            "        dx, dy = 0.5, 0.25\n",
            "        x1 = np.cos(t) - dx; y1 = np.sin(t) + dy\n",
            "        x2 = np.cos(t) + dx; y2 = np.sin(t) - dy\n",
            "        X = np.vstack([np.column_stack([x1, y1]), np.column_stack([x2, y2])])\n",
            "        return X, \"synthetic\"\n",
            "\n",
            "def main():\n",
            "    parser = argparse.ArgumentParser(description=\"Apply DBSCAN to make_moons (or synthetic) data; print TEST_PASS if clusters ≥ 2.\")\n",
            "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    parser.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples (default: 300).\")\n",
            "    parser.add_argument(\"--eps\", type=float, default=0.2, help=\"DBSCAN eps parameter (default: 0.2).\")\n",
            "    parser.add_argument(\"--min-samples\", type=int, default=5, help=\"DBSCAN min_samples parameter (default: 5).\")\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    # Set seeds inside main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except ImportError:\n",
            "        pass\n",
            "\n",
            "    X, source = load_moons_or_synthetic(n_samples=args.n_samples, seed=args.seed)\n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: failed to load dataset\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Apply DBSCAN\n",
            "    dbscan = DBSCAN(eps=args.eps, min_samples=args.min_samples)\n",
            "    labels = dbscan.fit_predict(X)\n",
            "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
            "\n",
            "    print(f\"dataset={source} clusters={n_clusters}\")\n",
            "    if n_clusters >= 2:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: fewer than 2 clusters found\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- qwen/qwen3-coder • Project 7: Synthetic Audio Classifier --- Latency: 11.59s ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "from scipy.fftpack import dct\n",
            "\n",
            "# Seed block\n",
            "random.seed(42)\n",
            "np.random.seed(42)\n",
            "try:\n",
            "    import torch\n",
            "    torch.manual_seed(42)\n",
            "except Exception:\n",
            "    pass\n",
            "\n",
            "def generate_synthetic_audio_samples(n_samples=250, n_features=100, seed=42):\n",
            "    \"\"\"Generate synthetic 1D audio-like data with 2 classes.\"\"\"\n",
            "    rng = np.random.default_rng(seed)\n",
            "    X = []\n",
            "    y = []\n",
            "    for i in range(n_samples):\n",
            "        # Class 0: low frequency dominant\n",
            "        if i % 2 == 0:\n",
            "            signal = rng.normal(0, 1, n_features) + np.sin(np.linspace(0, 4*np.pi, n_features)) * 1.5\n",
            "            label = 0\n",
            "        # Class 1: high frequency dominant\n",
            "        else:\n",
            "            signal = rng.normal(0, 1, n_features) + np.sin(np.linspace(0, 16*np.pi, n_features)) * 1.0\n",
            "            label = 1\n",
            "        X.append(signal)\n",
            "        y.append(label)\n",
            "    return np.array(X), np.array(y)\n",
            "\n",
            "def extract_mfcc_like_features(X, n_mfcc=13):\n",
            "    \"\"\"Extract MFCC-like features using DCT on log-spectrogram-like data.\"\"\"\n",
            "    # Simulate log-spectrogram: take magnitude of FFT and add small epsilon before log\n",
            "    spectrogram = np.abs(np.fft.fft(X, axis=1)[:, :X.shape[1]//2])\n",
            "    log_spec = np.log(spectrogram + 1e-10)\n",
            "    # Apply DCT along the feature axis to get MFCC-like coefficients\n",
            "    mfcc_features = dct(log_spec, type=2, axis=1, norm='ortho')[:, :n_mfcc]\n",
            "    return mfcc_features\n",
            "\n",
            "def main():\n",
            "    parser = argparse.ArgumentParser(description=\"Train audio classifier on synthetic 1D audio with MFCC-like features.\")\n",
            "    parser.add_argument(\"--test-size\", type=float, default=0.2, help=\"Fraction of data to use for testing.\")\n",
            "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed for reproducibility.\")\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    # Set seeds inside main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # Generate synthetic audio data\n",
            "    X_raw, y = generate_synthetic_audio_samples(n_samples=250, seed=args.seed)\n",
            "    if X_raw.size == 0 or y.size == 0:\n",
            "        print(\"TEST_FAIL: failed to generate synthetic audio data\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Extract MFCC-like features\n",
            "    try:\n",
            "        X_features = extract_mfcc_like_features(X_raw)\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: feature extraction failed: {e}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Train/test split\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        X_features, y, test_size=args.test_size, random_state=args.seed, stratify=y\n",
            "    )\n",
            "\n",
            "    # Train classifier\n",
            "    clf = LogisticRegression(max_iter=300, random_state=args.seed)\n",
            "    clf.fit(X_train, y_train)\n",
            "\n",
            "    # Predict and evaluate\n",
            "    y_pred = clf.predict(X_test)\n",
            "    acc = accuracy_score(y_test, y_pred)\n",
            "\n",
            "    print(f\"accuracy={acc:.3f}\")\n",
            "    if acc >= 0.75:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: accuracy below 0.75 threshold\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- qwen/qwen3-coder • Project 8: FashionMNIST KMeans --- Latency: 9.02s ---\n",
            "\n",
            "import argparse, sys\n",
            "import random, numpy as np\n",
            "\n",
            "# Seed block\n",
            "random.seed(42)\n",
            "np.random.seed(42)\n",
            "try:\n",
            "    import torch\n",
            "    torch.manual_seed(42)\n",
            "except Exception:\n",
            "    pass\n",
            "\n",
            "def load_fashion_mnist_or_fakedata(max_samples=2000, seed=42, allow_download=False):\n",
            "    try:\n",
            "        import torch\n",
            "        from torchvision import datasets, transforms\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        # Only attempt download if explicitly allowed\n",
            "        if allow_download:\n",
            "            train = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=tfm)\n",
            "            data = torch.utils.data.Subset(train, list(range(min(len(train), max_samples))))\n",
            "            X = torch.stack([x[0] for x in data])\n",
            "            return X.view(len(X), -1).numpy(), \"fashionmnist\"\n",
            "        else:\n",
            "            raise Exception(\"Download not allowed\")\n",
            "    except Exception:\n",
            "        # Fallback to FakeData with matching shape\n",
            "        from torchvision import transforms\n",
            "        from torchvision.datasets import FakeData\n",
            "        import torch\n",
            "        torch.manual_seed(seed)\n",
            "        tfm = transforms.ToTensor()\n",
            "        fake = FakeData(size=max_samples, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
            "        X = torch.stack([x[0] for x in fake])\n",
            "        return X.view(len(X), -1).numpy(), \"fakedata\"\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Apply KMeans (k=10) to FashionMNIST (or FakeData) grayscale images; compute inertia.\")\n",
            "    p.add_argument(\"--samples\", type=int, default=1000, help=\"Number of samples to use (default: 1000).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Allow FashionMNIST download if not cached.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # Set seeds inside main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    # Load dataset\n",
            "    X, source = load_fashion_mnist_or_fakedata(max_samples=args.samples, seed=args.seed, allow_download=args.allow_download)\n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset could not be loaded\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # Apply KMeans\n",
            "    try:\n",
            "        from sklearn.cluster import KMeans\n",
            "        kmeans = KMeans(n_clusters=10, random_state=args.seed, n_init=3)\n",
            "        kmeans.fit(X)\n",
            "        inertia = kmeans.inertia_\n",
            "        print(f\"inertia={inertia:.1f} dataset={source}\")\n",
            "        if inertia <= 50000:\n",
            "            print(\"TEST_PASS\")\n",
            "        else:\n",
            "            print(\"TEST_FAIL: inertia above threshold\")\n",
            "            sys.exit(1)\n",
            "    except Exception as e:\n",
            "        print(f\"TEST_FAIL: KMeans failed with error: {str(e)}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- qwen/qwen3-coder • Project 9: Iris Decision Tree --- Latency: 9.24s ---\n",
            "\n",
            "import argparse, sys\n",
            "import random, numpy as np\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "from sklearn.datasets import load_iris\n",
            "\n",
            "def load_iris_or_synthetic(seed=42):\n",
            "    try:\n",
            "        data = load_iris()\n",
            "        X, y = data.data, data.target\n",
            "        used = \"iris\"\n",
            "    except Exception:\n",
            "        rng = np.random.default_rng(seed)\n",
            "        n = 150\n",
            "        c = rng.integers(0, 3, size=n)\n",
            "        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\n",
            "        y = c\n",
            "        used = \"synthetic\"\n",
            "    return X, y, used\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train DecisionTreeClassifier on iris or synthetic data; print accuracy and depth.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y, used = load_iris_or_synthetic(args.seed)\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\n",
            "    clf = DecisionTreeClassifier(max_depth=5, random_state=args.seed)\n",
            "    clf.fit(Xtr, ytr)\n",
            "    acc = clf.score(Xte, yte)\n",
            "    depth = clf.get_depth()\n",
            "    print(f\"dataset={used} accuracy={acc:.3f} depth={depth}\")\n",
            "\n",
            "    if acc >= 0.9 and depth <= 5:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: accuracy < 0.9 or depth > 5\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- qwen/qwen3-coder • Project 10: Make Blobs GMM --- Latency: 10.35s ---\n",
            "\n",
            "import argparse, sys\n",
            "import random, numpy as np\n",
            "from sklearn.mixture import GaussianMixture\n",
            "from sklearn.datasets import make_blobs\n",
            "\n",
            "# Seed block\n",
            "random.seed(42)\n",
            "np.random.seed(42)\n",
            "try:\n",
            "    import torch\n",
            "    torch.manual_seed(42)\n",
            "except Exception:\n",
            "    pass\n",
            "\n",
            "def load_blobs_or_synthetic(n_samples=200, centers=4, seed=42):\n",
            "    try:\n",
            "        X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=2, random_state=seed)\n",
            "        return X, y, \"blobs\"\n",
            "    except Exception as e:\n",
            "        # Fallback synthetic: same structure\n",
            "        rng = np.random.default_rng(seed)\n",
            "        center_coords = rng.uniform(-10, 10, size=(centers, 2))\n",
            "        X = []\n",
            "        y = []\n",
            "        samples_per_center = n_samples // centers\n",
            "        for i, center in enumerate(center_coords):\n",
            "            pts = rng.normal(loc=center, scale=1.0, size=(samples_per_center, 2))\n",
            "            X.append(pts)\n",
            "            y.append(np.full(samples_per_center, i))\n",
            "        X = np.vstack(X)\n",
            "        y = np.hstack(y)\n",
            "        return X, y, \"synthetic\"\n",
            "\n",
            "def main():\n",
            "    parser = argparse.ArgumentParser(description=\"Fit GaussianMixture on make_blobs (n=200, centers=4) or synthetic fallback; print AIC; accept if AIC <= 1000.\")\n",
            "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    # Set seeds inside main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y, source = load_blobs_or_synthetic(n_samples=200, centers=4, seed=args.seed)\n",
            "    if X is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: failed to load/generate dataset\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    gmm = GaussianMixture(n_components=4, random_state=args.seed)\n",
            "    gmm.fit(X)\n",
            "    aic = gmm.aic(X)\n",
            "    print(f\"dataset={source} AIC={aic:.2f}\")\n",
            "\n",
            "    if aic <= 1000:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: AIC > 1000\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            ">>> qwen/qwen3-coder • JSON with title, description, code:\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Digits PCA Visualization\",\n",
            "    \"description\": \"Load sklearn's digits dataset, apply PCA to reduce to 2D, and save a scatter plot as 'digits_pca.png'. Print TEST_PASS if the file exists and is non-empty.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport os\\nimport random\\nimport numpy as np\\nfrom sklearn.decomposition import PCA\\nimport matplotlib.pyplot as plt\\n\\n# Seed block\\nrandom.seed(42)\\nnp.random.seed(42)\\ntry:\\n    import torch\\n    torch.manual_seed(42)\\nexcept Exception:\\n    pass\\n\\ndef load_digits_or_synthetic(seed=42):\\n    try:\\n        from sklearn.datasets import load_digits\\n        data = load_digits()\\n        X, y = data.data, data.target\\n        used = \\\"digits\\\"\\n    except Exception:\\n        # Fallback: synthetic 8x8-like grayscale data (64 features), 10 classes\\n        rng = np.random.default_rng(seed)\\n        n_samples = 300\\n        n_features = 64\\n        X = rng.integers(0, 16, size=(n_samples, n_features)).astype(np.float32)\\n        y = rng.integers(0, 10, size=n_samples)\\n        used = \\\"synthetic\\\"\\n    return X, y, used\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\\\"Apply PCA to sklearn digits (or synthetic) and save 2D scatter plot.\\\")\\n    parser.add_argument(\\\"--output\\\", type=str, default=\\\"digits_pca.png\\\", help=\\\"Output plot filename (default: digits_pca.png).\\\")\\n    parser.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = parser.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y, dataset_type = load_digits_or_synthetic(seed=args.seed)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset could not be loaded\\\")\\n        sys.exit(1)\\n\\n    # PCA to 2D\\n    pca = PCA(n_components=2)\\n    X_pca = pca.fit_transform(X)\\n\\n    # Plot\\n    plt.figure(figsize=(8, 6))\\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\\\"tab10\\\", s=5)\\n    plt.colorbar(scatter, ticks=range(10))\\n    plt.title(f\\\"PCA of {dataset_type.capitalize()} Dataset\\\")\\n    plt.xlabel(\\\"PC1\\\")\\n    plt.ylabel(\\\"PC2\\\")\\n    plt.tight_layout()\\n    plt.savefig(args.output)\\n    plt.close()\\n\\n    # Acceptance check\\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: output file missing or empty\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Cluster Silhouette\",\n",
            "    \"description\": \"Use sklearn's wine dataset to perform KMeans clustering (k=3), compute the average silhouette score, and print it. Print TEST_PASS if the score is ≥ 0.4.\",\n",
            "    \"code\": \"import argparse, sys\\nimport random, numpy as np\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import silhouette_score\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef load_wine_or_synthetic(seed=42):\\n    try:\\n        from sklearn.datasets import load_wine\\n        data = load_wine()\\n        X, used = data.data, \\\"wine\\\"\\n    except Exception:\\n        rng = np.random.default_rng(seed)\\n        n, d, k = 200, 13, 3\\n        X = rng.normal(0, 1, size=(n, d))\\n        y = rng.integers(0, k, size=n)\\n        for i in range(k):\\n            X[y == i] += rng.normal(0, 1, size=d) * 3\\n        used = \\\"synthetic\\\"\\n    return X, used\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Cluster wine dataset (or synthetic) with KMeans and compute silhouette score.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, used = load_wine_or_synthetic(args.seed)\\n    if X is None or len(X) \n",
            "== 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X)\\n\\n    kmeans = KMeans(n_clusters=3, random_state=args.seed, n_init=10)\\n    labels = kmeans.fit_predict(X_scaled)\\n\\n    score = silhouette_score(X_scaled, labels)\\n    print(f\\\"dataset={used} silhouette={score:.3f}\\\")\\n\\n    if score >= 0.4:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: silhouette score below 0.4\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer SVM\",\n",
            "    \"description\": \"Train an SVM classifier on sklearn's breast_cancer dataset, split 80/20, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\",\n",
            "    \"code\": \"import argparse, sys\\nimport random, numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score\\n\\ndef load_breast_cancer_or_synthetic(seed=42):\\n    try:\\n        from sklearn.datasets import load_breast_cancer\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n        used = \\\"breast_cancer\\\"\\n    except Exception:\\n        # Generate synthetic dataset matching breast cancer dims: 569x30 -> ~200x30\\n        rng = np.random.default_rng(seed)\\n        n_samples, n_features = 200, 30\\n        X = rng.standard_normal((n_samples, n_features))\\n        y = rng.integers(0, 2, size=n_samples)\\n        used = \\\"synthetic\\\"\\n    return X, y, used\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\\\"Train SVM on sklearn breast_cancer or synthetic fallback; seeds in main; explicit acceptance.\\\")\\n    parser.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Fraction of data to use for testing (default: 0.2).\\\")\\n    parser.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed for reproducibility (default: 42).\\\")\\n    args = parser.parse_args()\\n\\n    # Set seeds inside main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except ImportError:\\n        pass\\n\\n    # Load dataset\\n    X, y, dataset_name = load_breast_cancer_or_synthetic(seed=args.seed)\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: unable to load dataset\\\")\\n        sys.exit(1)\\n\\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # Scale features\\n    scaler = StandardScaler()\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_test_scaled = scaler.transform(X_test)\\n\\n    # Train SVM\\n    svm = SVC(kernel='rbf', random_state=args.seed)\\n    svm.fit(X_train_scaled, y_train)\\n\\n    # Predict and evaluate\\n    y_pred = svm.predict(X_test_scaled)\\n    acc = accuracy_score(y_test, y_pred)\\n\\n    print(f\\\"dataset={dataset_name} accuracy={acc:.4f}\\\")\\n\\n    # Acceptance threshold: stricter for real data\\n    threshold = 0.92 if dataset_name == \\\"breast_cancer\\\" else 0.80\\n    if acc >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy below {threshold}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Linear Regression\",\n",
            "    \"description\": \"Fit a linear regression model on sklearn's diabetes dataset, print the R² score on test split. Print TEST_PASS if R² ≥ 0.4.\",\n",
            "    \"code\": \"import argparse, sys\\nimport random, numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\n\\ndef load_diabetes_or_synthetic(seed=42):\\n    try:\\n        from sklearn.datasets import load_diabetes\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n        used = \\\"diabetes\\\"\\n    exce\n",
            "pt Exception:\\n        rng = np.random.default_rng(seed)\\n        n = 200\\n        X = rng.normal(0, 1, size=(n, 10))\\n        y = X.sum(axis=1) + rng.normal(0, 0.1, size=n)\\n        used = \\\"synthetic\\\"\\n    return X, y, used\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Fit linear regression on sklearn diabetes (no-download) or synthetic fallback.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Fraction of data to use for testing (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed for reproducibility (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y, used = load_diabetes_or_synthetic(seed=args.seed)\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: failed to load dataset\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\\n    model = LinearRegression()\\n    model.fit(Xtr, ytr)\\n    y_pred = model.predict(Xte)\\n    r2 = r2_score(yte, y_pred)\\n    print(f\\\"dataset={used} r2={r2:.3f}\\\")\\n\\n    if r2 >= 0.4:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: R2 score below 0.4\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST CNN Classifier\",\n",
            "    \"description\": \"Train a small CNN on MNIST (or FakeData if unavailable), print test accuracy after 5 epochs. Print TEST_PASS if accuracy ≥ 0.9.\",\n",
            "    \"code\": \"import argparse, sys\\nimport random, numpy as np\\n\\nrandom.seed(42)\\nnp.random.seed(42)\\ntry:\\n    import torch\\n    torch.manual_seed(42)\\nexcept Exception:\\n    pass\\n\\ndef load_mnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = datasets.MNIST(root=\\\"./data\\\", train=True, download=bool(allow_download), transform=tfm)\\n        test = datasets.MNIST(root=\\\"./data\\\", train=False, download=bool(allow_download), transform=tfm)\\n        if len(train) == 0 or len(test) == 0:\\n            raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\\n        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\\n        return train, test, True\\n    except Exception:\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        test = FakeData(size=max_test, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        return train, test, False\\n\\ndef main():\\n    import torch\\n    import torch.nn as nn\\n    import torch.optim as optim\\n    from torch.utils.data import DataLoader\\n\\n    p = argparse.ArgumentParser(description=\\\"Train CNN on MNIST (or FakeData); print TEST_PASS if test acc ≥ 0.9.\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=5, help=\\\"Number of training epochs (default: 5).\\\")\\n    p.add_argument(\\\"--batch-size\\\", type=int, default=128, help=\\\"Training batch size (default: 128).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Allow downloading MNIST if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    torch.manual_seed(args.seed)\\n\\n    train_ds, test_ds, is_real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\\n    train_loader = DataLoader(train_ds, batch_size=arg\n",
            "s.batch_size, shuffle=True)\\n    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False)\\n\\n    class SimpleCNN(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.conv1 = nn.Conv2d(1, 16, 3, 1)\\n            self.conv2 = nn.Conv2d(16, 32, 3, 1)\\n            self.pool = nn.MaxPool2d(2)\\n            self.fc1 = nn.Linear(32 * 5 * 5, 64)\\n            self.fc2 = nn.Linear(64, 10)\\n\\n        def forward(self, x):\\n            x = self.pool(torch.relu(self.conv1(x)))\\n            x = self.pool(torch.relu(self.conv2(x)))\\n            x = torch.flatten(x, 1)\\n            x = torch.relu(self.fc1(x))\\n            x = self.fc2(x)\\n            return x\\n\\n    model = SimpleCNN()\\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\\n    criterion = nn.CrossEntropyLoss()\\n\\n    model.train()\\n    for epoch in range(args.epochs):\\n        for xb, yb in train_loader:\\n            optimizer.zero_grad()\\n            output = model(xb)\\n            loss = criterion(output, yb)\\n            loss.backward()\\n            optimizer.step()\\n\\n    model.eval()\\n    correct = total = 0\\n    with torch.no_grad():\\n        for xb, yb in test_loader:\\n            output = model(xb)\\n            pred = output.argmax(dim=1)\\n            correct += (pred == yb).sum().item()\\n            total += yb.size(0)\\n\\n    acc = correct / total if total > 0 else 0\\n    print(f\\\"Test Accuracy: {acc:.4f} (dataset: {'MNIST' if is_real else 'FakeData'})\\\")\\n\\n    if acc >= 0.9:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.9 threshold\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Moons DBSCAN\",\n",
            "    \"description\": \"Generate sklearn's make_moons dataset (n=300), apply DBSCAN clustering, and print the number of clusters found. Print TEST_PASS if clusters ≥ 2.\",\n",
            "    \"code\": \"import argparse, sys\\nimport random, numpy as np\\nfrom sklearn.cluster import DBSCAN\\nfrom sklearn.datasets import make_moons\\n\\n# Seed block\\nrandom.seed(42)\\nnp.random.seed(42)\\ntry:\\n    import torch\\n    torch.manual_seed(42)\\nexcept Exception:\\n    pass\\n\\ndef load_moons_or_synthetic(n_samples=300, seed=42):\\n    \\\"\\\"\\\"Load make_moons data or fallback to synthetic if error.\\\"\\\"\\\"\\n    try:\\n        X, _ = make_moons(n_samples=n_samples, noise=0.1, random_state=seed)\\n        return X, \\\"moons\\\"\\n    except Exception:\\n        # Synthetic fallback matching moon shape roughly\\n        rng = np.random.default_rng(seed)\\n        t = rng.uniform(0, 2*np.pi, n_samples//2)\\n        dx, dy = 0.5, 0.25\\n        x1 = np.cos(t) - dx; y1 = np.sin(t) + dy\\n        x2 = np.cos(t) + dx; y2 = np.sin(t) - dy\\n        X = np.vstack([np.column_stack([x1, y1]), np.column_stack([x2, y2])])\\n        return X, \\\"synthetic\\\"\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\\\"Apply DBSCAN to make_moons (or synthetic) data; print TEST_PASS if clusters ≥ 2.\\\")\\n    parser.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    parser.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples (default: 300).\\\")\\n    parser.add_argument(\\\"--eps\\\", type=float, default=0.2, help=\\\"DBSCAN eps parameter (default: 0.2).\\\")\\n    parser.add_argument(\\\"--min-samples\\\", type=int, default=5, help=\\\"DBSCAN min_samples parameter (default: 5).\\\")\\n    args = parser.parse_args()\\n\\n    # Set seeds inside main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except ImportError:\\n        pass\\n\\n    X, source = load_moons_or_synthetic(n_samples=args.n_samples, seed=args.seed)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: failed to load dataset\\\")\\n        sys.exit(1)\\n\\n    # Apply DBSCAN\\n    dbscan = DBSCAN(eps=args.eps, min_samples=args.min_samples)\\n    labels = dbscan.fit_predict(X)\\n    n_clusters = len(s\n",
            "et(labels)) - (1 if -1 in labels else 0)\\n\\n    print(f\\\"dataset={source} clusters={n_clusters}\\\")\\n    if n_clusters >= 2:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: fewer than 2 clusters found\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Audio Classifier\",\n",
            "    \"description\": \"Generate 250 synthetic 1D audio-like samples with 2 classes, extract MFCC-like features, train a LogisticRegression, and print accuracy. Print TEST_PASS if accuracy ≥ 0.75.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\nfrom scipy.fftpack import dct\\n\\n# Seed block\\nrandom.seed(42)\\nnp.random.seed(42)\\ntry:\\n    import torch\\n    torch.manual_seed(42)\\nexcept Exception:\\n    pass\\n\\ndef generate_synthetic_audio_samples(n_samples=250, n_features=100, seed=42):\\n    \\\"\\\"\\\"Generate synthetic 1D audio-like data with 2 classes.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    X = []\\n    y = []\\n    for i in range(n_samples):\\n        # Class 0: low frequency dominant\\n        if i % 2 == 0:\\n            signal = rng.normal(0, 1, n_features) + np.sin(np.linspace(0, 4*np.pi, n_features)) * 1.5\\n            label = 0\\n        # Class 1: high frequency dominant\\n        else:\\n            signal = rng.normal(0, 1, n_features) + np.sin(np.linspace(0, 16*np.pi, n_features)) * 1.0\\n            label = 1\\n        X.append(signal)\\n        y.append(label)\\n    return np.array(X), np.array(y)\\n\\ndef extract_mfcc_like_features(X, n_mfcc=13):\\n    \\\"\\\"\\\"Extract MFCC-like features using DCT on log-spectrogram-like data.\\\"\\\"\\\"\\n    # Simulate log-spectrogram: take magnitude of FFT and add small epsilon before log\\n    spectrogram = np.abs(np.fft.fft(X, axis=1)[:, :X.shape[1]//2])\\n    log_spec = np.log(spectrogram + 1e-10)\\n    # Apply DCT along the feature axis to get MFCC-like coefficients\\n    mfcc_features = dct(log_spec, type=2, axis=1, norm='ortho')[:, :n_mfcc]\\n    return mfcc_features\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\\\"Train audio classifier on synthetic 1D audio with MFCC-like features.\\\")\\n    parser.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Fraction of data to use for testing.\\\")\\n    parser.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed for reproducibility.\\\")\\n    args = parser.parse_args()\\n\\n    # Set seeds inside main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Generate synthetic audio data\\n    X_raw, y = generate_synthetic_audio_samples(n_samples=250, seed=args.seed)\\n    if X_raw.size == 0 or y.size == 0:\\n        print(\\\"TEST_FAIL: failed to generate synthetic audio data\\\")\\n        sys.exit(1)\\n\\n    # Extract MFCC-like features\\n    try:\\n        X_features = extract_mfcc_like_features(X_raw)\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: feature extraction failed: {e}\\\")\\n        sys.exit(1)\\n\\n    # Train/test split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X_features, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # Train classifier\\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    # Predict and evaluate\\n    y_pred = clf.predict(X_test)\\n    acc = accuracy_score(y_test, y_pred)\\n\\n    print(f\\\"accuracy={acc:.3f}\\\")\\n    if acc >= 0.75:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.75 threshold\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"FashionMNIST KMeans\",\n",
            "    \"description\": \"Apply KMeans (k=10) to FashionMNIST grayscale images (or FakeData), c\n",
            "ompute inertia, and print it. Print TEST_PASS if inertia ≤ 50000.\",\n",
            "    \"code\": \"import argparse, sys\\nimport random, numpy as np\\n\\n# Seed block\\nrandom.seed(42)\\nnp.random.seed(42)\\ntry:\\n    import torch\\n    torch.manual_seed(42)\\nexcept Exception:\\n    pass\\n\\ndef load_fashion_mnist_or_fakedata(max_samples=2000, seed=42, allow_download=False):\\n    try:\\n        import torch\\n        from torchvision import datasets, transforms\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        # Only attempt download if explicitly allowed\\n        if allow_download:\\n            train = datasets.FashionMNIST(root=\\\"./data\\\", train=True, download=True, transform=tfm)\\n            data = torch.utils.data.Subset(train, list(range(min(len(train), max_samples))))\\n            X = torch.stack([x[0] for x in data])\\n            return X.view(len(X), -1).numpy(), \\\"fashionmnist\\\"\\n        else:\\n            raise Exception(\\\"Download not allowed\\\")\\n    except Exception:\\n        # Fallback to FakeData with matching shape\\n        from torchvision import transforms\\n        from torchvision.datasets import FakeData\\n        import torch\\n        torch.manual_seed(seed)\\n        tfm = transforms.ToTensor()\\n        fake = FakeData(size=max_samples, image_size=(1, 28, 28), num_classes=10, transform=tfm)\\n        X = torch.stack([x[0] for x in fake])\\n        return X.view(len(X), -1).numpy(), \\\"fakedata\\\"\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply KMeans (k=10) to FashionMNIST (or FakeData) grayscale images; compute inertia.\\\")\\n    p.add_argument(\\\"--samples\\\", type=int, default=1000, help=\\\"Number of samples to use (default: 1000).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Allow FashionMNIST download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds inside main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Load dataset\\n    X, source = load_fashion_mnist_or_fakedata(max_samples=args.samples, seed=args.seed, allow_download=args.allow_download)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset could not be loaded\\\")\\n        sys.exit(1)\\n\\n    # Apply KMeans\\n    try:\\n        from sklearn.cluster import KMeans\\n        kmeans = KMeans(n_clusters=10, random_state=args.seed, n_init=3)\\n        kmeans.fit(X)\\n        inertia = kmeans.inertia_\\n        print(f\\\"inertia={inertia:.1f} dataset={source}\\\")\\n        if inertia <= 50000:\\n            print(\\\"TEST_PASS\\\")\\n        else:\\n            print(\\\"TEST_FAIL: inertia above threshold\\\")\\n            sys.exit(1)\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: KMeans failed with error: {str(e)}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Decision Tree\",\n",
            "    \"description\": \"Train a DecisionTreeClassifier on sklearn's iris dataset, print test accuracy and tree depth. Print TEST_PASS if accuracy ≥ 0.9 and depth ≤ 5.\",\n",
            "    \"code\": \"import argparse, sys\\nimport random, numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.datasets import load_iris\\n\\ndef load_iris_or_synthetic(seed=42):\\n    try:\\n        data = load_iris()\\n        X, y = data.data, data.target\\n        used = \\\"iris\\\"\\n    except Exception:\\n        rng = np.random.default_rng(seed)\\n        n = 150\\n        c = rng.integers(0, 3, size=n)\\n        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\\n        y = c\\n        used = \\\"synthetic\\\"\\n    return X, y, used\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train DecisionTreeClassifier on iris or synthetic data; print accuracy and depth.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=f\n",
            "loat, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y, used = load_iris_or_synthetic(args.seed)\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\\n    clf = DecisionTreeClassifier(max_depth=5, random_state=args.seed)\\n    clf.fit(Xtr, ytr)\\n    acc = clf.score(Xte, yte)\\n    depth = clf.get_depth()\\n    print(f\\\"dataset={used} accuracy={acc:.3f} depth={depth}\\\")\\n\\n    if acc >= 0.9 and depth <= 5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy < 0.9 or depth > 5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Make Blobs GMM\",\n",
            "    \"description\": \"Generate sklearn's make_blobs dataset (n=200, centers=4), fit a GaussianMixture model, and print AIC score. Print TEST_PASS if AIC ≤ 1000.\",\n",
            "    \"code\": \"import argparse, sys\\nimport random, numpy as np\\nfrom sklearn.mixture import GaussianMixture\\nfrom sklearn.datasets import make_blobs\\n\\n# Seed block\\nrandom.seed(42)\\nnp.random.seed(42)\\ntry:\\n    import torch\\n    torch.manual_seed(42)\\nexcept Exception:\\n    pass\\n\\ndef load_blobs_or_synthetic(n_samples=200, centers=4, seed=42):\\n    try:\\n        X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=2, random_state=seed)\\n        return X, y, \\\"blobs\\\"\\n    except Exception as e:\\n        # Fallback synthetic: same structure\\n        rng = np.random.default_rng(seed)\\n        center_coords = rng.uniform(-10, 10, size=(centers, 2))\\n        X = []\\n        y = []\\n        samples_per_center = n_samples // centers\\n        for i, center in enumerate(center_coords):\\n            pts = rng.normal(loc=center, scale=1.0, size=(samples_per_center, 2))\\n            X.append(pts)\\n            y.append(np.full(samples_per_center, i))\\n        X = np.vstack(X)\\n        y = np.hstack(y)\\n        return X, y, \\\"synthetic\\\"\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\\\"Fit GaussianMixture on make_blobs (n=200, centers=4) or synthetic fallback; print AIC; accept if AIC <= 1000.\\\")\\n    parser.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = parser.parse_args()\\n\\n    # Set seeds inside main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y, source = load_blobs_or_synthetic(n_samples=200, centers=4, seed=args.seed)\\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: failed to load/generate dataset\\\")\\n        sys.exit(1)\\n\\n    gmm = GaussianMixture(n_components=4, random_state=args.seed)\\n    gmm.fit(X)\\n    aic = gmm.aic(X)\\n    print(f\\\"dataset={source} AIC={aic:.2f}\\\")\\n\\n    if aic <= 1000:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: AIC > 1000\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(anthropic_claude_sonnet_4_5_result[\"items_with_code\"], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "quVj0XmEp_95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b4ae2e-f0a4-4433-a07a-e756444d7bfa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Digits SVM Classifier\",\n",
            "    \"description\": \"Load sklearn's digits dataset, train a linear SVM, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.95.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train a linear SVM on sklearn digits dataset and print test accuracy.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Load digits dataset (no download required)\\n    try:\\n        data = load_digits()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load digits dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: digits dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # Split data\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # Build pipeline with scaler and linear SVM\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"svm\\\", LinearSVC(max_iter=2000, random_state=args.seed))\\n    ])\\n\\n    # Train\\n    clf.fit(X_train, y_train)\\n\\n    # Evaluate\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {acc:.4f}\\\")\\n\\n    # Acceptance check\\n    if acc >= 0.95:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.95\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Random Forest\",\n",
            "    \"description\": \"Load sklearn's wine dataset, train a RandomForestClassifier with 50 trees, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.92.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_wine\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train RandomForestClassifier on sklearn wine dataset; print TEST_PASS if accuracy >= 0.92.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--n-estimators\\\", type=int, default=50, help=\\\"Number of trees in forest (default: 50).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # load wine dataset (no download required)\\n    try:\\n        data = load_wine()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load wine dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: wine dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train RandomForest\\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    # evaluate\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {acc:.4f}\\\")\\n\\n    # acceptance check\\n    if acc >= 0.92:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.4f} < 0.92\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"K-Means on Blobs\",\n",
            "    \"description\": \"Generate 300 samples with make_blobs (3 clusters), run KMeans (k=3), and print silhouette score. Print TEST_PASS if silhouette ≥ 0.5.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import silhouette_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"K-Means clustering on synthetic blobs; print silhouette score and TEST_PASS if >= 0.5.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--n-clusters\\\", type=int, default=3, help=\\\"Number of clusters (default: 3).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples < 1:\\n        print(\\\"TEST_FAIL: n_samples must be >= 1\\\")\\n        sys.exit(1)\\n    if args.n_clusters < 2:\\n        print(\\\"TEST_FAIL: n_clusters must be >= 2\\\")\\n        sys.exit(1)\\n\\n    X, y_true = make_blobs(n_samples=args.n_samples, centers=args.n_clusters, random_state=args.seed)\\n    \\n    if X is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    kmeans = KMeans(n_clusters=args.n_clusters, random_state=args.seed, n_init=10)\\n    labels = kmeans.fit_predict(X)\\n\\n    sil = silhouette_score(X, labels)\\n    print(f\\\"n_samples={args.n_samples} n_clusters={args.n_clusters} silhouette={sil:.3f}\\\")\\n\\n    if sil >= 0.5:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: silhouette score {sil:.3f} < 0.5\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Diabetes Linear Regression\",\n",
            "    \"description\": \"Load sklearn's diabetes dataset, train a LinearRegression model, and print test R² score. Print TEST_PASS if R² ≥ 0.4.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train LinearRegression on sklearn diabetes dataset and print test R² score.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # load diabetes dataset (no download required)\\n    try:\\n        data = load_diabetes()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load diabetes dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed\\n    )\\n\\n    # train\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    # predict and evaluate\\n    y_pred = model.predict(X_test)\\n    r2 = r2_score(y_test, y_pred)\\n\\n    print(f\\\"R² score: {r2:.4f}\\\")\\n\\n    # acceptance: R² >= 0.4\\n    if r2 >= 0.4:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: R² score {r2:.4f} below threshold 0.4\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST Logistic Regression\",\n",
            "    \"description\": \"Load MNIST (or generate 1000 synthetic 28×28 grayscale images if unavailable), flatten pixels, train LogisticRegression, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.85.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef load_mnist_or_synthetic(seed=42, allow_download=False):\\n    \\\"\\\"\\\"Load MNIST if available/allowed, else generate 1000 synthetic 28x28 grayscale images.\\\"\\\"\\\"\\n    try:\\n        from torchvision import datasets, transforms\\n        import torch\\n        torch.manual_seed(seed)\\n        transform = transforms.Compose([transforms.ToTensor()])\\n        train_ds = datasets.MNIST(root='./data', train=True, download=bool(allow_download), transform=transform)\\n        test_ds = datasets.MNIST(root='./data', train=False, download=bool(allow_download), transform=transform)\\n        if len(train_ds) == 0 or len(test_ds) == 0:\\n            raise RuntimeError(\\\"MNIST cache missing and download disabled\\\")\\n        X_train = train_ds.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0\\n        y_train = train_ds.targets.numpy()\\n        X_test = test_ds.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0\\n        y_test = test_ds.targets.numpy()\\n        return X_train, y_train, X_test, y_test, True\\n    except Exception:\\n        rng = np.random.default_rng(seed)\\n        n_train = 800\\n        n_test = 200\\n        X_train = rng.random((n_train, 28*28), dtype=np.float32)\\n        y_train = rng.integers(0, 10, size=n_train)\\n        X_test = rng.random((n_test, 28*28), dtype=np.float32)\\n        y_test = rng.integers(0, 10, size=n_test)\\n        return X_train, y_train, X_test, y_test, False\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"MNIST logistic regression with fallback to synthetic 28x28 images.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--allow-download\\\", action=\\\"store_true\\\", help=\\\"Permit MNIST download if not cached.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X_train, y_train, X_test, y_test, is_real = load_mnist_or_synthetic(seed=args.seed, allow_download=args.allow_download)\\n\\n    if X_train is None or len(X_train) == 0 or X_test is None or len(X_test) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    from sklearn.linear_model import LogisticRegression\\n    clf = LogisticRegression(max_iter=100, solver='lbfgs', random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n    acc = clf.score(X_test, y_test)\\n\\n    dataset_name = \\\"MNIST\\\" if is_real else \\\"synthetic\\\"\\n    print(f\\\"dataset={dataset_name} test_accuracy={acc:.3f}\\\")\\n\\n    if acc >= 0.85:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} < 0.85\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Decision Tree\",\n",
            "    \"description\": \"Load sklearn's breast_cancer dataset, train a DecisionTreeClassifier (max_depth=5), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train DecisionTreeClassifier on breast_cancer dataset; print TEST_PASS if accuracy >= 0.90.\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--max-depth\\\", type=int, default=5, help=\\\"Max depth of decision tree (default: 5).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # load breast_cancer dataset\\n    try:\\n        data = load_breast_cancer()\\n        X, y = data.data, data.target\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load breast_cancer dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # train/test split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # train decision tree\\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\\n    clf.fit(X_train, y_train)\\n\\n    # evaluate\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {acc:.3f}\\\")\\n\\n    # acceptance check\\n    if acc >= 0.90:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} < 0.90\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Moons Neural Network\",\n",
            "    \"description\": \"Generate 400 samples with make_moons, train a 2-layer PyTorch MLP (10 hidden units), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.85.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 400 samples with make_moons, train a 2-layer PyTorch MLP (10 hidden units), print test accuracy.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--epochs\\\", type=int, default=100, help=\\\"Training epochs (default: 100).\\\")\\n    p.add_argument(\\\"--lr\\\", type=float, default=0.01, help=\\\"Learning rate (default: 0.01).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # generate dataset\\n    try:\\n        from sklearn.datasets import make_moons\\n        from sklearn.model_selection import train_test_split\\n        import torch\\n        import torch.nn as nn\\n        import torch.optim as optim\\n    except ImportError as e:\\n        print(f\\\"TEST_FAIL: missing dependency {e}\\\")\\n        sys.exit(1)\\n\\n    X, y = make_moons(n_samples=400, noise=0.1, random_state=args.seed)\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # convert to torch tensors\\n    X_train_t = torch.tensor(X_train, dtype=torch.float32)\\n    y_train_t = torch.tensor(y_train, dtype=torch.long)\\n    X_test_t = torch.tensor(X_test, dtype=torch.float32)\\n    y_test_t = torch.tensor(y_test, dtype=torch.long)\\n\\n    # define 2-layer MLP with 10 hidden units\\n    class MLP(nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.fc1 = nn.Linear(2, 10)\\n            self.fc2 = nn.Linear(10, 2)\\n        \\n        def forward(self, x):\\n            x = torch.relu(self.fc1(x))\\n            x = self.fc2(x)\\n            return x\\n\\n    model = MLP()\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = optim.SGD(model.parameters(), lr=args.lr)\\n\\n    # train\\n    model.train()\\n    for epoch in range(args.epochs):\\n        optimizer.zero_grad()\\n        outputs = model(X_train_t)\\n        loss = criterion(outputs, y_train_t)\\n        loss.backward()\\n        optimizer.step()\\n\\n    # evaluate\\n    model.eval()\\n    with torch.no_grad():\\n        test_outputs = model(X_test_t)\\n        _, predicted = torch.max(test_outputs, 1)\\n        correct = (predicted == y_test_t).sum().item()\\n        total = y_test_t.size(0)\\n        accuracy = correct / max(total, 1)\\n\\n    print(f\\\"test_accuracy={accuracy:.3f}\\\")\\n\\n    # acceptance check\\n    if accuracy >= 0.85:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {accuracy:.3f} < 0.85\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Circles SVC Classifier\",\n",
            "    \"description\": \"Generate 300 samples with make_circles, train an SVC with RBF kernel, and print test accuracy. Print TEST_PASS if accuracy ≥ 0.90.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.datasets import make_circles\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 300 samples with make_circles, train SVC with RBF kernel, print test accuracy.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=300, help=\\\"Number of samples to generate (default: 300).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--noise\\\", type=float, default=0.1, help=\\\"Noise level for make_circles (default: 0.1).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Generate circles dataset\\n    X, y = make_circles(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\\n    \\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # Split into train and test\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\\n    )\\n\\n    # Build pipeline with StandardScaler and SVC with RBF kernel\\n    clf = Pipeline([\\n        (\\\"scaler\\\", StandardScaler()),\\n        (\\\"svc\\\", SVC(kernel=\\\"rbf\\\", random_state=args.seed))\\n    ])\\n\\n    # Train\\n    clf.fit(X_train, y_train)\\n\\n    # Evaluate\\n    acc = clf.score(X_test, y_test)\\n    print(f\\\"Test accuracy: {acc:.3f}\\\")\\n\\n    # Acceptance check\\n    if acc >= 0.90:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold 0.90\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Time-Series Forecasting\",\n",
            "    \"description\": \"Generate 250 synthetic time-series samples (sine wave + noise), train a simple linear model to predict next value, and print mean absolute error. Print TEST_PASS if MAE ≤ 0.3.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\n\\ndef generate_synthetic_timeseries(n_samples=250, seq_len=50, seed=42):\\n    \\\"\\\"\\\"Generate synthetic time-series: sine wave + noise.\\\"\\\"\\\"\\n    rng = np.random.default_rng(seed)\\n    X = []\\n    y = []\\n    for i in range(n_samples):\\n        # random frequency and phase\\n        freq = rng.uniform(0.05, 0.15)\\n        phase = rng.uniform(0, 2 * np.pi)\\n        noise_level = rng.uniform(0.05, 0.15)\\n        # generate sequence\\n        t = np.arange(seq_len + 1)\\n        series = np.sin(2 * np.pi * freq * t + phase) + rng.normal(0, noise_level, size=seq_len + 1)\\n        # use first seq_len as features, last value as target\\n        X.append(series[:seq_len])\\n        y.append(series[seq_len])\\n    return np.array(X), np.array(y)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate 250 synthetic time-series samples (sine wave + noise), train linear model, print MAE.\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=250, help=\\\"Number of synthetic time-series samples (default: 250).\\\")\\n    p.add_argument(\\\"--seq-len\\\", type=int, default=50, help=\\\"Length of each time-series sequence (default: 50).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # generate synthetic time-series\\n    X, y = generate_synthetic_timeseries(n_samples=args.n_samples, seq_len=args.seq_len, seed=args.seed)\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset generation failed\\\")\\n        sys.exit(1)\\n\\n    # split train/test\\n    from sklearn.model_selection import train_test_split\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\\n\\n    # train simple linear model\\n    from sklearn.linear_model import LinearRegression\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    # predict and compute MAE\\n    y_pred = model.predict(X_test)\\n    from sklearn.metrics import mean_absolute_error\\n    mae = mean_absolute_error(y_test, y_pred)\\n\\n    print(f\\\"n_samples={args.n_samples} seq_len={args.seq_len} MAE={mae:.4f}\\\")\\n\\n    # acceptance: MAE <= 0.3\\n    if mae <= 0.3:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: MAE {mae:.4f} > 0.3\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"PCA Visualization on Iris\",\n",
            "    \"description\": \"Load sklearn's iris dataset, apply PCA to 2 components, save a scatter plot as pca_iris.png, and print TEST_PASS if file exists.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.decomposition import PCA\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Apply PCA to iris dataset and save scatter plot as pca_iris.png.\\\")\\n    p.add_argument(\\\"--output\\\", type=str, default=\\\"pca_iris.png\\\", help=\\\"Output filename for PCA scatter plot (default: pca_iris.png).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    args = p.parse_args()\\n\\n    # Set seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    # Load iris dataset (no download required)\\n    try:\\n        data = load_iris()\\n        X = data.data\\n        y = data.target\\n        target_names = data.target_names\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to load iris dataset: {e}\\\")\\n        sys.exit(1)\\n\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: iris dataset is empty\\\")\\n        sys.exit(1)\\n\\n    # Apply PCA to 2 components\\n    pca = PCA(n_components=2, random_state=args.seed)\\n    X_pca = pca.fit_transform(X)\\n\\n    # Create scatter plot\\n    plt.figure(figsize=(8, 6))\\n    colors = ['red', 'green', 'blue']\\n    for i, color, name in zip(range(3), colors, target_names):\\n        mask = y == i\\n        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], color=color, label=name, alpha=0.7, edgecolors='k')\\n    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\\n    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\\n    plt.title('PCA of Iris Dataset')\\n    plt.legend()\\n    plt.grid(True, alpha=0.3)\\n    plt.tight_layout()\\n\\n    # Save plot\\n    try:\\n        plt.savefig(args.output, dpi=100)\\n        plt.close()\\n    except Exception as e:\\n        print(f\\\"TEST_FAIL: failed to save plot: {e}\\\")\\n        sys.exit(1)\\n\\n    # Acceptance check: verify file exists\\n    import os\\n    if not os.path.isfile(args.output):\\n        print(f\\\"TEST_FAIL: output file {args.output} does not exist\\\")\\n        sys.exit(1)\\n\\n    file_size = os.path.getsize(args.output)\\n    if file_size == 0:\\n        print(f\\\"TEST_FAIL: output file {args.output} is empty\\\")\\n        sys.exit(1)\\n\\n    print(f\\\"PCA scatter plot saved to {args.output} ({file_size} bytes)\\\")\\n    print(\\\"TEST_PASS\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** So now we have more structured and more correct project codes and descriptions that before which they are acceptable for finetuning the smaller models."
      ],
      "metadata": {
        "id": "Pnro2mqpboNm"
      }
    }
  ]
}