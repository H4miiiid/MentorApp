{
  "items_with_code": [
    {
      "title": "Iris KNN Classifier",
      "description": "Load sklearn's iris dataset, split into train/test, train a k-NN classifier (k=3), and print test accuracy. Print TEST_PASS if accuracy ≥ 0.9.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Iris KNN classifier with k=3; prints TEST_PASS if accuracy >= 0.9.\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--k\", type=int, default=3, help=\"Number of neighbors for KNN (default: 3).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    # seeds in main\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    # load iris dataset (no download required)\n    try:\n        data = load_iris()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load iris dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: iris dataset is empty\")\n        sys.exit(1)\n\n    # split into train/test\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    # train k-NN classifier\n    knn = KNeighborsClassifier(n_neighbors=args.k)\n    knn.fit(X_train, y_train)\n\n    # evaluate on test set\n    accuracy = knn.score(X_test, y_test)\n    print(f\"Test accuracy: {accuracy:.3f}\")\n\n    # acceptance check\n    if accuracy >= 0.9:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {accuracy:.3f} < 0.9\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Wine Logistic Regression",
      "description": "Use sklearn wine dataset, scale features, train logistic regression, and report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Wine dataset logistic regression with feature scaling; TEST_PASS if accuracy >= 0.92.\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_wine()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load wine dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: wine dataset is empty\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\n    clf.fit(X_train_scaled, y_train)\n\n    accuracy = clf.score(X_test_scaled, y_test)\n    print(f\"Test accuracy: {accuracy:.4f}\")\n\n    if accuracy >= 0.92:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {accuracy:.4f} < 0.92\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Breast Cancer SVM",
      "description": "Train a linear SVM on sklearn breast_cancer dataset, report test accuracy. Print TEST_PASS if accuracy ≥ 0.93.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Train linear SVM on breast_cancer dataset; report test accuracy.\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--max-iter\", type=int, default=1000, help=\"Max iterations for LinearSVC (default: 1000).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_breast_cancer()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load breast_cancer dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset is empty\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm\", LinearSVC(max_iter=args.max_iter, random_state=args.seed))\n    ])\n\n    clf.fit(X_train, y_train)\n    acc = clf.score(X_test, y_test)\n\n    print(f\"test_accuracy={acc:.4f}\")\n\n    if acc >= 0.93:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.93\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Digits Linear Classifier",
      "description": "Classify sklearn digits using LogisticRegression, report test accuracy. Print TEST_PASS if accuracy ≥ 0.92.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Classify sklearn digits using LogisticRegression; TEST_PASS if accuracy >= 0.92.\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--max-iter\", type=int, default=1000, help=\"Max iterations for LogisticRegression (default: 1000).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_digits()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: digits dataset is empty\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"lr\", LogisticRegression(max_iter=args.max_iter, random_state=args.seed))\n    ])\n\n    clf.fit(X_train, y_train)\n    acc = clf.score(X_test, y_test)\n\n    print(f\"Test accuracy: {acc:.4f}\")\n\n    if acc >= 0.92:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {acc:.4f} below threshold 0.92\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Diabetes Ridge Regression",
      "description": "Fit Ridge regression on sklearn diabetes dataset, compute R² score. Print TEST_PASS if R² ≥ 0.4.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Ridge regression on sklearn diabetes dataset; TEST_PASS if R² ≥ 0.4.\")\n    p.add_argument(\"--alpha\", type=float, default=1.0, help=\"Ridge regularization strength (default: 1.0).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_diabetes()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load diabetes dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: diabetes dataset is empty\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed\n    )\n\n    model = Ridge(alpha=args.alpha, random_state=args.seed)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    r2 = r2_score(y_test, y_pred)\n\n    print(f\"R² score: {r2:.4f}\")\n\n    if r2 >= 0.4:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: R² score {r2:.4f} below threshold 0.4\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Moons Decision Tree",
      "description": "Generate make_moons data (n=300), train decision tree, plot decision boundary, save as 'moons_tree.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate make_moons data, train decision tree, plot decision boundary, save as moons_tree.png.\")\n    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n    p.add_argument(\"--noise\", type=float, default=0.2, help=\"Noise level for make_moons (default: 0.2).\")\n    p.add_argument(\"--max-depth\", type=int, default=5, help=\"Max depth of decision tree (default: 5).\")\n    p.add_argument(\"--output\", type=str, default=\"moons_tree.png\", help=\"Output filename for decision boundary plot (default: moons_tree.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    # Set seeds in main\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    # Generate make_moons data\n    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\n    \n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    # Train decision tree\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\n    clf.fit(X, y)\n\n    # Create decision boundary plot\n    h = 0.02\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    \n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap=plt.cm.RdYlBu)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.title(\"Decision Tree Boundary on make_moons\")\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    # Acceptance check\n    import os\n    if not os.path.exists(args.output):\n        print(f\"TEST_FAIL: output file {args.output} not created\")\n        sys.exit(1)\n    \n    file_size = os.path.getsize(args.output)\n    if file_size == 0:\n        print(f\"TEST_FAIL: output file {args.output} is empty\")\n        sys.exit(1)\n\n    acc = clf.score(X, y)\n    print(f\"Training accuracy: {acc:.3f}\")\n    print(f\"Decision boundary saved to {args.output}\")\n    print(\"TEST_PASS\")\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Circles Random Forest",
      "description": "Generate make_circles data (n=300), train random forest, report accuracy. Print TEST_PASS if accuracy ≥ 0.88.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate make_circles data (n=300), train random forest, report accuracy.\")\n    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--n-estimators\", type=int, default=100, help=\"Number of trees in random forest (default: 100).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    X, y = make_circles(n_samples=args.n_samples, noise=0.1, factor=0.5, random_state=args.seed)\n    \n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\n    \n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\n    clf.fit(Xtr, ytr)\n    \n    acc = clf.score(Xte, yte)\n    print(f\"accuracy={acc:.3f}\")\n    \n    if acc >= 0.88:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {acc:.3f} below threshold 0.88\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Blobs KMeans Clustering",
      "description": "Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score. Print TEST_PASS if silhouette ≥ 0.5.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate make_blobs data (n=200, centers=4), apply KMeans, compute silhouette score.\")\n    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of samples (default: 200).\")\n    p.add_argument(\"--centers\", type=int, default=4, help=\"Number of cluster centers (default: 4).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    if args.n_samples <= 0:\n        print(\"TEST_FAIL: n_samples must be positive\")\n        sys.exit(1)\n    if args.centers <= 0:\n        print(\"TEST_FAIL: centers must be positive\")\n        sys.exit(1)\n\n    X, y_true = make_blobs(n_samples=args.n_samples, centers=args.centers, random_state=args.seed)\n    \n    if X is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    kmeans = KMeans(n_clusters=args.centers, random_state=args.seed, n_init=10)\n    y_pred = kmeans.fit_predict(X)\n\n    sil = silhouette_score(X, y_pred)\n    print(f\"n_samples={args.n_samples} centers={args.centers} silhouette={sil:.3f}\")\n\n    if sil >= 0.5:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: silhouette score {sil:.3f} below threshold 0.5\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Classification Synthetic Data",
      "description": "Generate make_classification data (n=500, n_features=4), train an SGDClassifier inside a StandardScaler pipeline, report accuracy. Print TEST_PASS if accuracy ≥ 0.85.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate make_classification data (n=500, n_features=4), train SGDClassifier, report accuracy.\")\n    p.add_argument(\"--n-samples\", type=int, default=500, help=\"Number of samples (default: 500).\")\n    p.add_argument(\"--n-features\", type=int, default=4, help=\"Number of features (default: 4).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    X, y = make_classification(\n        n_samples=args.n_samples,\n        n_features=args.n_features,\n        n_informative=max(2, args.n_features // 2),\n        n_redundant=0,\n        n_clusters_per_class=1,\n        random_state=args.seed\n    )\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"sgd\", SGDClassifier(max_iter=1000, tol=1e-3, random_state=args.seed))\n    ])\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    print(f\"accuracy={acc:.3f}\")\n\n    if acc >= 0.85:\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: accuracy below 0.85\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Regression Synthetic Data",
      "description": "Generate make_regression data (n=300), fit LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate synthetic regression data (n=300), fit LinearRegression, compute R2; TEST_PASS if R2 >= 0.3.\")\n    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n    p.add_argument(\"--n-features\", type=int, default=10, help=\"Number of features (default: 10).\")\n    p.add_argument(\"--noise\", type=float, default=10.0, help=\"Standard deviation of Gaussian noise (default: 10.0).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    # Set seeds in main\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    # Generate synthetic regression data\n    X, y = make_regression(\n        n_samples=args.n_samples,\n        n_features=args.n_features,\n        noise=args.noise,\n        random_state=args.seed\n    )\n\n    # Validate dataset\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed\n    )\n\n    # Fit LinearRegression\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Predict and compute R²\n    y_pred = model.predict(X_test)\n    r2 = r2_score(y_test, y_pred)\n\n    print(f\"n_samples={args.n_samples} n_features={args.n_features} noise={args.noise}\")\n    print(f\"R2={r2:.4f}\")\n\n    # Acceptance check\n    if r2 >= 0.3:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: R2={r2:.4f} below threshold 0.3\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "PCA on Digits",
      "description": "Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as 'digits_pca.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Apply PCA (n_components=2) to sklearn digits, scatter plot first two components, save as digits_pca.png.\")\n    p.add_argument(\"--output\", type=str, default=\"digits_pca.png\", help=\"Output PNG file path (default: digits_pca.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_digits()\n        X = data.data\n        y = data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or len(X) == 0:\n        print(\"TEST_FAIL: digits dataset is empty\")\n        sys.exit(1)\n\n    pca = PCA(n_components=2, random_state=args.seed)\n    X_pca = pca.fit_transform(X)\n\n    plt.figure(figsize=(8, 6))\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\n    plt.colorbar(scatter, label='Digit')\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n    plt.title('PCA on Digits Dataset')\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    import os\n    if os.path.isfile(args.output):\n        print(f\"Saved PCA scatter plot to {args.output}\")\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: output file {args.output} not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Iris Pairplot Visualization",
      "description": "Plot pairwise feature scatter plots for iris dataset using matplotlib, save as 'iris_pairplot.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\ndef load_iris_or_synthetic(seed=42):\n    try:\n        from sklearn.datasets import load_iris\n        data = load_iris()\n        X = data.data\n        y = data.target\n        feature_names = data.feature_names\n        used = \"iris\"\n    except Exception:\n        rng = np.random.default_rng(seed)\n        n = 150\n        c = rng.integers(0, 3, size=n)\n        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\n        y = c\n        feature_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n        used = \"synthetic\"\n    return X, y, feature_names, used\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Plot pairwise feature scatter plots for iris dataset; save as iris_pairplot.png.\")\n    p.add_argument(\"--output\", type=str, default=\"iris_pairplot.png\", help=\"Output filename (default: iris_pairplot.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    X, y, feature_names, used = load_iris_or_synthetic(args.seed)\n    if X is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset not available\")\n        sys.exit(1)\n\n    n_features = X.shape[1]\n    fig, axes = plt.subplots(n_features, n_features, figsize=(12, 12))\n    \n    colors = ['red', 'green', 'blue']\n    for i in range(n_features):\n        for j in range(n_features):\n            ax = axes[i, j]\n            if i == j:\n                for cls in np.unique(y):\n                    ax.hist(X[y == cls, i], bins=15, alpha=0.6, color=colors[cls % len(colors)], label=f\"Class {cls}\")\n                ax.set_ylabel(\"Frequency\")\n            else:\n                for cls in np.unique(y):\n                    ax.scatter(X[y == cls, j], X[y == cls, i], alpha=0.6, s=10, color=colors[cls % len(colors)], label=f\"Class {cls}\")\n            \n            if i == n_features - 1:\n                ax.set_xlabel(feature_names[j])\n            else:\n                ax.set_xticklabels([])\n            \n            if j == 0:\n                ax.set_ylabel(feature_names[i])\n            else:\n                ax.set_yticklabels([])\n            \n            if i == 0 and j == n_features - 1:\n                ax.legend(loc='upper right', fontsize=8)\n\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    print(f\"dataset={used} saved={args.output}\")\n\n    import os\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file missing or empty\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Wine Feature Importance",
      "description": "Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart as 'wine_importance.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Train RandomForestClassifier on wine dataset, extract feature importances, save bar chart.\")\n    p.add_argument(\"--output\", type=str, default=\"wine_importance.png\", help=\"Output filename for feature importance bar chart (default: wine_importance.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--n-estimators\", type=int, default=100, help=\"Number of trees in RandomForest (default: 100).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_wine()\n        X, y = data.data, data.target\n        feature_names = data.feature_names\n    except Exception as e:\n        print(f\"TEST_FAIL: could not load wine dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: wine dataset is empty\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    clf = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.seed)\n    clf.fit(X_train, y_train)\n\n    importances = clf.feature_importances_\n    if importances is None or len(importances) == 0:\n        print(\"TEST_FAIL: feature importances are empty\")\n        sys.exit(1)\n\n    indices = np.argsort(importances)[::-1]\n\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(len(importances)), importances[indices], align='center')\n    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')\n    plt.xlabel('Feature')\n    plt.ylabel('Importance')\n    plt.title('Wine Dataset Feature Importances')\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    import os\n    if not os.path.isfile(args.output):\n        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n        sys.exit(1)\n\n    if os.path.getsize(args.output) == 0:\n        print(f\"TEST_FAIL: output file {args.output} is empty\")\n        sys.exit(1)\n\n    acc = clf.score(X_test, y_test)\n    print(f\"accuracy={acc:.3f} output={args.output}\")\n    print(\"TEST_PASS\")\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Breast Cancer Confusion Matrix",
      "description": "Train LogisticRegression on breast_cancer dataset, plot confusion matrix, save as 'bc_confusion.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Train LogisticRegression on breast_cancer, plot confusion matrix, save as bc_confusion.png.\")\n    p.add_argument(\"--output\", type=str, default=\"bc_confusion.png\", help=\"Output confusion matrix image path (default: bc_confusion.png).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_breast_cancer()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load breast_cancer dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset is empty\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    clf = LogisticRegression(max_iter=5000, random_state=args.seed)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n    \n    fig, ax = plt.subplots(figsize=(8, 6))\n    disp.plot(ax=ax, cmap='Blues')\n    plt.title('Breast Cancer Confusion Matrix')\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    import os\n    if not os.path.isfile(args.output):\n        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n        sys.exit(1)\n\n    if os.path.getsize(args.output) == 0:\n        print(f\"TEST_FAIL: output file {args.output} is empty\")\n        sys.exit(1)\n\n    print(f\"Confusion matrix saved to {args.output}\")\n    print(\"TEST_PASS\")\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Digits Confusion Heatmap",
      "description": "Train SVC on sklearn digits, compute confusion matrix, plot it with matplotlib (no seaborn), save as digits_heatmap.png. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nimport os\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Train SVC on digits dataset, plot confusion matrix heatmap, save as digits_heatmap.png.\")\n    p.add_argument(\"--output\", type=str, default=\"digits_heatmap.png\", help=\"Output heatmap filename (default: digits_heatmap.png).\")\n    p.add_argument(\"--test-size\", type=float, default=0.3, help=\"Test set fraction (default: 0.3).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_digits()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: digits dataset is empty\")\n        sys.exit(1)\n\n    Xtr, Xte, ytr, yte = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    clf = SVC(kernel='rbf', gamma='scale', random_state=args.seed)\n    clf.fit(Xtr, ytr)\n    ypred = clf.predict(Xte)\n\n    cm = confusion_matrix(yte, ypred)\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n    im = ax.imshow(cm, cmap='Blues')\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n    ax.set_title('Digits Confusion Matrix')\n    plt.colorbar(im, ax=ax)\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, cm[i, j], ha='center', va='center', color='black')\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    if not os.path.isfile(args.output):\n        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n        sys.exit(1)\n\n    print(f\"Confusion matrix heatmap saved to {args.output}\")\n    print(\"TEST_PASS\")\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Diabetes Residual Plot",
      "description": "Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as 'residuals_plot.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport os\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Fit LinearRegression on diabetes dataset, plot residuals vs predictions, save as residuals_plot.png.\")\n    p.add_argument(\"--output\", type=str, default=\"residuals_plot.png\", help=\"Output filename for residual plot (default: residuals_plot.png).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_diabetes()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load diabetes dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: diabetes dataset is empty\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed\n    )\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    residuals = y_test - y_pred\n    mse = mean_squared_error(y_test, y_pred)\n\n    print(f\"Test MSE: {mse:.2f}\")\n\n    plt.figure(figsize=(8, 6))\n    plt.scatter(y_pred, residuals, alpha=0.6, edgecolors='k')\n    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    plt.title('Residual Plot: Diabetes Dataset')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n\n    try:\n        plt.savefig(args.output, dpi=100)\n        print(f\"Residual plot saved to {args.output}\")\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to save plot: {e}\")\n        sys.exit(1)\n\n    if not os.path.isfile(args.output):\n        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n        sys.exit(1)\n\n    file_size = os.path.getsize(args.output)\n    if file_size == 0:\n        print(f\"TEST_FAIL: output file {args.output} is empty\")\n        sys.exit(1)\n\n    print(\"TEST_PASS\")\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Synthetic Audio Classification",
      "description": "Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef generate_synthetic_audio(n_samples=250, n_features=20, seed=42):\n    rng = np.random.default_rng(seed)\n    X = []\n    y = []\n    for i in range(n_samples):\n        if i < n_samples // 2:\n            # high pitch: higher frequency components\n            freq_base = rng.uniform(800, 1200)\n            label = 1\n        else:\n            # low pitch: lower frequency components\n            freq_base = rng.uniform(100, 400)\n            label = 0\n        # simulate audio features (e.g., spectral coefficients)\n        features = rng.normal(freq_base, 50, size=n_features)\n        X.append(features)\n        y.append(label)\n    return np.array(X), np.array(y)\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate 250 synthetic audio-like vectors labeled high/low pitch, train MLPClassifier, report accuracy.\")\n    p.add_argument(\"--n-samples\", type=int, default=250, help=\"Number of synthetic audio samples (default: 250).\")\n    p.add_argument(\"--n-features\", type=int, default=20, help=\"Number of features per sample (default: 20).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    # seeds in main\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    # generate synthetic audio dataset\n    X, y = generate_synthetic_audio(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\n    \n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    # split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    # train MLP\n    clf = MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=300, random_state=args.seed)\n    clf.fit(X_train, y_train)\n\n    # predict and evaluate\n    y_pred = clf.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n\n    print(f\"dataset=synthetic_audio n_samples={args.n_samples} accuracy={acc:.3f}\")\n\n    # acceptance: accuracy >= 0.7\n    if acc >= 0.7:\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: accuracy below 0.7\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Synthetic Time Series Forecast",
      "description": "Generate 200 synthetic time series samples, predict next value using lag features, compute R². Print TEST_PASS if R² ≥ 0.25.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ndef generate_synthetic_time_series(n_samples=200, seed=42):\n    \"\"\"Generate synthetic time series with trend, seasonality, and noise.\"\"\"\n    rng = np.random.default_rng(seed)\n    t = np.arange(n_samples)\n    trend = 0.05 * t\n    seasonality = 10 * np.sin(2 * np.pi * t / 20)\n    noise = rng.normal(0, 2, size=n_samples)\n    series = trend + seasonality + noise\n    return series\n\ndef create_lag_features(series, n_lags=3):\n    \"\"\"Create lag features for time series prediction.\"\"\"\n    X = []\n    y = []\n    for i in range(n_lags, len(series)):\n        X.append(series[i-n_lags:i])\n        y.append(series[i])\n    return np.array(X), np.array(y)\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate 200 synthetic time series samples, predict next value using lag features, compute R².\")\n    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of time series samples (default: 200).\")\n    p.add_argument(\"--n-lags\", type=int, default=3, help=\"Number of lag features (default: 3).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    if args.n_samples < 50:\n        print(\"TEST_FAIL: n_samples must be at least 50\")\n        sys.exit(1)\n    if args.n_lags < 1:\n        print(\"TEST_FAIL: n_lags must be at least 1\")\n        sys.exit(1)\n    if not (0 < args.test_size < 1):\n        print(\"TEST_FAIL: test_size must be between 0 and 1\")\n        sys.exit(1)\n\n    series = generate_synthetic_time_series(n_samples=args.n_samples, seed=args.seed)\n    X, y = create_lag_features(series, n_lags=args.n_lags)\n\n    if len(X) == 0 or len(y) == 0:\n        print(\"TEST_FAIL: insufficient data after creating lag features\")\n        sys.exit(1)\n\n    split_idx = int(len(X) * (1 - args.test_size))\n    if split_idx < 1 or split_idx >= len(X):\n        print(\"TEST_FAIL: invalid train/test split\")\n        sys.exit(1)\n\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    r2 = r2_score(y_test, y_pred)\n\n    print(f\"n_samples={args.n_samples} n_lags={args.n_lags} r2={r2:.4f}\")\n\n    if r2 >= 0.25:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: R² {r2:.4f} below threshold 0.25\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Synthetic Tabular Regression",
      "description": "Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R². Print TEST_PASS if R² ≥ 0.3.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\ndef generate_synthetic_tabular(n_samples=300, n_features=5, seed=42):\n    rng = np.random.default_rng(seed)\n    X = rng.normal(0, 1, size=(n_samples, n_features))\n    true_coef = rng.uniform(-2, 2, size=n_features)\n    y = X @ true_coef + rng.normal(0, 0.5, size=n_samples)\n    return X, y\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate 300 synthetic tabular rows with 5 features, train LinearRegression, compute R².\")\n    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of synthetic samples (default: 300).\")\n    p.add_argument(\"--n-features\", type=int, default=5, help=\"Number of features (default: 5).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    X, y = generate_synthetic_tabular(n_samples=args.n_samples, n_features=args.n_features, seed=args.seed)\n    \n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"n_samples={args.n_samples} n_features={args.n_features} R²={r2:.3f}\")\n    \n    if r2 >= 0.3:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: R²={r2:.3f} below threshold 0.3\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Synthetic Text Sentiment",
      "description": "Create 200 short synthetic sentences labeled positive or negative, vectorize with CountVectorizer, train a LogisticRegression, and print accuracy; print TEST_PASS if accuracy ≥ 0.7.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\ndef generate_synthetic_sentences(n=200, seed=42):\n    rng = np.random.default_rng(seed)\n    positive_templates = [\n        \"I love this product\",\n        \"This is amazing\",\n        \"Great experience\",\n        \"Wonderful service\",\n        \"Highly recommend\",\n        \"Excellent quality\",\n        \"Very satisfied\",\n        \"Best purchase ever\",\n        \"Fantastic results\",\n        \"Really enjoyed it\"\n    ]\n    negative_templates = [\n        \"I hate this product\",\n        \"This is terrible\",\n        \"Bad experience\",\n        \"Poor service\",\n        \"Do not recommend\",\n        \"Awful quality\",\n        \"Very disappointed\",\n        \"Worst purchase ever\",\n        \"Horrible results\",\n        \"Really disliked it\"\n    ]\n    sentences = []\n    labels = []\n    for i in range(n):\n        if i % 2 == 0:\n            template = positive_templates[rng.integers(0, len(positive_templates))]\n            label = 1\n        else:\n            template = negative_templates[rng.integers(0, len(negative_templates))]\n            label = 0\n        sentences.append(template)\n        labels.append(label)\n    return sentences, np.array(labels)\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate 200 synthetic sentiment sentences, vectorize with CountVectorizer, train LogisticRegression, print TEST_PASS if accuracy >= 0.7.\")\n    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of synthetic sentences (default: 200).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    sentences, labels = generate_synthetic_sentences(n=args.n_samples, seed=args.seed)\n    if len(sentences) == 0 or len(labels) == 0:\n        print(\"TEST_FAIL: no sentences generated\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        sentences, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\n    )\n\n    vectorizer = CountVectorizer()\n    X_train_vec = vectorizer.fit_transform(X_train)\n    X_test_vec = vectorizer.transform(X_test)\n\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\n    clf.fit(X_train_vec, y_train)\n\n    acc = clf.score(X_test_vec, y_test)\n    print(f\"accuracy={acc:.3f}\")\n\n    if acc >= 0.7:\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: accuracy below 0.7\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "MNIST CNN Classifier",
      "description": "If MNIST available, train simple CNN (2 conv layers), else generate FakeData (32x32 grayscale), train CNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.85 or fallback ≥ 0.6.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\n\ndef load_mnist_or_fakedata(seed=42, allow_download=False):\n    try:\n        import torch\n        from torchvision import datasets, transforms\n        torch.manual_seed(seed)\n        tfm = transforms.Compose([transforms.ToTensor()])\n        train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n        test = datasets.MNIST(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n        if len(train) == 0 or len(test) == 0:\n            raise RuntimeError(\"MNIST cache missing and download disabled\")\n        return train, test, True\n    except Exception:\n        import torch\n        from torchvision.datasets import FakeData\n        from torchvision import transforms\n        torch.manual_seed(seed)\n        tfm = transforms.Compose([transforms.ToTensor()])\n        train = FakeData(size=1000, image_size=(1, 32, 32), num_classes=10, transform=tfm)\n        test = FakeData(size=200, image_size=(1, 32, 32), num_classes=10, transform=tfm)\n        return train, test, False\n\ndef main():\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import DataLoader\n\n    p = argparse.ArgumentParser(description=\"MNIST CNN classifier with FakeData fallback; seeds in main; explicit acceptance.\")\n    p.add_argument(\"--epochs\", type=int, default=2, help=\"Training epochs (default: 2).\")\n    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\n\n    class SimpleCNN(nn.Module):\n        def __init__(self, input_size=28):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n            self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n            self.pool = nn.MaxPool2d(2, 2)\n            self.relu = nn.ReLU()\n            conv_out_size = (input_size // 4) * (input_size // 4) * 32\n            self.fc1 = nn.Linear(conv_out_size, 64)\n            self.fc2 = nn.Linear(64, 10)\n\n        def forward(self, x):\n            x = self.pool(self.relu(self.conv1(x)))\n            x = self.pool(self.relu(self.conv2(x)))\n            x = x.view(x.size(0), -1)\n            x = self.relu(self.fc1(x))\n            x = self.fc2(x)\n            return x\n\n    input_size = 28 if real else 32\n    model = SimpleCNN(input_size=input_size)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    model.train()\n    for epoch in range(args.epochs):\n        for xb, yb in train_loader:\n            optimizer.zero_grad()\n            outputs = model(xb)\n            loss = criterion(outputs, yb)\n            loss.backward()\n            optimizer.step()\n\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for xb, yb in test_loader:\n            outputs = model(xb)\n            _, predicted = torch.max(outputs.data, 1)\n            total += yb.size(0)\n            correct += (predicted == yb).sum().item()\n\n    accuracy = correct / max(total, 1)\n    dataset_name = \"MNIST\" if real else \"FakeData\"\n    print(f\"dataset={dataset_name} accuracy={accuracy:.3f}\")\n\n    threshold = 0.85 if real else 0.6\n    if accuracy >= threshold:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {accuracy:.3f} below threshold {threshold}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "FashionMNIST KNN",
      "description": "If FashionMNIST available, flatten images, train KNN (k=5), else use FakeData, train KNN, report accuracy. Print TEST_PASS if accuracy ≥ 0.7 or fallback ≥ 0.5.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\n\ndef load_fashionmnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\n    try:\n        import torch\n        from torchvision import datasets, transforms\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        train = datasets.FashionMNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n        test = datasets.FashionMNIST(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n        if len(train) == 0 or len(test) == 0:\n            raise RuntimeError(\"FashionMNIST cache missing and download disabled\")\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\n        return train, test, True\n    except Exception:\n        import torch\n        from torchvision import transforms\n        from torchvision.datasets import FakeData\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n        test = FakeData(size=max_test, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n        return train, test, False\n\ndef main():\n    p = argparse.ArgumentParser(description=\"FashionMNIST KNN (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n    p.add_argument(\"--k\", type=int, default=5, help=\"Number of neighbors for KNN (default: 5).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit FashionMNIST download if not cached.\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    train_ds, test_ds, real = load_fashionmnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n\n    X_train = []\n    y_train = []\n    for img, label in train_ds:\n        flat = img.numpy().flatten()\n        X_train.append(flat)\n        y_train.append(label)\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n\n    X_test = []\n    y_test = []\n    for img, label in test_ds:\n        flat = img.numpy().flatten()\n        X_test.append(flat)\n        y_test.append(label)\n    X_test = np.array(X_test)\n    y_test = np.array(y_test)\n\n    if len(X_train) == 0 or len(X_test) == 0:\n        print(\"TEST_FAIL: dataset not available\")\n        sys.exit(1)\n\n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(n_neighbors=args.k)\n    knn.fit(X_train, y_train)\n    acc = knn.score(X_test, y_test)\n\n    print(f\"dataset={'fashionmnist' if real else 'fake'} k={args.k} accuracy={acc:.3f}\")\n\n    threshold = 0.7 if real else 0.5\n    if acc >= threshold:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "CIFAR10 Class Balance",
      "description": "If CIFAR10 available, count class frequencies, else generate FakeData with 10 classes, count, save bar chart as 'class_balance.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\ndef load_cifar10_or_fakedata(seed=42, allow_download=False):\n    try:\n        import torch\n        from torchvision import datasets\n        torch.manual_seed(seed)\n        ds = datasets.CIFAR10(root=\"./data\", train=True, download=bool(allow_download))\n        if len(ds) == 0:\n            raise RuntimeError(\"CIFAR10 cache missing and download disabled\")\n        labels = [y for _, y in ds]\n        return labels, True\n    except Exception:\n        import torch\n        from torchvision.datasets import FakeData\n        torch.manual_seed(seed)\n        ds = FakeData(size=1000, image_size=(3, 32, 32), num_classes=10)\n        labels = [y for _, y in ds]\n        return labels, False\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Count CIFAR10 class frequencies or FakeData fallback; save bar chart; seeds in main.\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit CIFAR10 download if not cached.\")\n    p.add_argument(\"--output\", type=str, default=\"class_balance.png\", help=\"Output bar chart filename (default: class_balance.png).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    labels, real = load_cifar10_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n    if labels is None or len(labels) == 0:\n        print(\"TEST_FAIL: dataset not available\")\n        sys.exit(1)\n\n    counts = np.bincount(labels, minlength=10)\n    print(f\"dataset={'cifar10' if real else 'fake'} class_counts={counts.tolist()}\")\n\n    plt.figure(figsize=(8, 5))\n    plt.bar(range(10), counts, color='steelblue')\n    plt.xlabel(\"Class\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Class Balance\")\n    plt.xticks(range(10))\n    plt.tight_layout()\n    plt.savefig(args.output)\n    plt.close()\n\n    import os\n    if os.path.isfile(args.output):\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "ImageFolder Histogram",
      "description": "Try to load an ImageFolder and compute average RGB histogram, else fall back to FakeData. Save plot to histogram.png. Print TEST_PASS if file exists and is non-empty.",
      "code": "import argparse\nimport sys\nimport os\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\ndef load_imagefolder_or_fakedata(root_dir, num_fake=50, seed=42, allow_download=False):\n    try:\n        import torch\n        from torchvision import datasets, transforms\n        torch.manual_seed(seed)\n        if os.path.isdir(root_dir):\n            tfm = transforms.ToTensor()\n            ds = datasets.ImageFolder(root=root_dir, transform=tfm)\n            if len(ds) > 0:\n                return ds, True\n        raise RuntimeError(\"ImageFolder not available or empty\")\n    except Exception:\n        import torch\n        from torchvision import transforms\n        from torchvision.datasets import FakeData\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        ds = FakeData(size=num_fake, image_size=(3, 64, 64), num_classes=2, transform=tfm)\n        return ds, False\n\ndef compute_rgb_histogram(dataset, num_samples=None):\n    if num_samples is None:\n        num_samples = len(dataset)\n    else:\n        num_samples = min(num_samples, len(dataset))\n\n    r_sum = np.zeros(256, dtype=np.float64)\n    g_sum = np.zeros(256, dtype=np.float64)\n    b_sum = np.zeros(256, dtype=np.float64)\n\n    for i in range(num_samples):\n        img_tensor, _ = dataset[i]\n        img_np = (img_tensor.numpy() * 255).astype(np.uint8)\n        r_hist, _ = np.histogram(img_np[0].flatten(), bins=256, range=(0, 256))\n        g_hist, _ = np.histogram(img_np[1].flatten(), bins=256, range=(0, 256))\n        b_hist, _ = np.histogram(img_np[2].flatten(), bins=256, range=(0, 256))\n        r_sum += r_hist\n        g_sum += g_hist\n        b_sum += b_hist\n\n    r_avg = r_sum / num_samples\n    g_avg = g_sum / num_samples\n    b_avg = b_sum / num_samples\n    return r_avg, g_avg, b_avg\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Compute average RGB histogram from ImageFolder or FakeData and save as histogram.png.\")\n    p.add_argument(\"--root-dir\", type=str, default=\"./imagefolder_data\", help=\"Path to ImageFolder root directory (default: ./imagefolder_data).\")\n    p.add_argument(\"--num-fake\", type=int, default=50, help=\"Number of FakeData images if ImageFolder unavailable (default: 50).\")\n    p.add_argument(\"--output\", type=str, default=\"histogram.png\", help=\"Output histogram file path (default: histogram.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit dataset download if needed.\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    dataset, is_real = load_imagefolder_or_fakedata(\n        root_dir=args.root_dir,\n        num_fake=args.num_fake,\n        seed=args.seed,\n        allow_download=args.allow_download\n    )\n\n    if dataset is None or len(dataset) == 0:\n        print(\"TEST_FAIL: dataset not available or empty\")\n        sys.exit(1)\n\n    print(f\"Using {'ImageFolder' if is_real else 'FakeData'} with {len(dataset)} images\")\n\n    r_hist, g_hist, b_hist = compute_rgb_histogram(dataset)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    bins = np.arange(256)\n    ax.plot(bins, r_hist, color='red', alpha=0.7, label='Red')\n    ax.plot(bins, g_hist, color='green', alpha=0.7, label='Green')\n    ax.plot(bins, b_hist, color='blue', alpha=0.7, label='Blue')\n    ax.set_xlabel('Pixel Value')\n    ax.set_ylabel('Average Frequency')\n    ax.set_title('Average RGB Histogram')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    if os.path.isfile(args.output):\n        file_size = os.path.getsize(args.output)\n        if file_size > 0:\n            print(f\"Histogram saved to {args.output} ({file_size} bytes)\")\n            print(\"TEST_PASS\")\n        else:\n            print(\"TEST_FAIL: output file is empty\")\n            sys.exit(1)\n    else:\n        print(\"TEST_FAIL: output file does not exist\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "FakeData GAN Discriminator",
      "description": "Generate FakeData images, train simple CNN discriminator, report accuracy distinguishing real/fake. Print TEST_PASS if accuracy ≥ 0.7.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Train CNN discriminator on FakeData (real vs fake labels); print TEST_PASS if accuracy >= 0.7.\")\n    p.add_argument(\"--epochs\", type=int, default=3, help=\"Training epochs (default: 3).\")\n    p.add_argument(\"--batch\", type=int, default=64, help=\"Batch size (default: 64).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--n-real\", type=int, default=500, help=\"Number of real samples (default: 500).\")\n    p.add_argument(\"--n-fake\", type=int, default=500, help=\"Number of fake samples (default: 500).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import DataLoader, TensorDataset\n    from torchvision.datasets import FakeData\n    from torchvision import transforms\n\n    tfm = transforms.ToTensor()\n    real_ds = FakeData(size=args.n_real, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n    fake_ds = FakeData(size=args.n_fake, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n\n    real_images = []\n    fake_images = []\n    for i in range(args.n_real):\n        img, _ = real_ds[i]\n        real_images.append(img)\n    for i in range(args.n_fake):\n        img, _ = fake_ds[i]\n        fake_images.append(img)\n\n    real_images = torch.stack(real_images)\n    fake_images = torch.stack(fake_images)\n    real_labels = torch.ones(args.n_real, dtype=torch.long)\n    fake_labels = torch.zeros(args.n_fake, dtype=torch.long)\n\n    X = torch.cat([real_images, fake_images], dim=0)\n    y = torch.cat([real_labels, fake_labels], dim=0)\n\n    perm = torch.randperm(len(X))\n    X = X[perm]\n    y = y[perm]\n\n    split = int(0.8 * len(X))\n    X_train, X_test = X[:split], X[split:]\n    y_train, y_test = y[:split], y[split:]\n\n    train_ds = TensorDataset(X_train, y_train)\n    test_ds = TensorDataset(X_test, y_test)\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\n\n    class Discriminator(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Conv2d(1, 16, 3, 1, 1), nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Flatten(),\n                nn.Linear(32 * 7 * 7, 64), nn.ReLU(),\n                nn.Linear(64, 2)\n            )\n        def forward(self, x):\n            return self.net(x)\n\n    model = Discriminator()\n    opt = optim.Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.CrossEntropyLoss()\n\n    model.train()\n    for epoch in range(args.epochs):\n        for xb, yb in train_loader:\n            opt.zero_grad()\n            logits = model(xb)\n            loss = loss_fn(logits, yb)\n            loss.backward()\n            opt.step()\n\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for xb, yb in test_loader:\n            pred = model(xb).argmax(1)\n            correct += (pred == yb).sum().item()\n            total += yb.numel()\n\n    acc = correct / max(total, 1)\n    print(f\"discriminator_accuracy={acc:.3f}\")\n\n    if acc >= 0.7:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {acc:.3f} < 0.7\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "KMeans on CIFAR Colors",
      "description": "Load CIFAR10 pixels (or FakeData fallback), run KMeans(k=5) on RGB pixels, and print inertia. Print TEST_PASS if inertia < 2.0e5. Uses fewer images to reduce runtime.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\n\ndef load_cifar_or_fakedata(max_samples=500, seed=42, allow_download=False):\n    try:\n        import torch\n        from torchvision import datasets, transforms\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        train = datasets.CIFAR10(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n        if len(train) == 0:\n            raise RuntimeError(\"CIFAR10 cache missing and download disabled\")\n        subset = torch.utils.data.Subset(train, list(range(min(len(train), max_samples))))\n        loader = torch.utils.data.DataLoader(subset, batch_size=max_samples, shuffle=False)\n        for xb, _ in loader:\n            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\n            return pixels, True\n    except Exception:\n        import torch\n        from torchvision import transforms\n        from torchvision.datasets import FakeData\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        fake = FakeData(size=max_samples, image_size=(3, 32, 32), num_classes=10, transform=tfm)\n        loader = torch.utils.data.DataLoader(fake, batch_size=max_samples, shuffle=False)\n        for xb, _ in loader:\n            pixels = xb.permute(0, 2, 3, 1).reshape(-1, 3).numpy()\n            return pixels, False\n    return None, False\n\ndef main():\n    p = argparse.ArgumentParser(description=\"KMeans on CIFAR10 RGB pixels (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n    p.add_argument(\"--k\", type=int, default=5, help=\"Number of clusters (default: 5).\")\n    p.add_argument(\"--max-samples\", type=int, default=500, help=\"Max images to load (default: 500).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit CIFAR10 download if not cached.\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    pixels, real = load_cifar_or_fakedata(max_samples=args.max_samples, seed=args.seed, allow_download=args.allow_download)\n    if pixels is None or len(pixels) == 0:\n        print(\"TEST_FAIL: dataset not available\")\n        sys.exit(1)\n\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\n    kmeans.fit(pixels)\n    inertia = kmeans.inertia_\n\n    print(f\"dataset={'cifar10' if real else 'fake'} k={args.k} inertia={inertia:.2e}\")\n\n    if inertia < 2.0e5:\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: inertia >= 2.0e5\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Iris Decision Boundary",
      "description": "Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as 'iris_boundary.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Train DecisionTreeClassifier on iris (first two features), plot decision regions, save as iris_boundary.png.\")\n    p.add_argument(\"--output\", type=str, default=\"iris_boundary.png\", help=\"Output image path (default: iris_boundary.png).\")\n    p.add_argument(\"--max-depth\", type=int, default=3, help=\"Max depth of decision tree (default: 3).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_iris()\n        X = data.data[:, :2]\n        y = data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: could not load iris dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: iris dataset is empty\")\n        sys.exit(1)\n\n    clf = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.seed)\n    clf.fit(X, y)\n\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    h = 0.02\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolor='k', cmap='viridis')\n    plt.xlabel(data.feature_names[0])\n    plt.ylabel(data.feature_names[1])\n    plt.title(\"Iris Decision Boundary (first two features)\")\n    plt.colorbar(scatter)\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    import os\n    if os.path.isfile(args.output):\n        print(f\"Decision boundary saved to {args.output}\")\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: output file {args.output} not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Wine PCA Scatter",
      "description": "Apply PCA to wine dataset (2 components), scatter plot colored by target, save as 'wine_pca.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Apply PCA to wine dataset (2 components), scatter plot colored by target, save as wine_pca.png.\")\n    p.add_argument(\"--output\", type=str, default=\"wine_pca.png\", help=\"Output filename (default: wine_pca.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_wine()\n        X = data.data\n        y = data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load wine dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: wine dataset is empty\")\n        sys.exit(1)\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    pca = PCA(n_components=2, random_state=args.seed)\n    X_pca = pca.fit_transform(X_scaled)\n\n    plt.figure(figsize=(8, 6))\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, edgecolors='k')\n    plt.colorbar(scatter, label='Wine Class')\n    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n    plt.title('Wine Dataset PCA (2 Components)')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    import os\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\n        print(f\"Saved PCA scatter plot to {args.output}\")\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: output file {args.output} does not exist or is empty\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Breast Cancer ROC Curve",
      "description": "Train LogisticRegression on breast_cancer, compute ROC curve, save as 'roc_curve.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Train LogisticRegression on breast_cancer, compute ROC curve, save as roc_curve.png.\")\n    p.add_argument(\"--output\", type=str, default=\"roc_curve.png\", help=\"Output ROC curve image path (default: roc_curve.png).\")\n    p.add_argument(\"--test-size\", type=float, default=0.3, help=\"Test set fraction (default: 0.3).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_breast_cancer()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load breast_cancer dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset is empty\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    clf = LogisticRegression(max_iter=5000, random_state=args.seed)\n    clf.fit(X_train, y_train)\n\n    y_scores = clf.predict_proba(X_test)[:, 1]\n\n    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic - Breast Cancer')\n    plt.legend(loc='lower right')\n    plt.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    import os\n    if not os.path.isfile(args.output):\n        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n        sys.exit(1)\n\n    print(f\"ROC curve saved to {args.output}, AUC={roc_auc:.3f}\")\n    print(\"TEST_PASS\")\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Digits t-SNE Plot",
      "description": "Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as 'tsne_digits.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.manifold import TSNE\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Apply t-SNE to sklearn digits (subset 500 samples), scatter plot colored by digit, save as tsne_digits.png.\")\n    p.add_argument(\"--output\", type=str, default=\"tsne_digits.png\", help=\"Output PNG file path (default: tsne_digits.png).\")\n    p.add_argument(\"--n-samples\", type=int, default=500, help=\"Number of samples to use (default: 500).\")\n    p.add_argument(\"--perplexity\", type=float, default=30.0, help=\"t-SNE perplexity (default: 30.0).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_digits()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load digits dataset: {e}\")\n        sys.exit(1)\n\n    if len(X) == 0:\n        print(\"TEST_FAIL: digits dataset is empty\")\n        sys.exit(1)\n\n    n = min(args.n_samples, len(X))\n    indices = np.random.choice(len(X), size=n, replace=False)\n    X_sub = X[indices]\n    y_sub = y[indices]\n\n    try:\n        tsne = TSNE(n_components=2, perplexity=args.perplexity, random_state=args.seed)\n        X_embedded = tsne.fit_transform(X_sub)\n    except Exception as e:\n        print(f\"TEST_FAIL: t-SNE failed: {e}\")\n        sys.exit(1)\n\n    plt.figure(figsize=(8, 6))\n    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_sub, cmap='tab10', alpha=0.7, edgecolors='k', linewidth=0.5)\n    plt.colorbar(scatter, label='Digit')\n    plt.title('t-SNE of Digits Dataset')\n    plt.xlabel('t-SNE Component 1')\n    plt.ylabel('t-SNE Component 2')\n    plt.tight_layout()\n\n    try:\n        plt.savefig(args.output, dpi=100)\n        plt.close()\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to save plot: {e}\")\n        sys.exit(1)\n\n    import os\n    if os.path.isfile(args.output):\n        print(f\"Saved t-SNE plot to {args.output}\")\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file does not exist\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Diabetes Cross Validation",
      "description": "Run 5-fold CV on diabetes dataset with Ridge regression, compute mean R². Print TEST_PASS if mean R² ≥ 0.35.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Run 5-fold CV on diabetes dataset with Ridge regression; print TEST_PASS if mean R² ≥ 0.35.\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--alpha\", type=float, default=1.0, help=\"Ridge regularization strength (default: 1.0).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_diabetes()\n        X, y = data.data, data.target\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load diabetes dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: diabetes dataset is empty\")\n        sys.exit(1)\n\n    model = Ridge(alpha=args.alpha, random_state=args.seed)\n    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n    mean_r2 = scores.mean()\n\n    print(f\"5-fold CV R² scores: {scores}\")\n    print(f\"Mean R²: {mean_r2:.4f}\")\n\n    if mean_r2 >= 0.35:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: mean R² {mean_r2:.4f} < 0.35\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Moons SVM Margin",
      "description": "Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as 'svm_margin.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate make_moons data, train SVM with RBF kernel, plot decision boundary and margins, save as svm_margin.png.\")\n    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of samples (default: 200).\")\n    p.add_argument(\"--noise\", type=float, default=0.2, help=\"Noise level (default: 0.2).\")\n    p.add_argument(\"--C\", type=float, default=1.0, help=\"SVM regularization parameter (default: 1.0).\")\n    p.add_argument(\"--gamma\", type=float, default=2.0, help=\"RBF kernel gamma (default: 2.0).\")\n    p.add_argument(\"--output\", type=str, default=\"svm_margin.png\", help=\"Output image path (default: svm_margin.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    X, y = make_moons(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    clf = SVC(kernel='rbf', C=args.C, gamma=args.gamma)\n    clf.fit(X_scaled, y)\n    \n    x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\n    y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\n    h = 0.02\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    \n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.figure(figsize=(10, 8))\n    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 50), cmap='RdBu', alpha=0.6)\n    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n    plt.contour(xx, yy, Z, levels=[-1, 1], linewidths=1, colors='black', linestyles='dashed')\n    \n    plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c='red', edgecolors='k', marker='o', s=50, label='Class 0')\n    plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c='blue', edgecolors='k', marker='s', s=50, label='Class 1')\n    \n    support_vectors = clf.support_vectors_\n    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=200, linewidth=1, facecolors='none', edgecolors='green', label='Support Vectors')\n    \n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('SVM Decision Boundary and Margins (RBF Kernel)')\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n    \n    import os\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\n        print(f\"Saved decision boundary plot to {args.output}\")\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file not created or empty\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Circles Neural Net",
      "description": "Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy ≥ 0.87.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate make_circles data, train MLPClassifier, report accuracy. Print TEST_PASS if accuracy >= 0.87.\")\n    p.add_argument(\"--n-samples\", type=int, default=1000, help=\"Number of samples to generate (default: 1000).\")\n    p.add_argument(\"--noise\", type=float, default=0.1, help=\"Noise level for make_circles (default: 0.1).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    X, y = make_circles(n_samples=args.n_samples, noise=args.noise, random_state=args.seed)\n    \n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    clf = MLPClassifier(\n        hidden_layer_sizes=(100, 50),\n        max_iter=500,\n        random_state=args.seed,\n        early_stopping=True,\n        validation_fraction=0.1\n    )\n    clf.fit(X_train_scaled, y_train)\n\n    accuracy = clf.score(X_test_scaled, y_test)\n    print(f\"accuracy={accuracy:.3f}\")\n\n    if accuracy >= 0.87:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.87\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Blobs Elbow Method",
      "description": "Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as 'elbow_plot.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate make_blobs data, compute KMeans inertia for k=1..8, plot elbow curve, save as elbow_plot.png.\")\n    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples to generate (default: 300).\")\n    p.add_argument(\"--n-features\", type=int, default=2, help=\"Number of features (default: 2).\")\n    p.add_argument(\"--centers\", type=int, default=4, help=\"Number of centers for make_blobs (default: 4).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--output\", type=str, default=\"elbow_plot.png\", help=\"Output filename for elbow plot (default: elbow_plot.png).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    X, y = make_blobs(n_samples=args.n_samples, n_features=args.n_features, centers=args.centers, random_state=args.seed)\n    if X is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    inertias = []\n    k_range = range(1, 9)\n    for k in k_range:\n        km = KMeans(n_clusters=k, random_state=args.seed, n_init=10)\n        km.fit(X)\n        inertias.append(km.inertia_)\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(list(k_range), inertias, marker='o')\n    plt.xlabel('Number of clusters (k)')\n    plt.ylabel('Inertia')\n    plt.title('Elbow Method for Optimal k')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(args.output)\n    plt.close()\n\n    import os\n    if os.path.isfile(args.output):\n        print(f\"Elbow plot saved to {args.output}\")\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Classification Learning Curve",
      "description": "Generate make_classification data, train LogisticRegression, plot learning curve, save as 'learning_curve.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.linear_model import LogisticRegression\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate make_classification data, train LogisticRegression, plot learning curve, save as learning_curve.png.\")\n    p.add_argument(\"--n-samples\", type=int, default=500, help=\"Number of samples (default: 500).\")\n    p.add_argument(\"--n-features\", type=int, default=20, help=\"Number of features (default: 20).\")\n    p.add_argument(\"--output\", type=str, default=\"learning_curve.png\", help=\"Output file path (default: learning_curve.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    X, y = make_classification(\n        n_samples=args.n_samples,\n        n_features=args.n_features,\n        n_informative=max(2, args.n_features // 2),\n        n_redundant=max(0, args.n_features // 4),\n        random_state=args.seed\n    )\n\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\n\n    train_sizes = np.linspace(0.1, 1.0, 10)\n    train_sizes_abs, train_scores, val_scores = learning_curve(\n        clf, X, y,\n        train_sizes=train_sizes,\n        cv=5,\n        scoring='accuracy',\n        random_state=args.seed,\n        n_jobs=1\n    )\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(train_sizes_abs, train_mean, 'o-', label='Training score', linewidth=2)\n    plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2)\n    plt.plot(train_sizes_abs, val_mean, 'o-', label='Validation score', linewidth=2)\n    plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2)\n    plt.xlabel('Training Set Size')\n    plt.ylabel('Accuracy')\n    plt.title('Learning Curve (LogisticRegression)')\n    plt.legend(loc='best')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    import os\n    if os.path.exists(args.output) and os.path.getsize(args.output) > 0:\n        print(f\"Learning curve saved to {args.output}\")\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file not created or empty\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Regression Residual Analysis",
      "description": "Generate make_regression data, train LinearRegression, plot residual histogram, save as 'resid_hist.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport os\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate regression data, train LinearRegression, plot residual histogram, save as resid_hist.png.\")\n    p.add_argument(\"--n-samples\", type=int, default=300, help=\"Number of samples (default: 300).\")\n    p.add_argument(\"--n-features\", type=int, default=5, help=\"Number of features (default: 5).\")\n    p.add_argument(\"--noise\", type=float, default=10.0, help=\"Noise level (default: 10.0).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--output\", type=str, default=\"resid_hist.png\", help=\"Output histogram filename (default: resid_hist.png).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    X, y = make_regression(n_samples=args.n_samples, n_features=args.n_features, noise=args.noise, random_state=args.seed)\n    \n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed)\n    \n    model = LinearRegression()\n    model.fit(Xtr, ytr)\n    \n    y_pred = model.predict(Xte)\n    residuals = yte - y_pred\n    \n    plt.figure(figsize=(8, 6))\n    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n    plt.xlabel('Residuals')\n    plt.ylabel('Frequency')\n    plt.title('Residual Histogram')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n    \n    if not os.path.exists(args.output):\n        print(f\"TEST_FAIL: output file {args.output} not created\")\n        sys.exit(1)\n    \n    file_size = os.path.getsize(args.output)\n    if file_size == 0:\n        print(f\"TEST_FAIL: output file {args.output} is empty\")\n        sys.exit(1)\n    \n    print(f\"Residual histogram saved to {args.output} (size: {file_size} bytes)\")\n    print(\"TEST_PASS\")\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Synthetic NLP Spam Filter",
      "description": "Generate 250 synthetic email texts labeled spam/ham, vectorize with TF-IDF, train Naive Bayes, report accuracy. Print TEST_PASS if accuracy ≥ 0.7.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef generate_synthetic_emails(n=250, seed=42):\n    rng = np.random.default_rng(seed)\n    spam_words = [\"win\", \"free\", \"prize\", \"click\", \"offer\", \"buy\", \"discount\", \"limited\", \"urgent\", \"cash\", \"bonus\", \"guarantee\", \"credit\", \"loan\", \"money\"]\n    ham_words = [\"meeting\", \"schedule\", \"report\", \"project\", \"team\", \"update\", \"review\", \"discuss\", \"attached\", \"please\", \"thanks\", \"regards\", \"confirm\", \"deadline\", \"agenda\"]\n    \n    texts = []\n    labels = []\n    \n    for i in range(n):\n        is_spam = rng.random() < 0.5\n        if is_spam:\n            num_words = rng.integers(5, 15)\n            words = rng.choice(spam_words, size=num_words, replace=True)\n            text = \" \".join(words)\n            labels.append(1)\n        else:\n            num_words = rng.integers(5, 15)\n            words = rng.choice(ham_words, size=num_words, replace=True)\n            text = \" \".join(words)\n            labels.append(0)\n        texts.append(text)\n    \n    return texts, labels\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate 250 synthetic spam/ham emails, train Naive Bayes with TF-IDF, report accuracy.\")\n    p.add_argument(\"--n-samples\", type=int, default=250, help=\"Number of synthetic emails to generate (default: 250).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    texts, labels = generate_synthetic_emails(n=args.n_samples, seed=args.seed)\n    \n    if len(texts) == 0 or len(labels) == 0:\n        print(\"TEST_FAIL: no data generated\")\n        sys.exit(1)\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        texts, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\n    )\n    \n    vectorizer = TfidfVectorizer(max_features=100)\n    X_train_vec = vectorizer.fit_transform(X_train)\n    X_test_vec = vectorizer.transform(X_test)\n    \n    clf = MultinomialNB()\n    clf.fit(X_train_vec, y_train)\n    \n    y_pred = clf.predict(X_test_vec)\n    acc = accuracy_score(y_test, y_pred)\n    \n    print(f\"n_samples={args.n_samples} accuracy={acc:.3f}\")\n    \n    if acc >= 0.7:\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: accuracy below 0.7\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Synthetic Recommender System",
      "description": "Generate 200 synthetic user-item ratings, apply matrix factorization (NMF), compute RMSE. Print TEST_PASS if RMSE ≤ 1.5.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.decomposition import NMF\nfrom sklearn.metrics import mean_squared_error\n\ndef generate_synthetic_ratings(n_users=50, n_items=40, n_ratings=200, seed=42):\n    \"\"\"Generate synthetic user-item ratings matrix.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Create sparse ratings: user_id, item_id, rating\n    user_ids = rng.integers(0, n_users, size=n_ratings)\n    item_ids = rng.integers(0, n_items, size=n_ratings)\n    ratings = rng.uniform(1.0, 5.0, size=n_ratings)\n    \n    # Build dense matrix (users x items)\n    R = np.zeros((n_users, n_items))\n    for u, i, r in zip(user_ids, item_ids, ratings):\n        R[u, i] = r\n    \n    # Mask: which entries are observed\n    mask = (R > 0)\n    \n    return R, mask, user_ids, item_ids, ratings\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Synthetic recommender with NMF; RMSE ≤ 1.5 for TEST_PASS.\")\n    p.add_argument(\"--n-users\", type=int, default=50, help=\"Number of users (default: 50).\")\n    p.add_argument(\"--n-items\", type=int, default=40, help=\"Number of items (default: 40).\")\n    p.add_argument(\"--n-ratings\", type=int, default=200, help=\"Number of ratings (default: 200).\")\n    p.add_argument(\"--n-components\", type=int, default=10, help=\"NMF latent factors (default: 10).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n    \n    # Set seeds in main\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n    \n    # Generate synthetic ratings\n    R, mask, user_ids, item_ids, ratings = generate_synthetic_ratings(\n        n_users=args.n_users,\n        n_items=args.n_items,\n        n_ratings=args.n_ratings,\n        seed=args.seed\n    )\n    \n    if R is None or mask is None or np.sum(mask) == 0:\n        print(\"TEST_FAIL: no ratings generated\")\n        sys.exit(1)\n    \n    print(f\"Generated {args.n_ratings} ratings for {args.n_users} users and {args.n_items} items\")\n    \n    # Apply NMF (Non-negative Matrix Factorization)\n    # NMF requires non-negative values; our ratings are already ≥ 1.0\n    model = NMF(n_components=args.n_components, init='random', random_state=args.seed, max_iter=500)\n    \n    try:\n        W = model.fit_transform(R)  # user factors\n        H = model.components_       # item factors\n        R_pred = W @ H              # reconstructed matrix\n    except Exception as e:\n        print(f\"TEST_FAIL: NMF failed - {e}\")\n        sys.exit(1)\n    \n    # Compute RMSE on observed ratings only\n    observed_true = R[mask]\n    observed_pred = R_pred[mask]\n    \n    if len(observed_true) == 0:\n        print(\"TEST_FAIL: no observed ratings to evaluate\")\n        sys.exit(1)\n    \n    rmse = np.sqrt(mean_squared_error(observed_true, observed_pred))\n    print(f\"RMSE on observed ratings: {rmse:.4f}\")\n    \n    # Acceptance: RMSE ≤ 1.5\n    if rmse <= 1.5:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: RMSE {rmse:.4f} > 1.5\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Synthetic Speech Emotion",
      "description": "Generate 200 synthetic speech-like feature vectors labeled emotion, train RandomForest, report accuracy. Print TEST_PASS if accuracy ≥ 0.65.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef generate_synthetic_speech_emotion(n_samples=200, n_features=13, n_classes=4, seed=42):\n    \"\"\"Generate synthetic speech-like feature vectors with emotion labels.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Emotion classes: 0=neutral, 1=happy, 2=sad, 3=angry\n    y = rng.integers(0, n_classes, size=n_samples)\n    \n    # Generate MFCC-like features (13 coefficients typical for speech)\n    X = np.zeros((n_samples, n_features))\n    \n    for i in range(n_samples):\n        emotion = y[i]\n        # Each emotion has characteristic feature patterns\n        if emotion == 0:  # neutral\n            X[i] = rng.normal(0.0, 1.0, size=n_features)\n        elif emotion == 1:  # happy (higher pitch, more energy)\n            X[i] = rng.normal(1.5, 1.2, size=n_features)\n        elif emotion == 2:  # sad (lower pitch, less energy)\n            X[i] = rng.normal(-1.5, 0.8, size=n_features)\n        elif emotion == 3:  # angry (high energy, variable pitch)\n            X[i] = rng.normal(0.5, 2.0, size=n_features)\n    \n    return X, y\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate 200 synthetic speech emotion vectors, train RandomForest, report accuracy.\")\n    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of synthetic samples (default: 200).\")\n    p.add_argument(\"--n-features\", type=int, default=13, help=\"Number of features per sample (default: 13).\")\n    p.add_argument(\"--n-classes\", type=int, default=4, help=\"Number of emotion classes (default: 4).\")\n    p.add_argument(\"--test-size\", type=float, default=0.25, help=\"Test set fraction (default: 0.25).\")\n    p.add_argument(\"--n-estimators\", type=int, default=100, help=\"Number of trees in RandomForest (default: 100).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n    \n    # Set seeds in main\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n    \n    # Generate synthetic speech emotion dataset\n    X, y = generate_synthetic_speech_emotion(\n        n_samples=args.n_samples,\n        n_features=args.n_features,\n        n_classes=args.n_classes,\n        seed=args.seed\n    )\n    \n    # Validate dataset\n    if X is None or y is None or len(X) == 0:\n        print(\"TEST_FAIL: dataset generation failed\")\n        sys.exit(1)\n    \n    if len(X) != args.n_samples:\n        print(f\"TEST_FAIL: expected {args.n_samples} samples, got {len(X)}\")\n        sys.exit(1)\n    \n    # Split dataset\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n    \n    # Train RandomForest\n    clf = RandomForestClassifier(\n        n_estimators=args.n_estimators,\n        random_state=args.seed,\n        n_jobs=-1\n    )\n    clf.fit(X_train, y_train)\n    \n    # Predict and evaluate\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    print(f\"samples={args.n_samples} features={args.n_features} classes={args.n_classes}\")\n    print(f\"train_size={len(X_train)} test_size={len(X_test)}\")\n    print(f\"accuracy={accuracy:.3f}\")\n    \n    # Acceptance check\n    if accuracy >= 0.65:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {accuracy:.3f} below threshold 0.65\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Synthetic Financial Forecast",
      "description": "Generate 300 synthetic stock-like sequences, predict direction (up/down), train LogisticRegression, report accuracy. Print TEST_PASS if accuracy ≥ 0.6.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndef generate_synthetic_stock_sequences(n_sequences=300, seq_len=20, seed=42):\n    \"\"\"Generate synthetic stock-like sequences with up/down labels.\"\"\"\n    rng = np.random.default_rng(seed)\n    sequences = []\n    labels = []\n    \n    for _ in range(n_sequences):\n        # Generate a random walk with drift\n        drift = rng.uniform(-0.02, 0.02)\n        volatility = rng.uniform(0.01, 0.05)\n        returns = rng.normal(drift, volatility, seq_len)\n        prices = 100 * np.exp(np.cumsum(returns))\n        \n        # Label: 1 if final price > initial price, else 0\n        label = 1 if prices[-1] > prices[0] else 0\n        \n        sequences.append(prices)\n        labels.append(label)\n    \n    return np.array(sequences), np.array(labels)\n\ndef extract_features(sequences):\n    \"\"\"Extract simple features from price sequences.\"\"\"\n    features = []\n    for seq in sequences:\n        # Compute basic statistics\n        mean_price = np.mean(seq)\n        std_price = np.std(seq)\n        min_price = np.min(seq)\n        max_price = np.max(seq)\n        price_range = max_price - min_price\n        \n        # Compute returns\n        returns = np.diff(seq) / seq[:-1]\n        mean_return = np.mean(returns)\n        std_return = np.std(returns)\n        \n        # Trend: slope of linear fit\n        x = np.arange(len(seq))\n        trend = np.polyfit(x, seq, 1)[0]\n        \n        # Final vs initial price ratio\n        price_ratio = seq[-1] / seq[0]\n        \n        features.append([\n            mean_price, std_price, min_price, max_price, price_range,\n            mean_return, std_return, trend, price_ratio\n        ])\n    \n    return np.array(features)\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Generate 300 synthetic stock sequences, predict direction with LogisticRegression.\")\n    p.add_argument(\"--n-sequences\", type=int, default=300, help=\"Number of synthetic sequences (default: 300).\")\n    p.add_argument(\"--seq-len\", type=int, default=20, help=\"Length of each sequence (default: 20).\")\n    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    # Set seeds in main\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    # Validate inputs\n    if args.n_sequences <= 0:\n        print(\"TEST_FAIL: n_sequences must be positive\")\n        sys.exit(1)\n    if args.seq_len <= 1:\n        print(\"TEST_FAIL: seq_len must be > 1\")\n        sys.exit(1)\n    if not (0 < args.test_size < 1):\n        print(\"TEST_FAIL: test_size must be in (0, 1)\")\n        sys.exit(1)\n\n    # Generate synthetic data\n    sequences, labels = generate_synthetic_stock_sequences(\n        n_sequences=args.n_sequences,\n        seq_len=args.seq_len,\n        seed=args.seed\n    )\n    \n    if len(sequences) == 0 or len(labels) == 0:\n        print(\"TEST_FAIL: failed to generate synthetic data\")\n        sys.exit(1)\n\n    # Extract features\n    X = extract_features(sequences)\n    y = labels\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=args.test_size, random_state=args.seed, stratify=y\n    )\n\n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Train LogisticRegression\n    clf = LogisticRegression(max_iter=1000, random_state=args.seed)\n    clf.fit(X_train_scaled, y_train)\n\n    # Evaluate\n    accuracy = clf.score(X_test_scaled, y_test)\n    print(f\"n_sequences={args.n_sequences} seq_len={args.seq_len} accuracy={accuracy:.3f}\")\n\n    # Acceptance check\n    if accuracy >= 0.6:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {accuracy:.3f} < 0.6\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "MNIST Autoencoder",
      "description": "Load MNIST (or FakeData fallback), train a small autoencoder for a few epochs, reconstruct one image, and save original/reconstructed pair to autoencode.png. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport os\nimport random\nimport numpy as np\n\ndef load_mnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\n    try:\n        import torch\n        from torchvision import datasets, transforms\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n        if len(train) == 0:\n            raise RuntimeError(\"MNIST cache missing and download disabled\")\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n        return train, True\n    except Exception:\n        import torch\n        from torchvision import transforms\n        from torchvision.datasets import FakeData\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n        return train, False\n\ndef main():\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import DataLoader\n    import matplotlib\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n\n    p = argparse.ArgumentParser(description=\"MNIST autoencoder (opt-in download) or FakeData fallback; saves autoencode.png.\")\n    p.add_argument(\"--epochs\", type=int, default=3, help=\"Training epochs (default: 3).\")\n    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n    p.add_argument(\"--output\", type=str, default=\"autoencode.png\", help=\"Output image path (default: autoencode.png).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    train_ds, real = load_mnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n\n    class Autoencoder(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.encoder = nn.Sequential(\n                nn.Flatten(),\n                nn.Linear(28*28, 128),\n                nn.ReLU(),\n                nn.Linear(128, 64),\n                nn.ReLU(),\n                nn.Linear(64, 32)\n            )\n            self.decoder = nn.Sequential(\n                nn.Linear(32, 64),\n                nn.ReLU(),\n                nn.Linear(64, 128),\n                nn.ReLU(),\n                nn.Linear(128, 28*28),\n                nn.Sigmoid()\n            )\n        def forward(self, x):\n            z = self.encoder(x)\n            recon = self.decoder(z)\n            return recon.view(-1, 1, 28, 28)\n\n    model = Autoencoder()\n    opt = optim.Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.MSELoss()\n\n    model.train()\n    for epoch in range(args.epochs):\n        total_loss = 0.0\n        for xb, _ in train_loader:\n            opt.zero_grad()\n            recon = model(xb)\n            loss = loss_fn(recon, xb)\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(train_loader)\n        print(f\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\")\n\n    model.eval()\n    with torch.no_grad():\n        sample_x, _ = next(iter(DataLoader(train_ds, batch_size=1, shuffle=False)))\n        sample_recon = model(sample_x)\n\n    orig = sample_x[0, 0].cpu().numpy()\n    recon = sample_recon[0, 0].cpu().numpy()\n\n    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n    axes[0].imshow(orig, cmap='gray')\n    axes[0].set_title('Original')\n    axes[0].axis('off')\n    axes[1].imshow(recon, cmap='gray')\n    axes[1].set_title('Reconstructed')\n    axes[1].axis('off')\n    plt.tight_layout()\n    plt.savefig(args.output)\n    plt.close()\n\n    print(f\"dataset={'mnist' if real else 'fake'} output={args.output}\")\n\n    if os.path.isfile(args.output):\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "FashionMNIST VAE Latent",
      "description": "Train a small VAE on FashionMNIST (or FakeData fallback), collect 2D latent codes, and save scatter plot to latent_vae.png. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport os\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\ndef load_fashionmnist_or_fakedata(max_train=2000, seed=42, allow_download=False):\n    try:\n        import torch\n        from torchvision import datasets, transforms\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        train = datasets.FashionMNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n        if len(train) == 0:\n            raise RuntimeError(\"FashionMNIST cache missing and download disabled\")\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n        return train, True\n    except Exception:\n        import torch\n        from torchvision import transforms\n        from torchvision.datasets import FakeData\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n        return train, False\n\ndef main():\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import DataLoader\n\n    p = argparse.ArgumentParser(description=\"FashionMNIST VAE latent space visualization with opt-in download or FakeData fallback.\")\n    p.add_argument(\"--epochs\", type=int, default=2, help=\"Training epochs (default: 2).\")\n    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n    p.add_argument(\"--latent-dim\", type=int, default=2, help=\"Latent dimension (default: 2).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit FashionMNIST download if not cached.\")\n    p.add_argument(\"--output\", type=str, default=\"latent_vae.png\", help=\"Output latent space plot filename (default: latent_vae.png).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    train_ds, real = load_fashionmnist_or_fakedata(max_train=2000, seed=args.seed, allow_download=args.allow_download)\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n\n    class VAE(nn.Module):\n        def __init__(self, latent_dim=2):\n            super().__init__()\n            self.latent_dim = latent_dim\n            self.encoder = nn.Sequential(\n                nn.Flatten(),\n                nn.Linear(28*28, 256), nn.ReLU(),\n                nn.Linear(256, 128), nn.ReLU()\n            )\n            self.fc_mu = nn.Linear(128, latent_dim)\n            self.fc_logvar = nn.Linear(128, latent_dim)\n            self.decoder = nn.Sequential(\n                nn.Linear(latent_dim, 128), nn.ReLU(),\n                nn.Linear(128, 256), nn.ReLU(),\n                nn.Linear(256, 28*28), nn.Sigmoid()\n            )\n\n        def encode(self, x):\n            h = self.encoder(x)\n            return self.fc_mu(h), self.fc_logvar(h)\n\n        def reparameterize(self, mu, logvar):\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std)\n            return mu + eps * std\n\n        def decode(self, z):\n            return self.decoder(z).view(-1, 1, 28, 28)\n\n        def forward(self, x):\n            mu, logvar = self.encode(x)\n            z = self.reparameterize(mu, logvar)\n            recon = self.decode(z)\n            return recon, mu, logvar\n\n    def vae_loss(recon, x, mu, logvar):\n        bce = nn.functional.binary_cross_entropy(recon, x, reduction='sum')\n        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n        return bce + kld\n\n    model = VAE(latent_dim=args.latent_dim)\n    opt = optim.Adam(model.parameters(), lr=1e-3)\n\n    model.train()\n    for epoch in range(args.epochs):\n        total_loss = 0.0\n        for xb, _ in train_loader:\n            opt.zero_grad()\n            recon, mu, logvar = model(xb)\n            loss = vae_loss(recon, xb, mu, logvar)\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(train_loader.dataset)\n        print(f\"epoch={epoch+1}/{args.epochs} loss={avg_loss:.4f}\")\n\n    model.eval()\n    latents = []\n    labels = []\n    with torch.no_grad():\n        for xb, yb in train_loader:\n            mu, _ = model.encode(xb)\n            latents.append(mu.cpu().numpy())\n            labels.append(yb.cpu().numpy())\n    latents = np.concatenate(latents, axis=0)\n    labels = np.concatenate(labels, axis=0)\n\n    plt.figure(figsize=(8, 6))\n    scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10', alpha=0.6, s=10)\n    plt.colorbar(scatter, label='Class')\n    plt.xlabel('Latent Dim 1')\n    plt.ylabel('Latent Dim 2')\n    plt.title(f\"VAE Latent Space ({'FashionMNIST' if real else 'FakeData'})\")\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n    print(f\"Saved latent space plot to {args.output}\")\n\n    if os.path.isfile(args.output):\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "CIFAR10 Transfer Features",
      "description": "If CIFAR10 available, extract features via pretrained CNN, else use FakeData, train LogisticRegression on features, report accuracy. Print TEST_PASS if accuracy ≥ 0.6 or fallback ≥ 0.4.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\n\ndef load_cifar10_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\n    try:\n        import torch\n        from torchvision import datasets, transforms\n        torch.manual_seed(seed)\n        tfm = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        train = datasets.CIFAR10(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n        test = datasets.CIFAR10(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n        if len(train) == 0 or len(test) == 0:\n            raise RuntimeError(\"CIFAR10 cache missing and download disabled\")\n        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n        test = torch.utils.data.Subset(test, list(range(min(len(test), max_test))))\n        return train, test, True\n    except Exception:\n        import torch\n        from torchvision import transforms\n        from torchvision.datasets import FakeData\n        torch.manual_seed(seed)\n        tfm = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        train = FakeData(size=max_train, image_size=(3, 32, 32), num_classes=10, transform=tfm)\n        test = FakeData(size=max_test, image_size=(3, 32, 32), num_classes=10, transform=tfm)\n        return train, test, False\n\ndef extract_features(model, dataloader, device):\n    import torch\n    model.eval()\n    features_list = []\n    labels_list = []\n    with torch.no_grad():\n        for xb, yb in dataloader:\n            xb = xb.to(device)\n            feat = model(xb)\n            features_list.append(feat.cpu().numpy())\n            labels_list.append(yb.numpy())\n    X = np.vstack(features_list)\n    y = np.concatenate(labels_list)\n    return X, y\n\ndef main():\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader\n    from sklearn.linear_model import LogisticRegression\n\n    p = argparse.ArgumentParser(description=\"CIFAR10 transfer features (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n    p.add_argument(\"--max-train\", type=int, default=2000, help=\"Max training samples (default: 2000).\")\n    p.add_argument(\"--max-test\", type=int, default=500, help=\"Max test samples (default: 500).\")\n    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit CIFAR10 download if not cached.\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    train_ds, test_ds, real = load_cifar10_or_fakedata(\n        max_train=args.max_train,\n        max_test=args.max_test,\n        seed=args.seed,\n        allow_download=args.allow_download\n    )\n    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=False)\n    test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\n\n    device = torch.device(\"cpu\")\n\n    class FeatureExtractor(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = nn.Sequential(\n                nn.Conv2d(3, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n                nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n                nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.AdaptiveAvgPool2d(1)\n            )\n        def forward(self, x):\n            x = self.conv(x)\n            return x.view(x.size(0), -1)\n\n    model = FeatureExtractor().to(device)\n\n    print(\"Extracting training features...\")\n    X_train, y_train = extract_features(model, train_loader, device)\n    print(\"Extracting test features...\")\n    X_test, y_test = extract_features(model, test_loader, device)\n\n    if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n        print(\"TEST_FAIL: no samples extracted\")\n        sys.exit(1)\n\n    clf = LogisticRegression(max_iter=500, random_state=args.seed)\n    clf.fit(X_train, y_train)\n    acc = clf.score(X_test, y_test)\n\n    print(f\"acc={acc:.3f} dataset={'cifar10' if real else 'fake'}\")\n\n    threshold = 0.6 if real else 0.4\n    if acc >= threshold:\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "ImageFolder Augmentation",
      "description": "If ImageFolder available, apply rotation/flips to 20 images, else generate FakeData, augment, save augmented grid as 'augment_grid.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport os\nimport random\nimport numpy as np\nfrom pathlib import Path\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Apply rotation/flips to ImageFolder or FakeData; save augmented grid as augment_grid.png.\")\n    p.add_argument(\"--imagefolder-path\", type=str, default=None, help=\"Path to ImageFolder root (default: None, use FakeData).\")\n    p.add_argument(\"--num-images\", type=int, default=20, help=\"Number of images to augment (default: 20).\")\n    p.add_argument(\"--output\", type=str, default=\"augment_grid.png\", help=\"Output grid filename (default: augment_grid.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit dataset download if needed.\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        import torch\n        from torchvision import datasets, transforms\n        from torchvision.utils import make_grid\n        from PIL import Image\n    except ImportError:\n        print(\"TEST_FAIL: torchvision or PIL not available\")\n        sys.exit(1)\n\n    torch.manual_seed(args.seed)\n\n    use_imagefolder = False\n    if args.imagefolder_path and os.path.isdir(args.imagefolder_path):\n        try:\n            base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n            ds = datasets.ImageFolder(root=args.imagefolder_path, transform=base_tfm)\n            if len(ds) > 0:\n                use_imagefolder = True\n                print(f\"Using ImageFolder from {args.imagefolder_path} with {len(ds)} images\")\n        except Exception as e:\n            print(f\"ImageFolder failed: {e}, falling back to FakeData\")\n\n    if not use_imagefolder:\n        print(\"Using FakeData fallback\")\n        base_tfm = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n        ds = datasets.FakeData(size=args.num_images, image_size=(3, 64, 64), num_classes=10, transform=base_tfm)\n\n    num_samples = min(args.num_images, len(ds))\n    indices = list(range(num_samples))\n    random.shuffle(indices)\n    indices = indices[:num_samples]\n\n    augmentations = [\n        transforms.RandomRotation(degrees=30),\n        transforms.RandomHorizontalFlip(p=1.0),\n        transforms.RandomVerticalFlip(p=1.0),\n        transforms.Compose([transforms.RandomRotation(degrees=15), transforms.RandomHorizontalFlip(p=0.5)]),\n    ]\n\n    augmented_images = []\n    for idx in indices:\n        img_tensor, _ = ds[idx]\n        img_pil = transforms.ToPILImage()(img_tensor)\n        aug = random.choice(augmentations)\n        aug_pil = aug(img_pil)\n        aug_tensor = transforms.ToTensor()(aug_pil)\n        augmented_images.append(aug_tensor)\n\n    if len(augmented_images) == 0:\n        print(\"TEST_FAIL: no images to augment\")\n        sys.exit(1)\n\n    grid_tensor = make_grid(augmented_images, nrow=5, padding=2, normalize=False)\n    grid_pil = transforms.ToPILImage()(grid_tensor)\n    grid_pil.save(args.output)\n    print(f\"Saved augmented grid to {args.output} with {len(augmented_images)} images\")\n\n    if os.path.isfile(args.output):\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "FakeData Style Transfer",
      "description": "Generate FakeData content/style pairs, apply basic style transfer algorithm, save result as 'style_transfer.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nfrom pathlib import Path\n\ndef main():\n    p = argparse.ArgumentParser(description=\"FakeData style transfer: generate content/style pairs, apply basic transfer, save result.\")\n    p.add_argument(\"--output\", type=str, default=\"style_transfer.png\", help=\"Output image path (default: style_transfer.png).\")\n    p.add_argument(\"--size\", type=int, default=256, help=\"Image size (default: 256).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    # seeds in main\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        import torch\n        from torchvision.datasets import FakeData\n        from torchvision import transforms\n        from PIL import Image\n    except ImportError as e:\n        print(f\"TEST_FAIL: missing dependency {e}\")\n        sys.exit(1)\n\n    # generate FakeData content and style images\n    torch.manual_seed(args.seed)\n    tfm = transforms.Compose([\n        transforms.Resize((args.size, args.size)),\n        transforms.ToTensor()\n    ])\n    \n    fake_ds = FakeData(size=2, image_size=(3, args.size, args.size), transform=tfm)\n    content_tensor, _ = fake_ds[0]\n    style_tensor, _ = fake_ds[1]\n\n    # basic style transfer: blend content and style with simple weighted average\n    # convert to numpy for manipulation\n    content_np = content_tensor.permute(1, 2, 0).numpy()\n    style_np = style_tensor.permute(1, 2, 0).numpy()\n    \n    # simple transfer: 70% content + 30% style\n    transferred = 0.7 * content_np + 0.3 * style_np\n    transferred = np.clip(transferred, 0, 1)\n    \n    # convert back to PIL and save\n    transferred_uint8 = (transferred * 255).astype(np.uint8)\n    result_img = Image.fromarray(transferred_uint8, mode='RGB')\n    result_img.save(args.output)\n    \n    # acceptance check\n    if Path(args.output).exists():\n        print(f\"Saved style transfer result to {args.output}\")\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: output file {args.output} not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "KMeans CIFAR Segmentation",
      "description": "If CIFAR10 available, segment image via KMeans clustering, else use FakeData, save segmented image as 'segmented.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport os\nimport random\nimport numpy as np\nfrom PIL import Image\n\ndef load_cifar_or_fakedata(seed=42, allow_download=False):\n    try:\n        import torch\n        from torchvision import datasets, transforms\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        ds = datasets.CIFAR10(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n        if len(ds) == 0:\n            raise RuntimeError(\"CIFAR10 cache missing and download disabled\")\n        img_tensor, _ = ds[0]\n        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n        return img_np, True\n    except Exception:\n        import torch\n        from torchvision import transforms\n        from torchvision.datasets import FakeData\n        torch.manual_seed(seed)\n        tfm = transforms.ToTensor()\n        ds = FakeData(size=1, image_size=(3, 32, 32), num_classes=10, transform=tfm)\n        img_tensor, _ = ds[0]\n        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n        return img_np, False\n\ndef main():\n    p = argparse.ArgumentParser(description=\"KMeans CIFAR10 segmentation (opt-in download) or FakeData fallback; save segmented.png; TEST_PASS if file exists.\")\n    p.add_argument(\"--k\", type=int, default=4, help=\"Number of clusters for KMeans (default: 4).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit CIFAR10 download if not cached.\")\n    p.add_argument(\"--output\", type=str, default=\"segmented.png\", help=\"Output segmented image path (default: segmented.png).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    img, real = load_cifar_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n    if img is None or img.size == 0:\n        print(\"TEST_FAIL: image not available\")\n        sys.exit(1)\n\n    h, w, c = img.shape\n    pixels = img.reshape(-1, c).astype(np.float32)\n\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init=10, max_iter=100)\n    labels = kmeans.fit_predict(pixels)\n    centers = kmeans.cluster_centers_.astype(np.uint8)\n\n    segmented = centers[labels].reshape(h, w, c)\n\n    pil_img = Image.fromarray(segmented, mode=\"RGB\")\n    pil_img.save(args.output)\n\n    print(f\"dataset={'cifar10' if real else 'fake'} k={args.k} saved={args.output}\")\n\n    if os.path.isfile(args.output):\n        print(\"TEST_PASS\")\n    else:\n        print(\"TEST_FAIL: output file not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Iris Outlier Detection",
      "description": "Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as 'outliers.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.datasets import load_iris\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Train IsolationForest on iris dataset, detect outliers, plot normal vs outlier points, save as outliers.png.\")\n    p.add_argument(\"--contamination\", type=float, default=0.1, help=\"Expected proportion of outliers (default: 0.1).\")\n    p.add_argument(\"--output\", type=str, default=\"outliers.png\", help=\"Output plot filename (default: outliers.png).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_iris()\n        X = data.data\n        feature_names = data.feature_names\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to load iris dataset: {e}\")\n        sys.exit(1)\n\n    if X is None or len(X) == 0:\n        print(\"TEST_FAIL: iris dataset is empty\")\n        sys.exit(1)\n\n    iso = IsolationForest(contamination=args.contamination, random_state=args.seed)\n    iso.fit(X)\n    predictions = iso.predict(X)\n\n    normal_mask = predictions == 1\n    outlier_mask = predictions == -1\n\n    normal_points = X[normal_mask]\n    outlier_points = X[outlier_mask]\n\n    num_outliers = outlier_points.shape[0]\n    num_normal = normal_points.shape[0]\n\n    print(f\"Total samples: {len(X)}\")\n    print(f\"Normal points: {num_normal}\")\n    print(f\"Outliers detected: {num_outliers}\")\n\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n    ax.scatter(normal_points[:, 0], normal_points[:, 1], c='blue', label='Normal', alpha=0.6, edgecolors='k')\n    ax.scatter(outlier_points[:, 0], outlier_points[:, 1], c='red', label='Outlier', alpha=0.8, edgecolors='k', s=100)\n    ax.set_xlabel(feature_names[0])\n    ax.set_ylabel(feature_names[1])\n    ax.set_title(\"Iris Outlier Detection (IsolationForest)\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    try:\n        plt.savefig(args.output, dpi=100, bbox_inches='tight')\n        print(f\"Plot saved to {args.output}\")\n    except Exception as e:\n        print(f\"TEST_FAIL: failed to save plot: {e}\")\n        sys.exit(1)\n\n    import os\n    if not os.path.isfile(args.output):\n        print(f\"TEST_FAIL: output file {args.output} does not exist\")\n        sys.exit(1)\n\n    if os.path.getsize(args.output) == 0:\n        print(f\"TEST_FAIL: output file {args.output} is empty\")\n        sys.exit(1)\n\n    print(\"TEST_PASS\")\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    },
    {
      "title": "Wine Anomaly Score",
      "description": "Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as 'anomaly_scores.png'. Print TEST_PASS if file exists.",
      "code": "import argparse\nimport sys\nimport random\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.preprocessing import StandardScaler\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Apply OneClassSVM to wine dataset, compute anomaly scores, plot histogram, save as anomaly_scores.png.\")\n    p.add_argument(\"--output\", type=str, default=\"anomaly_scores.png\", help=\"Output histogram filename (default: anomaly_scores.png).\")\n    p.add_argument(\"--nu\", type=float, default=0.1, help=\"OneClassSVM nu parameter (default: 0.1).\")\n    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n    args = p.parse_args()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    try:\n        import torch\n        torch.manual_seed(args.seed)\n    except Exception:\n        pass\n\n    try:\n        data = load_wine()\n        X = data.data\n        if X is None or len(X) == 0:\n            raise ValueError(\"wine dataset is empty\")\n    except Exception as e:\n        print(f\"TEST_FAIL: could not load wine dataset: {e}\")\n        sys.exit(1)\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    oc_svm = OneClassSVM(nu=args.nu, kernel='rbf', gamma='auto')\n    oc_svm.fit(X_scaled)\n\n    scores = oc_svm.decision_function(X_scaled)\n\n    plt.figure(figsize=(8, 5))\n    plt.hist(scores, bins=30, edgecolor='black', alpha=0.7)\n    plt.xlabel('Anomaly Score')\n    plt.ylabel('Frequency')\n    plt.title('Wine Dataset Anomaly Scores (OneClassSVM)')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(args.output, dpi=100)\n    plt.close()\n\n    import os\n    if os.path.isfile(args.output):\n        print(f\"Histogram saved to {args.output}\")\n        print(\"TEST_PASS\")\n    else:\n        print(f\"TEST_FAIL: output file {args.output} not created\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n# END_OF_SCRIPT"
    }
  ]
}
