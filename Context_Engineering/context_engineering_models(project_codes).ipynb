{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHbnVlVPrXcuwZRf121Q1Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CmgoMnluG1Y",
        "outputId": "67d74b38-2e9c-43df-912a-052bd698dd77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is the test file to generate 10 samples (description + code) in JSON format with different models."
      ],
      "metadata": {
        "id": "eWREQ6kyUoWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Generate project descriptions and titles with three different models"
      ],
      "metadata": {
        "id": "HfK-afzwXPZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup the GPT-5 model and OpenRouter API key"
      ],
      "metadata": {
        "id": "ElC6Hg0L9q8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install requests jsonschema"
      ],
      "metadata": {
        "id": "S5uFmQWFVKMt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, re, textwrap, datetime\n",
        "import requests\n",
        "from jsonschema import Draft7Validator\n",
        "\n",
        "OPENROUTER_API_KEY = \"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\"\n",
        "MODELS = [\n",
        "    #\"openai/gpt-5\",\n",
        "    \"anthropic/claude-sonnet-4.5\",\n",
        "    \"qwen/qwen3-coder\",\n",
        "]"
      ],
      "metadata": {
        "id": "K0l84vbFVFOK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Set the variables"
      ],
      "metadata": {
        "id": "OO1bcsPjWvXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a meticulous AI project designer.\n",
        "Your job: produce concise, implementable AI mini-project ideas.\n",
        "\n",
        "Rules:\n",
        "- Output must be valid JSON ONLY (no extra text).\n",
        "- Each item has exactly two keys: \"title\" and \"description\".\n",
        "- Titles are short and specific (≤ 6 words).\n",
        "- Descriptions are 1–2 sentences, concrete, and implementable offline in 20–60 minutes.\n",
        "- Prefer single-file, single-metric projects with tiny data and fast runtime.\n",
        "- Avoid duplicate or near-duplicate ideas.\n",
        "- Prefer standard Python libs or widely used ML libs (numpy, pandas, scikit-learn, PyTorch, TensorFlow, OpenCV).\n",
        "- No external downloads; use built-in toy datasets (e.g., sklearn iris/digits) or tiny synthetic data.\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "5vOUmWY-V7b6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEW_SHOTS = \"\"\"\n",
        "{\"title\":\"Iris SVM CLI\",\n",
        " \"description\":\"Train a small SVM on sklearn's iris dataset and print test accuracy. Provide a --predict flag that accepts four comma-separated features and prints the predicted class.\"}\n",
        "{\"title\":\"Canny Edge CLI\",\n",
        " \"description\":\"Load an image path or generate a 128×128 synthetic image, run OpenCV Canny with --t1/--t2 thresholds, save edges.png, and print TEST_PASS if the edge map is non-empty.\"}\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "jkZyHY39Wi0b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. A function for describing the task"
      ],
      "metadata": {
        "id": "3heZju0CXrbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def build_task(n=10):\n",
        "    return textwrap.dedent(f\"\"\"\n",
        "    Task: Generate {n} distinct AI mini-project ideas.\n",
        "\n",
        "    Constraints:\n",
        "    - Return a JSON array of length {n}.\n",
        "    - Each item: object with exactly \"title\" (string) and \"description\" (string).\n",
        "    - No comments, no prose outside JSON.\n",
        "\n",
        "    Scope & Simplicity:\n",
        "    - Each project is doable offline in 20–60 minutes on CPU.\n",
        "    - Single-file mindset: one clear goal, one primary metric or artifact.\n",
        "    - Keep dependencies minimal (numpy/pandas/sklearn/torch/tf/opencv only).\n",
        "\n",
        "    Data:\n",
        "    - Do NOT require internet downloads.\n",
        "    - Use built-in toy datasets (e.g., sklearn iris/digits) or tiny synthetic data generation.\n",
        "    - If an image/text input is needed, allow a synthetic fallback.\n",
        "\n",
        "    Focus areas ONLY:\n",
        "    - NLP, CV, Tabular. Do not include TimeSeries or Audio.\n",
        "\n",
        "    Description style:\n",
        "    - Titles ≤ 6 words, specific.\n",
        "    - Descriptions are 1–2 sentences with concrete I/O hints (flags, paths, outputs).\n",
        "    - Include at least one quick validation (e.g., accuracy threshold, file existence, non-empty output).\n",
        "\n",
        "    Diversity (within the allowed areas):\n",
        "    - Avoid repeating the same idea or trivial variants.\n",
        "\n",
        "    Follow the style of these examples without repeating them:\n",
        "    {FEW_SHOTS}\n",
        "\n",
        "    Now produce the JSON array of {n} items.\n",
        "    \"\"\").strip()\n"
      ],
      "metadata": {
        "id": "6-5DGgCOXZjl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. OpenRouter call helper"
      ],
      "metadata": {
        "id": "Fp4QApEQXwmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to make a safe Python variable name from a slug\n",
        "def varname_from_slug(slug: str) -> str:\n",
        "    name = slug.lower().replace(\"/\", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\")\n",
        "    return f\"{name}_result\""
      ],
      "metadata": {
        "id": "NzPyoCu1N0-S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic OpenRouter caller taking model_id\n",
        "def call_openrouter_model(model_id, messages, temperature=0.2, top_p=0.9, max_tokens=6000):\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
        "        \"X-Title\": f\"Multi-Model Project Generator\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": float(temperature),\n",
        "        \"top_p\": float(top_p),\n",
        "        \"max_tokens\": int(max_tokens),\n",
        "    }\n",
        "    t0 = time.time()\n",
        "    r = requests.post(url, headers=headers, json=payload, timeout=120)\n",
        "    latency = time.time() - t0\n",
        "    r.raise_for_status()\n",
        "    content = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content, latency"
      ],
      "metadata": {
        "id": "rWSuWp7WMSRz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validator for title/description array\n",
        "from jsonschema import Draft7Validator\n",
        "def validate_items(arr, N):\n",
        "    ITEM_SCHEMA = {\n",
        "        \"type\":\"object\",\n",
        "        \"required\":[\"title\",\"description\"],\n",
        "        \"properties\":{\n",
        "            \"title\":{\"type\":\"string\",\"minLength\":3, \"maxLength\":100},\n",
        "            \"description\":{\"type\":\"string\",\"minLength\":20, \"maxLength\":600}\n",
        "        },\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "    ARRAY_SCHEMA = {\"type\":\"array\",\"items\":ITEM_SCHEMA, \"minItems\":N, \"maxItems\":N}\n",
        "    errs = [e.message for e in Draft7Validator(ARRAY_SCHEMA).iter_errors(arr)]\n",
        "    titles = [ (x.get(\"title\") or \"\").strip().lower() for x in arr ]\n",
        "    if len(set(titles)) != len(titles):\n",
        "        errs.append(\"Duplicate titles detected.\")\n",
        "    return errs"
      ],
      "metadata": {
        "id": "Vf2XvtJbOCqB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust extractor (keeps your previous logic)\n",
        "import re, json, time, requests\n",
        "def extract_json_array(text: str):\n",
        "    m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", text, re.IGNORECASE)\n",
        "    if m:\n",
        "        text = m.group(1).strip()\n",
        "    start = text.find('[')\n",
        "    if start == -1:\n",
        "        return None\n",
        "    depth = 0\n",
        "    for i in range(start, len(text)):\n",
        "        ch = text[i]\n",
        "        if ch == '[': depth += 1\n",
        "        elif ch == ']':\n",
        "            depth -= 1\n",
        "            if depth == 0:\n",
        "                return text[start:i+1]\n",
        "    return None"
      ],
      "metadata": {
        "id": "FPYRGxi3OGxS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single shared message set\n",
        "N = 10\n",
        "task = build_task(N) + \"\\n\\nReturn a raw JSON array only — no prose, no code fences, no markdown.\"\n",
        "messages = [\n",
        "    {\"role\":\"system\",\"content\": SYSTEM_PROMPT},\n",
        "    {\"role\":\"user\",\"content\": task}\n",
        "]\n",
        "\n",
        "# Loop models and create a separate variable per model with the results\n",
        "for model_id in MODELS:\n",
        "    print(f\"\\n===== {model_id} =====\")\n",
        "    varname = varname_from_slug(model_id)         # e.g. \"openai_gpt_5_result\"\n",
        "    try:\n",
        "        raw, secs = call_openrouter_model(model_id, messages, temperature=0.2)\n",
        "        json_str = extract_json_array(raw)\n",
        "        if not json_str:\n",
        "            globals()[varname] = {\n",
        "                \"raw\": raw, \"json_str\": None, \"items\": None,\n",
        "                \"errors\": [\"No JSON array found\"], \"latency\": secs\n",
        "            }\n",
        "            print(f\"❌ No JSON array found | {secs:.1f}s\")\n",
        "            continue\n",
        "\n",
        "        items = json.loads(json_str)\n",
        "        errors = validate_items(items, N)\n",
        "\n",
        "        globals()[varname] = {\n",
        "            \"raw\": raw, \"json_str\": json_str, \"items\": items,\n",
        "            \"errors\": errors, \"latency\": secs\n",
        "        }\n",
        "\n",
        "        if errors:\n",
        "            print(f\"⚠️ Parsed but validation errors ({len(errors)}) | {secs:.1f}s\")\n",
        "            for e in errors[:5]:\n",
        "                print(\" -\", e)\n",
        "        else:\n",
        "            print(f\"✅ Valid JSON ({len(items)} items) | {secs:.1f}s\")\n",
        "            print(json.dumps(items[:2], indent=2, ensure_ascii=False))  # preview first 2\n",
        "    except Exception as e:\n",
        "        globals()[varname] = {\n",
        "            \"raw\": None, \"json_str\": None, \"items\": None,\n",
        "            \"errors\": [str(e)], \"latency\": None\n",
        "        }\n",
        "        print(\"❌ Exception:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoWEsUznP-wi",
        "outputId": "0dac8420-b409-45c2-ce40-51e71f8bec9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== anthropic/claude-sonnet-4.5 =====\n",
            "✅ Valid JSON (10 items) | 15.0s\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Digits KNN Classifier\",\n",
            "    \"description\": \"Train a k-NN classifier on sklearn's digits dataset and report test accuracy. Accept --k flag to set neighbors and --predict flag with a 64-element comma-separated list to classify a single digit.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Text Sentiment Analyzer\",\n",
            "    \"description\": \"Generate 200 synthetic sentences with positive/negative labels using keyword rules, train a logistic regression on TF-IDF features, and print validation accuracy. Accept --text flag to classify a new sentence.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "===== qwen/qwen3-coder =====\n",
            "✅ Valid JSON (10 items) | 15.8s\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Digit Classifier CLI\",\n",
            "    \"description\": \"Train a logistic regression on sklearn's digits dataset and print test accuracy. Add a --predict flag to classify a comma-separated 64-element pixel vector from stdin.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Blob Clustering\",\n",
            "    \"description\": \"Generate 2D isotropic Gaussian blobs, fit k-means with k=3, plot clusters with centroids, and save to clusters.png. Validate with inertia < 50.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model_id in MODELS:\n",
        "    print(\" -\", varname_from_slug(model_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEr6Y84rbtaM",
        "outputId": "9694da18-7aa8-433a-a7de-f1b9e2623e2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - anthropic_claude_sonnet_4_5_result\n",
            " - qwen_qwen3_coder_result\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All the resuts for three models that I used are below:"
      ],
      "metadata": {
        "id": "4lC6XKaeW9ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(anthropic_claude_sonnet_4_5_result['items'], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXqi-EmWRcja",
        "outputId": "f1cbc297-6981-4f6b-9fff-8085bcc94fa9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Digits KNN Classifier\",\n",
            "    \"description\": \"Train a k-NN classifier on sklearn's digits dataset and report test accuracy. Accept --k flag to set neighbors and --predict flag with a 64-element comma-separated list to classify a single digit.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Text Sentiment Analyzer\",\n",
            "    \"description\": \"Generate 200 synthetic sentences with positive/negative labels using keyword rules, train a logistic regression on TF-IDF features, and print validation accuracy. Accept --text flag to classify a new sentence.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Gaussian Blob Clustering CLI\",\n",
            "    \"description\": \"Generate 300 2D points from three Gaussian blobs, run k-means clustering, and save a scatter plot colored by cluster. Print silhouette score and TEST_PASS if score > 0.5.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST Subset Neural Net\",\n",
            "    \"description\": \"Load 1000 samples from sklearn's digits dataset, train a small PyTorch MLP for 10 epochs, and print final test accuracy. Save model weights to digits_model.pth and verify file exists.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Wine Quality Decision Tree\",\n",
            "    \"description\": \"Train a decision tree on sklearn's wine dataset, print test accuracy and feature importances. Accept --max_depth flag to control tree depth and --export to save tree visualization as tree.dot.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Sobel Filter Image Processor\",\n",
            "    \"description\": \"Apply OpenCV Sobel operator (x and y gradients) to a synthetic or loaded grayscale image, compute gradient magnitude, save sobel_out.png, and print TEST_PASS if mean magnitude > 10.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Breast Cancer Logistic Regression\",\n",
            "    \"description\": \"Train logistic regression on sklearn's breast cancer dataset, print test accuracy and confusion matrix. Accept --threshold flag to adjust classification threshold and report precision/recall.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic NER Tagger\",\n",
            "    \"description\": \"Generate 100 synthetic sentences with person/location tags using templates, train a simple token-level classifier, and print F1 score. Accept --sentence flag to tag entities in new input.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Histogram Equalization CLI\",\n",
            "    \"description\": \"Load or generate a low-contrast grayscale image, apply OpenCV histogram equalization, save equalized.png, and print TEST_PASS if output histogram variance exceeds input by 20%.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Boston Housing Ridge Regressor\",\n",
            "    \"description\": \"Train ridge regression on a synthetic housing dataset (10 features, 500 samples), print RMSE and R² score. Accept --alpha flag for regularization strength and --predict with feature vector for price estimation.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(qwen_qwen3_coder_result['items'], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv0RTeu2Ur29",
        "outputId": "fa4f2c7f-e902-4517-b612-a0171662e327"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Digit Classifier CLI\",\n",
            "    \"description\": \"Train a logistic regression on sklearn's digits dataset and print test accuracy. Add a --predict flag to classify a comma-separated 64-element pixel vector from stdin.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Blob Clustering\",\n",
            "    \"description\": \"Generate 2D isotropic Gaussian blobs, fit k-means with k=3, plot clusters with centroids, and save to clusters.png. Validate with inertia < 50.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Iris Decision Tree Viz\",\n",
            "    \"description\": \"Train a decision tree on iris dataset, export to iris_tree.dot, and print TEST_PASS if the dot file contains at least 5 nodes.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"MNIST CNN Scratch\",\n",
            "    \"description\": \"Build a 2-layer CNN in PyTorch on MNIST (use torchvision.datasets.MNIST with download=False), train for 1 epoch, and print final train accuracy > 80%.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Text Sentiment SVM\",\n",
            "    \"description\": \"Use sklearn's movie_reviews corpus (or synthetic binary strings), train a TF-IDF + SVM, and print accuracy. Include a --predict flag for raw text input.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Hough Circle Detector\",\n",
            "    \"description\": \"Load image or create 64x64 synthetic with 3 circles, apply HoughCircles, draw detected circles, save output.png, and print TEST_PASS if 3 circles found.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Boston Outlier Removal\",\n",
            "    \"description\": \"Load sklearn's boston dataset, apply IsolationForest, print number of outliers removed, and save clean data to cleaned.csv with < 50 rows removed.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic XOR MLP\",\n",
            "    \"description\": \"Generate 2D XOR data, train a 3-layer MLP with PyTorch, plot decision boundary, save to xor_mlp.png, and print TEST_PASS if loss < 0.1.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Face Blur Detector\",\n",
            "    \"description\": \"Use synthetic 32x32 grayscale faces (or numpy arrays), compute Laplacian variance, classify as blurry if < 100, and print blur count from 10 samples.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Color Quantization KMeans\",\n",
            "    \"description\": \"Load image or generate 64x64 RGB image, reshape pixels, apply k-means with k=4, replace pixels with cluster centers, save quantized.png, and print TEST_PASS if 4 unique colors.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generating the codes for each projects that have been generated earlier with the models"
      ],
      "metadata": {
        "id": "HXJrkTU-XJ3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- External memory (compact, curated) --------\n",
        "memory = {\n",
        "  \"style_guide\": [\n",
        "    \"Single-file script with `if __name__ == '__main__':` entrypoint.\",\n",
        "    \"Use argparse with clear --help and sensible defaults.\",\n",
        "    \"Prefer standard library datasets (sklearn, torchvision, keras).\",\n",
        "    \"Attempt auto-download/cache with short timeout; if unavailable, fallback to a tiny structured synthetic dataset.\",\n",
        "    \"Fix randomness: set seeds for random, numpy; torch if used; run on CPU by default.\",\n",
        "    \"Validate inputs (paths, columns, image loads) and fail gracefully with one-line reason.\",\n",
        "    \"Keep runtime < 2 minutes (few epochs, small subsets).\",\n",
        "    \"Print `TEST_PASS` on success; otherwise `TEST_FAIL: <reason>`.\"\n",
        "  ],\n",
        "  \"lessons\": [\n",
        "    \"When standardizing features use sklearn.pipeline.Pipeline to avoid leakage.\",\n",
        "    \"For OpenCV Canny, expose --threshold1 and --threshold2; convert to grayscale before edges.\",\n",
        "    \"For CSV tasks, explicitly validate required columns; show a friendly error if missing.\",\n",
        "    \"For plotting, save figures to disk and plt.close() to avoid backend issues.\",\n",
        "    \"One file only. Return exactly ONE ```python block. No extra prose.\",\n",
        "    \"CLI + help. Use a single 'argparse.ArgumentParser()'. All help strings are single-line (no embedded newlines).\",\n",
        "    \"Seeds in 'main()'. Expose '--seed' and set seeds for random, numpy, and torch (if present) inside main().\",\n",
        "    \"Data access policy. Only use library datasets when --allow-download is passed. Otherwise do not download; use a robust fallback (sklearn tabular, torchvision FakeData, PIL shapes, etc.). If using 20NG, call with download_if_missing=False unless allowed.\",\n",
        "    \"Task–dataset match. Choose datasets that match the task (e.g., do not use 20 Newsgroups for spam/ham).\",\n",
        "    \"CV safety. For OpenCV: 1. convert to grayscale if needed. 2. ensure input to detectors is uint8 (cv2.convertScaleAbs if needed). 3. for Haar, check face_cascade.empty() == False or fail.\",\n",
        "    \"Acceptance contract. Implement explicit pass/fail checks (files exist, metrics ≥ thresholds, non-empty edge map, etc.). Print TEST_PASS only when all conditions hold; otherwise TEST_FAIL: <reason> and sys.exit(1).\",\n",
        "    \"No broken syntax. Never split identifiers across lines. Never break f-strings or string literals across lines.\",\n",
        "    \"End marker. Append '# END_OF_SCRIPT' as the last line of the file.\"\n",
        "  ],\n",
        "  \"snippets\": [\n",
        "    # seed block to embed in each script\n",
        "    \"import random, numpy as np\\nrandom.seed(42)\\nnp.random.seed(42)\\ntry:\\n    import torch\\n    torch.manual_seed(42)\\nexcept Exception:\\n    pass\"\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "9lD2Trr6Ux6B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT_CODEGEN = \"\"\"\n",
        "You are a meticulous senior Python engineer who writes production-quality, runnable scripts.\n",
        "Priorities: (1) correctness, (2) reproducibility, (3) clarity, (4) speed.\n",
        "\n",
        "Formatting & Output Contract:\n",
        "- Return ONE code block only: ```python ...```\n",
        "- The code must be a single file with `if __name__ == \"__main__\":` entrypoint.\n",
        "- Provide a clear CLI via argparse and `--help`. All help strings must be single-line (no embedded newlines).\n",
        "- Do not print explanations. Do not include markdown outside the single code block.\n",
        "- Append `# END_OF_SCRIPT` as the final line of the file.\n",
        "\n",
        "Data Access Policy:\n",
        "- Prefer standard, library-provided datasets that Colab typically caches:\n",
        "  - sklearn.datasets (iris, wine, diabetes, digits)\n",
        "  - torchvision.datasets (MNIST, FashionMNIST, CIFAR-10/100)\n",
        "  - tensorflow.keras.datasets (mnist, cifar10/cifar100, imdb, reuters)\n",
        "- Only attempt loading/auto-downloading these datasets when the user passes `--allow-download`.\n",
        "- If `--allow-download` is not passed, do NOT download; immediately fall back to a robust local/synthetic option\n",
        "  (e.g., sklearn tabular, torchvision FakeData, PIL-drawn shapes, sklearn digits).\n",
        "- For sklearn 20 Newsgroups, call with `download_if_missing=False` unless `--allow-download` is passed.\n",
        "- Use small subsets and few epochs to keep runtime < 2 minutes.\n",
        "- Do not call arbitrary external URLs or third-party APIs.\n",
        "\n",
        "Behavioral Rules:\n",
        "- Expose `--seed` and set seeds **inside `main()`** for `random`, `numpy`, and `torch` (if available); run on CPU by default.\n",
        "- Choose datasets that match the task semantics (e.g., do NOT use 20 Newsgroups for spam/ham).\n",
        "- Validate inputs (paths, columns, image loads, audio length) and fail gracefully with a concise message.\n",
        "- For OpenCV tasks: convert to grayscale when needed; ensure `uint8` input (use `cv2.convertScaleAbs` if necessary);\n",
        "  for Haar cascades, ensure `face_cascade.empty() == False` or fail.\n",
        "- Keep the code minimal, readable, and fully runnable in a fresh Colab.\n",
        "- Never split identifiers across lines; never break string literals or f-strings across lines.\n",
        "- Implement explicit acceptance checks tied to the task (files exist, metrics ≥ thresholds, non-empty edge map, etc.).\n",
        "- Print `TEST_PASS` only when all acceptance conditions hold; otherwise print `TEST_FAIL: <reason>` and `sys.exit(1)`.\n",
        "\n",
        "Self-Check Before Returning (silently revise if any item fails):\n",
        "- argparse help strings are single-line.\n",
        "- Seeds are applied in `main()` for random/numpy/torch.\n",
        "- No downloads unless `--allow-download` is passed; otherwise robust fallback is used.\n",
        "- Dataset matches task semantics.\n",
        "- Acceptance checks implemented; `TEST_PASS`/`TEST_FAIL` behavior present.\n",
        "- File ends with `# END_OF_SCRIPT`.\n",
        "- Code parses without SyntaxError.\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "dTqjD9yBhJ6O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEWSHOTS_CODE = \"\"\"\n",
        "Example A (tabular classification with sklearn iris -> fallback synthetic; seeds-in-main; single-line help; acceptance checks)\n",
        "```python\n",
        "import argparse, sys\n",
        "import random, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def load_iris_or_synthetic(seed=42):\n",
        "    try:\n",
        "        from sklearn.datasets import load_iris  # no download required\n",
        "        data = load_iris()\n",
        "        X, y, used = data.data, data.target, \"iris\"\n",
        "    except Exception:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        n = 210\n",
        "        c = rng.integers(0, 3, size=n)\n",
        "        X = rng.normal(0, 1, size=(n, 4)) + c[:, None] * 1.5\n",
        "        y, used = c, \"synthetic\"\n",
        "    return X, y, used\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"Iris (no-download) or synthetic fallback; seeds set in main; explicit acceptance.\")\n",
        "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    # seeds in main\n",
        "    random.seed(args.seed); np.random.seed(args.seed)\n",
        "    try:\n",
        "        import torch; torch.manual_seed(args.seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    X, y, used = load_iris_or_synthetic(args.seed)\n",
        "    if X is None or y is None or len(X) == 0:\n",
        "        print(\"TEST_FAIL: dataset not available\"); sys.exit(1)\n",
        "\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\n",
        "    clf = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LogisticRegression(max_iter=300))])\n",
        "    clf.fit(Xtr, ytr)\n",
        "    acc = clf.score(Xte, yte)\n",
        "    print(f\"dataset={used} accuracy={acc:.3f}\")\n",
        "    # acceptance: stricter if iris, looser if synthetic\n",
        "    if acc >= (0.85 if used == \"iris\" else 0.70):\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: accuracy below threshold\"); sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\n",
        "```\n",
        "\n",
        "Example B (vision MNIST with opt-in download -> fallback FakeData; uint8 safety; seeds-in-main; acceptance checks)\n",
        "```python\n",
        "import argparse, sys, os\n",
        "import random, numpy as np\n",
        "\n",
        "def load_mnist_or_fakedata(max_train=2000, max_test=500, seed=42, allow_download=False):\n",
        "    try:\n",
        "        import torch\n",
        "        from torchvision import datasets, transforms\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        # only download if explicitly allowed\n",
        "        train = datasets.MNIST(root=\"./data\", train=True, download=bool(allow_download), transform=tfm)\n",
        "        test  = datasets.MNIST(root=\"./data\", train=False, download=bool(allow_download), transform=tfm)\n",
        "        # if dataset objects are empty because cache missing and download disabled, trigger fallback\n",
        "        if len(train) == 0 or len(test) == 0:\n",
        "            raise RuntimeError(\"MNIST cache missing and download disabled\")\n",
        "        train = torch.utils.data.Subset(train, list(range(min(len(train), max_train))))\n",
        "        test  = torch.utils.data.Subset(test,  list(range(min(len(test),  max_test))))\n",
        "        return train, test, True\n",
        "    except Exception:\n",
        "        import torch\n",
        "        from torchvision import transforms\n",
        "        from torchvision.datasets import FakeData\n",
        "        torch.manual_seed(seed)\n",
        "        tfm = transforms.ToTensor()\n",
        "        train = FakeData(size=max_train, image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
        "        test  = FakeData(size=max_test,  image_size=(1, 28, 28), num_classes=10, transform=tfm)\n",
        "        return train, test, False\n",
        "\n",
        "def main():\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    p = argparse.ArgumentParser(description=\"MNIST (opt-in download) or FakeData fallback; seeds in main; explicit acceptance.\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=1, help=\"Training epochs (default: 1).\")\n",
        "    p.add_argument(\"--batch\", type=int, default=128, help=\"Batch size (default: 128).\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
        "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Permit MNIST download if not cached.\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    # seeds in main\n",
        "    random.seed(args.seed); np.random.seed(args.seed); torch.manual_seed(args.seed)\n",
        "\n",
        "    train_ds, test_ds, real = load_mnist_or_fakedata(seed=args.seed, allow_download=args.allow_download)\n",
        "    train = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
        "    test  = DataLoader(test_ds,  batch_size=args.batch, shuffle=False)\n",
        "\n",
        "    class TinyCNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Conv2d(1, 16, 3, 1), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(16, 32, 3, 1), nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(32*5*5, 64), nn.ReLU(),\n",
        "                nn.Linear(64, 10)\n",
        "            )\n",
        "        def forward(self, x): return self.net(x)\n",
        "\n",
        "    model = TinyCNN()\n",
        "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for _ in range(args.epochs):\n",
        "        for xb, yb in train:\n",
        "            # ensure uint8 -> float32 is handled by ToTensor; just train\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "    # eval + acceptance\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test:\n",
        "            pred = model(xb).argmax(1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.numel()\n",
        "    acc = correct / max(total, 1)\n",
        "    print(f\"acc={acc:.3f} dataset={'mnist' if real else 'fake'}\")\n",
        "    # stricter if real, looser if fake\n",
        "    if acc >= (0.85 if real else 0.20):\n",
        "        print(\"TEST_PASS\")\n",
        "    else:\n",
        "        print(\"TEST_FAIL: accuracy below threshold\"); sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# END_OF_SCRIPT\n",
        "```\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "Zfk26-lPhjg-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, requests\n",
        "\n",
        "# --- helpers ---\n",
        "def extract_code_block(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Return the first code block content if present; otherwise return the whole text.\n",
        "    Prefers ```python ... ``` but accepts ``` ... ```.\n",
        "    \"\"\"\n",
        "    m = re.search(r\"```(?:python)?\\s*([\\s\\S]*?)\\s*```\", text, re.IGNORECASE)\n",
        "    return m.group(1) if m else text\n",
        "\n",
        "def print_long(s: str, width: int = 4000):\n",
        "    \"\"\"Print long strings without Colab truncation, in chunks.\"\"\"\n",
        "    for i in range(0, len(s), width):\n",
        "        print(s[i:i+width])"
      ],
      "metadata": {
        "id": "OqSL3SvAmQsI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Task builder (per project) --------\n",
        "def build_code_task(project, memory):\n",
        "    guide = \"\\n- \".join(memory[\"style_guide\"])\n",
        "    lessons = \"\\n- \".join(memory[\"lessons\"][-6:])\n",
        "    seed_block = memory[\"snippets\"][0]\n",
        "\n",
        "    return f\"\"\"\n",
        "    PROJECT TITLE:\n",
        "    {project['title']}\n",
        "\n",
        "    PROJECT DESCRIPTION:\n",
        "    {project['description']}\n",
        "\n",
        "    Follow this style guide:\n",
        "    - {guide}\n",
        "\n",
        "    Incorporate recent lessons:\n",
        "    - {lessons}\n",
        "\n",
        "    Hard guardrails (must follow):\n",
        "    - Return ONE code block only: ```python ...``` (no extra prose).\n",
        "    - Single file with `if __name__ == \"__main__\":` entrypoint.\n",
        "    - Use argparse; **all help strings are single-line** (no embedded newlines).\n",
        "    - Expose `--seed` and set seeds **inside `main()`** for random, numpy, and torch (if available).\n",
        "    - **Data policy:** Only attempt library dataset downloads when `--allow-download` is passed. Otherwise DO NOT download; use a robust fallback suited to this task (e.g., sklearn tabular, torchvision FakeData, PIL-drawn shapes, sklearn digits).\n",
        "    - If using sklearn 20 Newsgroups, call with `download_if_missing=False` unless `--allow-download` is passed.\n",
        "    - Choose datasets that **match the task semantics** (e.g., do NOT use 20 Newsgroups for spam/ham).\n",
        "    - For OpenCV tasks: convert to grayscale when needed; ensure `uint8` input (use `cv2.convertScaleAbs` if necessary); for Haar cascades verify `face_cascade.empty()==False` or fail.\n",
        "    - Never split identifiers across lines; never break string literals or f-strings across lines.\n",
        "\n",
        "    Embed this seed block near the top of the script:\n",
        "    ```python\n",
        "    {seed_block}\"\"\"\n"
      ],
      "metadata": {
        "id": "GAf1N6exiOEE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- OpenRouter caller (model passed in) --------\n",
        "\n",
        "def call_openrouter_model(model_id, messages, temperature=0.2, top_p=0.9, max_tokens=7000):\n",
        "  url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "  headers = {\n",
        "  \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
        "  \"X-Title\": \"Code Generation (per-model)\",\n",
        "  }\n",
        "  if not headers[\"Authorization\"]:\n",
        "    raise RuntimeError(\"OPENROUTER_API_KEY environment variable not set.\")\n",
        "  payload = {\n",
        "    \"model\": model_id,\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": float(temperature),\n",
        "    \"top_p\": float(top_p),\n",
        "    \"max_tokens\": int(max_tokens),\n",
        "    }\n",
        "  r = requests.post(url, headers=headers, json=payload, timeout=120)\n",
        "  r.raise_for_status()\n",
        "  return r.json()[\"choices\"][0][\"message\"][\"content\"]"
      ],
      "metadata": {
        "id": "o4q9n4p5itoy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------- Map: model slug -> per-model dict variable name --------\n",
        "\n",
        "MODEL_TO_RESULTVAR = {\n",
        "#\"openai/gpt-5\": \"openai_gpt_5_result\",\n",
        "\"anthropic/claude-sonnet-4.5\": \"anthropic_claude_sonnet_4_5_result\" ,\n",
        "\"qwen/qwen3-coder\": \"qwen_qwen3_coder_result\"\n",
        "}"
      ],
      "metadata": {
        "id": "jhRCCtsKjETU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Generate code for the first 2 projects per model --------\n",
        "import time\n",
        "\n",
        "print(\"Generating code for first 2 items of each model's projects...\\n\")\n",
        "per_model_generated_code = {}\n",
        "\n",
        "def code_items_varname(slug: str) -> str:\n",
        "    return slug.lower().replace(\"/\", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\") + \"_code_items\"\n",
        "\n",
        "\n",
        "for model_id, varname in MODEL_TO_RESULTVAR.items():\n",
        "    result = globals().get(varname)\n",
        "    if not result or not result.get(\"items\"):\n",
        "        print(f\"Skipping {model_id}: no items found in `{varname}`\")\n",
        "        continue\n",
        "\n",
        "    projects = result[\"items\"][:2]  # first 2 only for test\n",
        "    print(f\"\\n===== {model_id}: generating and attaching code for {len(projects)} projects =====\")\n",
        "\n",
        "    items_with_code = []\n",
        "    for idx, proj in enumerate(projects, 1):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT_CODEGEN},\n",
        "            {\"role\": \"user\", \"content\": \"Study these two short code examples and copy their structure (CLI, dataset policy, seeds, TEST_PASS contract).\"},\n",
        "            {\"role\": \"assistant\", \"content\": FEWSHOTS_CODE},\n",
        "            {\"role\": \"user\", \"content\": build_code_task(proj, memory)},\n",
        "        ]\n",
        "        raw = call_openrouter_model(model_id, messages, temperature=0.2, max_tokens=7000)\n",
        "        code = extract_code_block(raw)\n",
        "        time.sleep(10)\n",
        "\n",
        "        item = {\n",
        "            \"title\": proj[\"title\"],\n",
        "            \"description\": proj[\"description\"],\n",
        "            \"code\": code\n",
        "        }\n",
        "        items_with_code.append(item)\n",
        "\n",
        "        # show the full code (no truncation)\n",
        "        print(f\"\\n--- {model_id} • Project {idx}: {proj['title']} ---\\n\")\n",
        "        print_long(code)  # full code printed\n",
        "\n",
        "    # put per-model list into a dedicated variable\n",
        "    var_codes = code_items_varname(model_id)  # e.g., openai_gpt_5_code_items\n",
        "    globals()[var_codes] = items_with_code\n",
        "\n",
        "    # also store inside the original result dict under 'items_with_code' for convenience\n",
        "    result[\"items_with_code\"] = items_with_code\n",
        "\n",
        "    # pretty JSON view of the per-model list\n",
        "    print(f\"\\n>>> {model_id} • JSON with title, description, code:\")\n",
        "    print_long(json.dumps(items_with_code, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S6ZV3JUYkuZW",
        "outputId": "d46701f0-6b21-4a9b-c531-f11c1ff35ea6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating code for first 2 items of each model's projects...\n",
            "\n",
            "\n",
            "===== anthropic/claude-sonnet-4.5: generating and attaching code for 2 projects =====\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 1: Digits KNN Classifier ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.neighbors import KNeighborsClassifier\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "def load_digits_or_synthetic(seed=42):\n",
            "    \"\"\"Load sklearn digits (no download required) or fallback to synthetic.\"\"\"\n",
            "    try:\n",
            "        from sklearn.datasets import load_digits\n",
            "        data = load_digits()\n",
            "        X, y, used = data.data, data.target, \"digits\"\n",
            "    except Exception:\n",
            "        rng = np.random.default_rng(seed)\n",
            "        n = 1797\n",
            "        X = rng.uniform(0, 16, size=(n, 64))\n",
            "        y = rng.integers(0, 10, size=n)\n",
            "        used = \"synthetic\"\n",
            "    return X, y, used\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train k-NN on sklearn digits; predict single digit from 64-element CSV.\")\n",
            "    p.add_argument(\"--k\", type=int, default=3, help=\"Number of neighbors (default: 3).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--predict\", type=str, default=None, help=\"Comma-separated 64 floats to classify a single digit.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    # seeds in main\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    X, y, used = load_digits_or_synthetic(args.seed)\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    # single prediction mode\n",
            "    if args.predict is not None:\n",
            "        parts = args.predict.split(\",\")\n",
            "        if len(parts) != 64:\n",
            "            print(f\"TEST_FAIL: --predict requires exactly 64 values, got {len(parts)}\")\n",
            "            sys.exit(1)\n",
            "        try:\n",
            "            sample = np.array([float(v) for v in parts]).reshape(1, -1)\n",
            "        except ValueError:\n",
            "            print(\"TEST_FAIL: --predict values must be numeric\")\n",
            "            sys.exit(1)\n",
            "        clf = KNeighborsClassifier(n_neighbors=args.k)\n",
            "        clf.fit(X, y)\n",
            "        pred = clf.predict(sample)[0]\n",
            "        print(f\"dataset={used} k={args.k} predicted_digit={pred}\")\n",
            "        print(\"TEST_PASS\")\n",
            "        return\n",
            "\n",
            "    # train/test mode\n",
            "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\n",
            "    clf = KNeighborsClassifier(n_neighbors=args.k)\n",
            "    clf.fit(Xtr, ytr)\n",
            "    ypred = clf.predict(Xte)\n",
            "    acc = accuracy_score(yte, ypred)\n",
            "    print(f\"dataset={used} k={args.k} test_accuracy={acc:.3f}\")\n",
            "\n",
            "    # acceptance: stricter if real digits, looser if synthetic\n",
            "    threshold = 0.90 if used == \"digits\" else 0.15\n",
            "    if acc >= threshold:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(f\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold:.3f}\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            "--- anthropic/claude-sonnet-4.5 • Project 2: Synthetic Text Sentiment Analyzer ---\n",
            "\n",
            "import argparse\n",
            "import sys\n",
            "import random\n",
            "import numpy as np\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "def generate_synthetic_sentences(n=200, seed=42):\n",
            "    rng = np.random.default_rng(seed)\n",
            "    pos_words = [\"excellent\", \"amazing\", \"wonderful\", \"great\", \"fantastic\", \"love\", \"perfect\", \"best\", \"happy\", \"brilliant\"]\n",
            "    neg_words = [\"terrible\", \"awful\", \"horrible\", \"worst\", \"hate\", \"bad\", \"disappointing\", \"poor\", \"sad\", \"useless\"]\n",
            "    neutral = [\"the\", \"is\", \"was\", \"a\", \"an\", \"this\", \"that\", \"it\", \"very\", \"really\", \"quite\", \"so\", \"much\"]\n",
            "    \n",
            "    sentences = []\n",
            "    labels = []\n",
            "    for _ in range(n):\n",
            "        label = rng.integers(0, 2)\n",
            "        if label == 1:\n",
            "            core = rng.choice(pos_words, size=rng.integers(1, 3), replace=True)\n",
            "        else:\n",
            "            core = rng.choice(neg_words, size=rng.integers(1, 3), replace=True)\n",
            "        filler = rng.choice(neutral, size=rng.integers(2, 5), replace=True)\n",
            "        words = list(core) + list(filler)\n",
            "        rng.shuffle(words)\n",
            "        sentences.append(\" \".join(words))\n",
            "        labels.append(label)\n",
            "    return sentences, np.array(labels)\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Generate synthetic sentiment sentences, train logistic regression on TF-IDF, and classify new text.\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed (default: 42).\")\n",
            "    p.add_argument(\"--n-samples\", type=int, default=200, help=\"Number of synthetic sentences (default: 200).\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set fraction (default: 0.2).\")\n",
            "    p.add_argument(\"--text\", type=str, default=None, help=\"Optional sentence to classify after training.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    if args.n_samples < 10:\n",
            "        print(\"TEST_FAIL: n_samples must be >= 10\")\n",
            "        sys.exit(1)\n",
            "    if not (0.0 < args.test_size < 1.0):\n",
            "        print(\"TEST_FAIL: test_size must be in (0, 1)\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    sentences, labels = generate_synthetic_sentences(n=args.n_samples, seed=args.seed)\n",
            "    if len(sentences) == 0 or len(labels) == 0:\n",
            "        print(\"TEST_FAIL: failed to generate synthetic data\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    X_train, X_test, y_train, y_test = train_test_split(\n",
            "        sentences, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\n",
            "    )\n",
            "\n",
            "    vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
            "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
            "    X_test_tfidf = vectorizer.transform(X_test)\n",
            "\n",
            "    clf = LogisticRegression(max_iter=300, random_state=args.seed)\n",
            "    clf.fit(X_train_tfidf, y_train)\n",
            "\n",
            "    y_pred = clf.predict(X_test_tfidf)\n",
            "    acc = accuracy_score(y_test, y_pred)\n",
            "    print(f\"validation_accuracy={acc:.3f}\")\n",
            "\n",
            "    if args.text is not None:\n",
            "        text_tfidf = vectorizer.transform([args.text])\n",
            "        pred_label = clf.predict(text_tfidf)[0]\n",
            "        sentiment = \"positive\" if pred_label == 1 else \"negative\"\n",
            "        print(f\"input_text='{args.text}' predicted_sentiment={sentiment}\")\n",
            "\n",
            "    if acc >= 0.70:\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: accuracy below 0.70\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n",
            "\n",
            ">>> anthropic/claude-sonnet-4.5 • JSON with title, description, code:\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Digits KNN Classifier\",\n",
            "    \"description\": \"Train a k-NN classifier on sklearn's digits dataset and report test accuracy. Accept --k flag to set neighbors and --predict flag with a 64-element comma-separated list to classify a single digit.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\ndef load_digits_or_synthetic(seed=42):\\n    \\\"\\\"\\\"Load sklearn digits (no download required) or fallback to synthetic.\\\"\\\"\\\"\\n    try:\\n        from sklearn.datasets import load_digits\\n        data = load_digits()\\n        X, y, used = data.data, data.target, \\\"digits\\\"\\n    except Exception:\\n        rng = np.random.default_rng(seed)\\n        n = 1797\\n        X = rng.uniform(0, 16, size=(n, 64))\\n        y = rng.integers(0, 10, size=n)\\n        used = \\\"synthetic\\\"\\n    return X, y, used\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train k-NN on sklearn digits; predict single digit from 64-element CSV.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=3, help=\\\"Number of neighbors (default: 3).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--predict\\\", type=str, default=None, help=\\\"Comma-separated 64 floats to classify a single digit.\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y, used = load_digits_or_synthetic(args.seed)\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    # single prediction mode\\n    if args.predict is not None:\\n        parts = args.predict.split(\\\",\\\")\\n        if len(parts) != 64:\\n            print(f\\\"TEST_FAIL: --predict requires exactly 64 values, got {len(parts)}\\\")\\n            sys.exit(1)\\n        try:\\n            sample = np.array([float(v) for v in parts]).reshape(1, -1)\\n        except ValueError:\\n            print(\\\"TEST_FAIL: --predict values must be numeric\\\")\\n            sys.exit(1)\\n        clf = KNeighborsClassifier(n_neighbors=args.k)\\n        clf.fit(X, y)\\n        pred = clf.predict(sample)[0]\\n        print(f\\\"dataset={used} k={args.k} predicted_digit={pred}\\\")\\n        print(\\\"TEST_PASS\\\")\\n        return\\n\\n    # train/test mode\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\\n    clf = KNeighborsClassifier(n_neighbors=args.k)\\n    clf.fit(Xtr, ytr)\\n    ypred = clf.predict(Xte)\\n    acc = accuracy_score(yte, ypred)\\n    print(f\\\"dataset={used} k={args.k} test_accuracy={acc:.3f}\\\")\\n\\n    # acceptance: stricter if real digits, looser if synthetic\\n    threshold = 0.90 if used == \\\"digits\\\" else 0.15\\n    if acc >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold:.3f}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Text Sentiment Analyzer\",\n",
            "    \"description\": \"Generate 200 synthetic sentences with positive/negative labels using keyword rules, train a logistic regression on TF-IDF features, and print validation accuracy. Accept --text flag to classify a new sentence.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_sentences(n=200, seed=42):\\n    rng = np.random.default_rng(seed)\\n\n",
            "    pos_words = [\\\"excellent\\\", \\\"amazing\\\", \\\"wonderful\\\", \\\"great\\\", \\\"fantastic\\\", \\\"love\\\", \\\"perfect\\\", \\\"best\\\", \\\"happy\\\", \\\"brilliant\\\"]\\n    neg_words = [\\\"terrible\\\", \\\"awful\\\", \\\"horrible\\\", \\\"worst\\\", \\\"hate\\\", \\\"bad\\\", \\\"disappointing\\\", \\\"poor\\\", \\\"sad\\\", \\\"useless\\\"]\\n    neutral = [\\\"the\\\", \\\"is\\\", \\\"was\\\", \\\"a\\\", \\\"an\\\", \\\"this\\\", \\\"that\\\", \\\"it\\\", \\\"very\\\", \\\"really\\\", \\\"quite\\\", \\\"so\\\", \\\"much\\\"]\\n    \\n    sentences = []\\n    labels = []\\n    for _ in range(n):\\n        label = rng.integers(0, 2)\\n        if label == 1:\\n            core = rng.choice(pos_words, size=rng.integers(1, 3), replace=True)\\n        else:\\n            core = rng.choice(neg_words, size=rng.integers(1, 3), replace=True)\\n        filler = rng.choice(neutral, size=rng.integers(2, 5), replace=True)\\n        words = list(core) + list(filler)\\n        rng.shuffle(words)\\n        sentences.append(\\\" \\\".join(words))\\n        labels.append(label)\\n    return sentences, np.array(labels)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate synthetic sentiment sentences, train logistic regression on TF-IDF, and classify new text.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of synthetic sentences (default: 200).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--text\\\", type=str, default=None, help=\\\"Optional sentence to classify after training.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples < 10:\\n        print(\\\"TEST_FAIL: n_samples must be >= 10\\\")\\n        sys.exit(1)\\n    if not (0.0 < args.test_size < 1.0):\\n        print(\\\"TEST_FAIL: test_size must be in (0, 1)\\\")\\n        sys.exit(1)\\n\\n    sentences, labels = generate_synthetic_sentences(n=args.n_samples, seed=args.seed)\\n    if len(sentences) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: failed to generate synthetic data\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        sentences, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\\n    )\\n\\n    vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\\n    X_train_tfidf = vectorizer.fit_transform(X_train)\\n    X_test_tfidf = vectorizer.transform(X_test)\\n\\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\\n    clf.fit(X_train_tfidf, y_train)\\n\\n    y_pred = clf.predict(X_test_tfidf)\\n    acc = accuracy_score(y_test, y_pred)\\n    print(f\\\"validation_accuracy={acc:.3f}\\\")\\n\\n    if args.text is not None:\\n        text_tfidf = vectorizer.transform([args.text])\\n        pred_label = clf.predict(text_tfidf)[0]\\n        sentiment = \\\"positive\\\" if pred_label == 1 else \\\"negative\\\"\\n        print(f\\\"input_text='{args.text}' predicted_sentiment={sentiment}\\\")\\n\\n    if acc >= 0.70:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.70\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  }\n",
            "]\n",
            "\n",
            "===== qwen/qwen3-coder: generating and attaching code for 2 projects =====\n",
            "\n",
            "--- qwen/qwen3-coder • Project 1: Digit Classifier CLI ---\n",
            "\n",
            "import argparse, sys\n",
            "import random, numpy as np\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.datasets import load_digits\n",
            "\n",
            "def load_digits_or_synthetic(seed=42):\n",
            "    try:\n",
            "        digits = load_digits()\n",
            "        X, y = digits.data, digits.target\n",
            "        used = \"digits\"\n",
            "    except Exception:\n",
            "        rng = np.random.default_rng(seed)\n",
            "        n = 300\n",
            "        X = rng.integers(0, 16, size=(n, 64)).astype(np.float32)\n",
            "        y = rng.integers(0, 10, size=n)\n",
            "        used = \"synthetic\"\n",
            "    return X, y, used\n",
            "\n",
            "def main():\n",
            "    p = argparse.ArgumentParser(description=\"Train logistic regression on digits dataset or synthetic fallback; classify stdin vector with --predict.\")\n",
            "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Fraction of data to use for testing (default: 0.2).\")\n",
            "    p.add_argument(\"--seed\", type=int, default=42, help=\"Random seed for reproducibility (default: 42).\")\n",
            "    p.add_argument(\"--predict\", action=\"store_true\", help=\"Read a comma-separated 64-element vector from stdin and predict its class.\")\n",
            "    p.add_argument(\"--allow-download\", action=\"store_true\", help=\"Allow downloading the digits dataset if not cached.\")\n",
            "    args = p.parse_args()\n",
            "\n",
            "    random.seed(args.seed)\n",
            "    np.random.seed(args.seed)\n",
            "    try:\n",
            "        import torch\n",
            "        torch.manual_seed(args.seed)\n",
            "    except Exception:\n",
            "        pass\n",
            "\n",
            "    if args.predict:\n",
            "        try:\n",
            "            line = sys.stdin.readline().strip()\n",
            "            vec = list(map(float, line.split(\",\")))\n",
            "            if len(vec) != 64:\n",
            "                print(\"TEST_FAIL: input vector must have exactly 64 elements\")\n",
            "                sys.exit(1)\n",
            "            vec = np.array(vec).reshape(1, -1)\n",
            "        except Exception:\n",
            "            print(\"TEST_FAIL: failed to parse input vector\")\n",
            "            sys.exit(1)\n",
            "\n",
            "        try:\n",
            "            X, y, used = load_digits_or_synthetic(seed=args.seed)\n",
            "            clf = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LogisticRegression(max_iter=300))])\n",
            "            clf.fit(X, y)\n",
            "            pred = clf.predict(vec)[0]\n",
            "            print(f\"predicted_class={pred}\")\n",
            "            print(\"TEST_PASS\")\n",
            "        except Exception as e:\n",
            "            print(f\"TEST_FAIL: prediction failed: {e}\")\n",
            "            sys.exit(1)\n",
            "        return\n",
            "\n",
            "    # Training mode\n",
            "    X, y, used = load_digits_or_synthetic(seed=args.seed)\n",
            "    if X is None or y is None or len(X) == 0:\n",
            "        print(\"TEST_FAIL: dataset not available\")\n",
            "        sys.exit(1)\n",
            "\n",
            "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\n",
            "    clf = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LogisticRegression(max_iter=300))])\n",
            "    clf.fit(Xtr, ytr)\n",
            "    acc = clf.score(Xte, yte)\n",
            "    print(f\"dataset={used} accuracy={acc:.3f}\")\n",
            "\n",
            "    if acc >= (0.85 if used == \"digits\" else 0.70):\n",
            "        print(\"TEST_PASS\")\n",
            "    else:\n",
            "        print(\"TEST_FAIL: accuracy below threshold\")\n",
            "        sys.exit(1)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# END_OF_SCRIPT\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "429 Client Error: Too Many Requests for url: https://openrouter.ai/api/v1/chat/completions",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-715495703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuild_code_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         ]\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_openrouter_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_code_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4214751829.py\u001b[0m in \u001b[0;36mcall_openrouter_model\u001b[0;34m(model_id, messages, temperature, top_p, max_tokens)\u001b[0m\n\u001b[1;32m     19\u001b[0m     }\n\u001b[1;32m     20\u001b[0m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://openrouter.ai/api/v1/chat/completions"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(anthropic_claude_sonnet_4_5_result[\"items_with_code\"], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "quVj0XmEp_95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ea2ade-7118-4902-d5d2-d4aa988641ea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"title\": \"Digits KNN Classifier\",\n",
            "    \"description\": \"Train a k-NN classifier on sklearn's digits dataset and report test accuracy. Accept --k flag to set neighbors and --predict flag with a 64-element comma-separated list to classify a single digit.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\ndef load_digits_or_synthetic(seed=42):\\n    \\\"\\\"\\\"Load sklearn digits (no download required) or fallback to synthetic.\\\"\\\"\\\"\\n    try:\\n        from sklearn.datasets import load_digits\\n        data = load_digits()\\n        X, y, used = data.data, data.target, \\\"digits\\\"\\n    except Exception:\\n        rng = np.random.default_rng(seed)\\n        n = 1797\\n        X = rng.uniform(0, 16, size=(n, 64))\\n        y = rng.integers(0, 10, size=n)\\n        used = \\\"synthetic\\\"\\n    return X, y, used\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Train k-NN on sklearn digits; predict single digit from 64-element CSV.\\\")\\n    p.add_argument(\\\"--k\\\", type=int, default=3, help=\\\"Number of neighbors (default: 3).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--predict\\\", type=str, default=None, help=\\\"Comma-separated 64 floats to classify a single digit.\\\")\\n    args = p.parse_args()\\n\\n    # seeds in main\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    X, y, used = load_digits_or_synthetic(args.seed)\\n    if X is None or y is None or len(X) == 0:\\n        print(\\\"TEST_FAIL: dataset not available\\\")\\n        sys.exit(1)\\n\\n    # single prediction mode\\n    if args.predict is not None:\\n        parts = args.predict.split(\\\",\\\")\\n        if len(parts) != 64:\\n            print(f\\\"TEST_FAIL: --predict requires exactly 64 values, got {len(parts)}\\\")\\n            sys.exit(1)\\n        try:\\n            sample = np.array([float(v) for v in parts]).reshape(1, -1)\\n        except ValueError:\\n            print(\\\"TEST_FAIL: --predict values must be numeric\\\")\\n            sys.exit(1)\\n        clf = KNeighborsClassifier(n_neighbors=args.k)\\n        clf.fit(X, y)\\n        pred = clf.predict(sample)[0]\\n        print(f\\\"dataset={used} k={args.k} predicted_digit={pred}\\\")\\n        print(\\\"TEST_PASS\\\")\\n        return\\n\\n    # train/test mode\\n    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, random_state=args.seed, stratify=y)\\n    clf = KNeighborsClassifier(n_neighbors=args.k)\\n    clf.fit(Xtr, ytr)\\n    ypred = clf.predict(Xte)\\n    acc = accuracy_score(yte, ypred)\\n    print(f\\\"dataset={used} k={args.k} test_accuracy={acc:.3f}\\\")\\n\\n    # acceptance: stricter if real digits, looser if synthetic\\n    threshold = 0.90 if used == \\\"digits\\\" else 0.15\\n    if acc >= threshold:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(f\\\"TEST_FAIL: accuracy {acc:.3f} below threshold {threshold:.3f}\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Synthetic Text Sentiment Analyzer\",\n",
            "    \"description\": \"Generate 200 synthetic sentences with positive/negative labels using keyword rules, train a logistic regression on TF-IDF features, and print validation accuracy. Accept --text flag to classify a new sentence.\",\n",
            "    \"code\": \"import argparse\\nimport sys\\nimport random\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\ndef generate_synthetic_sentences(n=200, seed=42):\\n    rng = np.random.default_rng(seed)\\n    pos_words = [\\\"excellent\\\", \\\"amazing\\\", \\\"wonderful\\\", \\\"great\\\", \\\"fantastic\\\", \\\"love\\\", \\\"perfect\\\", \\\"best\\\", \\\"happy\\\", \\\"brilliant\\\"]\\n    neg_words = [\\\"terrible\\\", \\\"awful\\\", \\\"horrible\\\", \\\"worst\\\", \\\"hate\\\", \\\"bad\\\", \\\"disappointing\\\", \\\"poor\\\", \\\"sad\\\", \\\"useless\\\"]\\n    neutral = [\\\"the\\\", \\\"is\\\", \\\"was\\\", \\\"a\\\", \\\"an\\\", \\\"this\\\", \\\"that\\\", \\\"it\\\", \\\"very\\\", \\\"really\\\", \\\"quite\\\", \\\"so\\\", \\\"much\\\"]\\n    \\n    sentences = []\\n    labels = []\\n    for _ in range(n):\\n        label = rng.integers(0, 2)\\n        if label == 1:\\n            core = rng.choice(pos_words, size=rng.integers(1, 3), replace=True)\\n        else:\\n            core = rng.choice(neg_words, size=rng.integers(1, 3), replace=True)\\n        filler = rng.choice(neutral, size=rng.integers(2, 5), replace=True)\\n        words = list(core) + list(filler)\\n        rng.shuffle(words)\\n        sentences.append(\\\" \\\".join(words))\\n        labels.append(label)\\n    return sentences, np.array(labels)\\n\\ndef main():\\n    p = argparse.ArgumentParser(description=\\\"Generate synthetic sentiment sentences, train logistic regression on TF-IDF, and classify new text.\\\")\\n    p.add_argument(\\\"--seed\\\", type=int, default=42, help=\\\"Random seed (default: 42).\\\")\\n    p.add_argument(\\\"--n-samples\\\", type=int, default=200, help=\\\"Number of synthetic sentences (default: 200).\\\")\\n    p.add_argument(\\\"--test-size\\\", type=float, default=0.2, help=\\\"Test set fraction (default: 0.2).\\\")\\n    p.add_argument(\\\"--text\\\", type=str, default=None, help=\\\"Optional sentence to classify after training.\\\")\\n    args = p.parse_args()\\n\\n    random.seed(args.seed)\\n    np.random.seed(args.seed)\\n    try:\\n        import torch\\n        torch.manual_seed(args.seed)\\n    except Exception:\\n        pass\\n\\n    if args.n_samples < 10:\\n        print(\\\"TEST_FAIL: n_samples must be >= 10\\\")\\n        sys.exit(1)\\n    if not (0.0 < args.test_size < 1.0):\\n        print(\\\"TEST_FAIL: test_size must be in (0, 1)\\\")\\n        sys.exit(1)\\n\\n    sentences, labels = generate_synthetic_sentences(n=args.n_samples, seed=args.seed)\\n    if len(sentences) == 0 or len(labels) == 0:\\n        print(\\\"TEST_FAIL: failed to generate synthetic data\\\")\\n        sys.exit(1)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        sentences, labels, test_size=args.test_size, random_state=args.seed, stratify=labels\\n    )\\n\\n    vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\\n    X_train_tfidf = vectorizer.fit_transform(X_train)\\n    X_test_tfidf = vectorizer.transform(X_test)\\n\\n    clf = LogisticRegression(max_iter=300, random_state=args.seed)\\n    clf.fit(X_train_tfidf, y_train)\\n\\n    y_pred = clf.predict(X_test_tfidf)\\n    acc = accuracy_score(y_test, y_pred)\\n    print(f\\\"validation_accuracy={acc:.3f}\\\")\\n\\n    if args.text is not None:\\n        text_tfidf = vectorizer.transform([args.text])\\n        pred_label = clf.predict(text_tfidf)[0]\\n        sentiment = \\\"positive\\\" if pred_label == 1 else \\\"negative\\\"\\n        print(f\\\"input_text='{args.text}' predicted_sentiment={sentiment}\\\")\\n\\n    if acc >= 0.70:\\n        print(\\\"TEST_PASS\\\")\\n    else:\\n        print(\\\"TEST_FAIL: accuracy below 0.70\\\")\\n        sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n# END_OF_SCRIPT\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** So now we have more structured and more correct project codes and descriptions that before which they are acceptable for finetuning the smaller models."
      ],
      "metadata": {
        "id": "Pnro2mqpboNm"
      }
    }
  ]
}
